# Adicionar as colunas ausentes com NA
for (col in missing_cols) {
data[[col]] <- NA
}
# Selecionar as colunas na ordem correta
data <- data[required_columns]
return(data)
}
# Aplicar a padronização e combinar todos os arquivos
combined_data_list <- lapply(file_list, standardize_columns)
combined_data <- do.call(rbind, combined_data_list)
# Converter a coluna Date para o tipo Date
combined_data$Date <- as.Date(combined_data$Date)
# Filtrar os dados para os últimos 12 meses
last_year_date <- Sys.Date() - 365
data_last_year <- combined_data %>% filter(Date >= last_year_date)
# Calcular a média de fechamento ajustado para cada commodity nos últimos 12 meses
average_close <- data_last_year %>%
group_by(Ticker) %>%
summarise(Average_Close = mean(Adj.Close, na.rm = TRUE))
# Carregar os pacotes necessários
library(vroom)
library(plotly)
library(quantmod)
library(dplyr)
# Definir os tickers das commodities e o período de tempo
tickers <- c(
"ZC=F", # Corn Futures
"ZO=F", # Wheat Futures
"KE=F", # KC HRW Wheat Futures
"ZR=F", # Rough Rice Futures
"GF=F", # Feeder Cattle Futures
"ZS=F", # Soybean Meal Futures
"ZM=F", # Soybean Futures
"ZL=F"  # Soybean Oil Futures
)
start_date <- as.Date("2000-01-01")
end_date <- Sys.Date()
# Diretório para salvar os arquivos CSV
dir.create("data", showWarnings = FALSE)
# Passo 1: Baixar dados históricos e salvar em arquivos CSV
for (ticker in tickers) {
cat("Baixando dados para:", ticker, "\n")
# Tentar obter os dados do Yahoo Finance
tryCatch({
data_env <- new.env()
getSymbols(ticker, src = "yahoo", from = start_date, to = end_date, env = data_env, auto.assign = TRUE)
# Extrair os dados do ambiente e verificar o tipo
data <- data_env[[ticker]]
# Verificar se os dados são do tipo xts
if (is.xts(data)) {
# Converter para data frame e incluir a coluna Date
data <- data.frame(Date = index(data), coredata(data))
# Remover prefixos do ticker nos nomes das colunas
colnames(data) <- gsub(paste0("^", gsub("=F", "", ticker), "\\."), "", colnames(data))
# Selecionar apenas as colunas necessárias
required_columns <- c("Date", "Open", "High", "Low", "Close", "Volume", "Adjusted")
data <- data[, colnames(data) %in% required_columns, drop = FALSE]
# Verificar se a coluna 'Adjusted' existe, se não, criar a partir de 'Close'
if (!"Adjusted" %in% colnames(data)) {
data$Adjusted <- data$Close
}
# Garantir que todas as colunas necessárias estejam presentes
missing_cols <- setdiff(required_columns, colnames(data))
for (col in missing_cols) {
data[[col]] <- NA
}
# Selecionar as colunas na ordem correta
data <- data[required_columns]
# Adicionar a coluna 'Ticker' ao data frame
data$Ticker <- ticker
# Remover linhas com valores ausentes
data <- na.omit(data)
# Renomear 'Adjusted' para 'Adj.Close'
colnames(data)[colnames(data) == "Adjusted"] <- "Adj.Close"
# Salvar em CSV
write.csv(data, file = paste0("data/", ticker, "_data.csv"), row.names = FALSE)
} else {
cat("Erro: Dados para", ticker, "não estão no formato esperado (xts).\n")
}
}, error = function(e) {
cat("Erro ao baixar os dados para:", ticker, "\n")
cat("Erro:", conditionMessage(e), "\n")
})
}
# Passo 2: Ler os arquivos CSV e padronizar colunas antes de combinar
file_list <- list.files("data", pattern = "*_data.csv", full.names = TRUE)
# Função para garantir que todos os arquivos tenham as colunas necessárias
standardize_columns <- function(file) {
data <- read.csv(file)
# Garantir que todas as colunas necessárias estejam presentes
required_columns <- c("Date", "Open", "High", "Low", "Close", "Volume", "Adj.Close", "Ticker")
missing_cols <- setdiff(required_columns, colnames(data))
# Adicionar as colunas ausentes com NA
for (col in missing_cols) {
data[[col]] <- NA
}
# Selecionar as colunas na ordem correta
data <- data[required_columns]
return(data)
}
# Aplicar a padronização e combinar todos os arquivos
combined_data_list <- lapply(file_list, standardize_columns)
combined_data <- do.call(rbind, combined_data_list)
# Converter a coluna Date para o tipo Date
combined_data$Date <- as.Date(combined_data$Date)
# Filtrar os dados para os últimos 12 meses
last_year_date <- Sys.Date() - 365
data_last_year <- combined_data %>% filter(Date >= last_year_date)
# Calcular a média de fechamento ajustado para cada commodity nos últimos 12 meses
average_close <- data_last_year %>%
group_by(Ticker) %>%
summarise(Average_Close = mean(Adj.Close, na.rm = TRUE))
print("Média do preço de fechamento ajustado nos últimos 12 meses:")
print(average_close)
# Calcular retornos diários usando o preço de fechamento ajustado
data_last_year <- data_last_year %>%
group_by(Ticker) %>%
arrange(Date) %>%
mutate(Return = Adj.Close / lag(Adj.Close) - 1)
# Calcular volatilidade (desvio padrão dos retornos) para cada commodity nos últimos 12 meses
volatility <- data_last_year %>%
group_by(Ticker) %>%
summarise(Volatility = sd(Return, na.rm = TRUE))
print("Volatilidade dos retornos nos últimos 12 meses:")
print(volatility)
# Gráfico interativo dos preços de fechamento ajustados
price_plot <- data_last_year %>%
plot_ly(x = ~Date, y = ~Adj.Close, color = ~Ticker, type = 'scatter', mode = 'lines') %>%
layout(title = "Preços de Fechamento Ajustados - Últimos 12 Meses",
xaxis = list(title = "Data"),
yaxis = list(title = "Preço de Fechamento Ajustado (USD)"))
# Gráfico interativo dos retornos
return_plot <- data_last_year %>%
plot_ly(x = ~Date, y = ~Return, color = ~Ticker, type = 'scatter', mode = 'lines') %>%
layout(title = "Retornos Diários - Últimos 12 Meses",
xaxis = list(title = "Data"),
yaxis = list(title = "Retorno Diário"))
# Gráfico de volatilidade (gráfico de barras)
volatility_plot <- volatility %>%
plot_ly(x = ~Ticker, y = ~Volatility, type = 'bar', color = ~Ticker) %>%
layout(title = "Volatilidade dos Retornos - Últimos 12 Meses",
xaxis = list(title = "Ticker"),
yaxis = list(title = "Volatilidade"))
# Exibir os gráficos
print(price_plot)
print(return_plot)
# Carregar os pacotes necessários
library(vroom)
library(plotly)
library(quantmod)
library(dplyr)
# Definir os tickers das commodities e o período de tempo
tickers <- c(
"ZC=F", # Corn Futures
"ZO=F", # Wheat Futures
"KE=F", # KC HRW Wheat Futures
"ZR=F", # Rough Rice Futures
"GF=F", # Feeder Cattle Futures
"ZS=F", # Soybean Meal Futures
"ZM=F", # Soybean Futures
"ZL=F"  # Soybean Oil Futures
)
start_date <- as.Date("2000-01-01")
end_date <- Sys.Date()
# Diretório para salvar os arquivos CSV
dir.create("data", showWarnings = FALSE)
# Passo 1: Baixar dados históricos e salvar em arquivos CSV
for (ticker in tickers) {
cat("Baixando dados para:", ticker, "\n")
# Tentar obter os dados do Yahoo Finance
tryCatch({
data_env <- new.env()
getSymbols(ticker, src = "yahoo", from = start_date, to = end_date, env = data_env, auto.assign = TRUE)
# Extrair os dados do ambiente e verificar o tipo
data <- data_env[[ticker]]
# Verificar se os dados são do tipo xts
if (is.xts(data)) {
# Converter para data frame e incluir a coluna Date
data <- data.frame(Date = index(data), coredata(data))
# Remover prefixos do ticker nos nomes das colunas
colnames(data) <- gsub(paste0("^", gsub("=F", "", ticker), "\\."), "", colnames(data))
# Selecionar apenas as colunas necessárias
required_columns <- c("Date", "Open", "High", "Low", "Close", "Volume", "Adjusted")
data <- data[, colnames(data) %in% required_columns, drop = FALSE]
# Verificar se a coluna 'Adjusted' existe, se não, criar a partir de 'Close'
if (!"Adjusted" %in% colnames(data)) {
data$Adjusted <- data$Close
}
# Garantir que todas as colunas necessárias estejam presentes
missing_cols <- setdiff(required_columns, colnames(data))
for (col in missing_cols) {
data[[col]] <- NA
}
# Selecionar as colunas na ordem correta
data <- data[required_columns]
# Adicionar a coluna 'Ticker' ao data frame
data$Ticker <- ticker
# Remover linhas com valores ausentes
data <- na.omit(data)
# Renomear 'Adjusted' para 'Adj.Close'
colnames(data)[colnames(data) == "Adjusted"] <- "Adj.Close"
# Salvar em CSV
write.csv(data, file = paste0("data/", ticker, "_data.csv"), row.names = FALSE)
} else {
cat("Erro: Dados para", ticker, "não estão no formato esperado (xts).\n")
}
}, error = function(e) {
cat("Erro ao baixar os dados para:", ticker, "\n")
cat("Erro:", conditionMessage(e), "\n")
})
}
# Passo 2: Ler os arquivos CSV e padronizar colunas antes de combinar
file_list <- list.files("data", pattern = "*_data.csv", full.names = TRUE)
# Função para garantir que todos os arquivos tenham as colunas necessárias
standardize_columns <- function(file) {
data <- read.csv(file)
# Extrair o ticker do nome do arquivo e adicionar ao data frame
data$Ticker <- sub("data/(.*)_data.csv", "\\1", file)
# Garantir que todas as colunas necessárias estejam presentes
required_columns <- c("Date", "Open", "High", "Low", "Close", "Volume", "Adj.Close", "Ticker")
missing_cols <- setdiff(required_columns, colnames(data))
# Adicionar as colunas ausentes com NA
for (col in missing_cols) {
data[[col]] <- NA
}
# Selecionar as colunas na ordem correta
data <- data[required_columns]
return(data)
}
# Aplicar a padronização e combinar todos os arquivos
combined_data_list <- lapply(file_list, standardize_columns)
combined_data <- do.call(rbind, combined_data_list)
# Converter a coluna Date para o tipo Date
combined_data$Date <- as.Date(combined_data$Date)
# Filtrar os dados para os últimos 12 meses
last_year_date <- Sys.Date() - 365
data_last_year <- combined_data %>% filter(Date >= last_year_date)
# Calcular a média de fechamento ajustado para cada commodity nos últimos 12 meses
average_close <- data_last_year %>%
group_by(Ticker) %>%
summarise(Average_Close = mean(Adj.Close, na.rm = TRUE))
print("Média do preço de fechamento ajustado nos últimos 12 meses:")
print(average_close)
# Calcular retornos diários usando o preço de fechamento ajustado
data_last_year <- data_last_year %>%
group_by(Ticker) %>%
arrange(Date) %>%
mutate(Return = Adj.Close / lag(Adj.Close) - 1)
# Calcular volatilidade (desvio padrão dos retornos) para cada commodity nos últimos 12 meses
volatility <- data_last_year %>%
group_by(Ticker) %>%
summarise(Volatility = sd(Return, na.rm = TRUE))
print("Volatilidade dos retornos nos últimos 12 meses:")
print(volatility)
# Gráfico interativo dos preços de fechamento ajustados
price_plot <- data_last_year %>%
plot_ly(x = ~Date, y = ~Adj.Close, color = ~Ticker, type = 'scatter', mode = 'lines') %>%
layout(title = "Preços de Fechamento Ajustados - Últimos 12 Meses",
xaxis = list(title = "Data"),
yaxis = list(title = "Preço de Fechamento Ajustado (USD)"))
# Gráfico interativo dos retornos
return_plot <- data_last_year %>%
plot_ly(x = ~Date, y = ~Return, color = ~Ticker, type = 'scatter', mode = 'lines') %>%
layout(title = "Retornos Diários - Últimos 12 Meses",
xaxis = list(title = "Data"),
yaxis = list(title = "Retorno Diário"))
# Gráfico de volatilidade (gráfico de barras)
volatility_plot <- volatility %>%
plot_ly(x = ~Ticker, y = ~Volatility, type = 'bar', color = ~Ticker) %>%
layout(title = "Volatilidade dos Retornos - Últimos 12 Meses",
xaxis = list(title = "Ticker"),
yaxis = list(title = "Volatilidade"))
# Exibir os gráficos
print(price_plot)
print(return_plot)
print(volatility_plot)
# Instalar pacotes necessários
if (!requireNamespace("sparklyr", quietly = TRUE)) install.packages("sparklyr")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
if (!requireNamespace("quantmod", quietly = TRUE)) install.packages("quantmod")
if (!requireNamespace("PerformanceAnalytics", quietly = TRUE)) install.packages("PerformanceAnalytics")
if (!requireNamespace("DBI", quietly = TRUE)) install.packages("DBI")
library(sparklyr)
library(dplyr)
library(ggplot2)
library(quantmod)
library(PerformanceAnalytics)
# Configuração do Spark
if (Sys.getenv("DATABRICKS_RUNTIME_VERSION") != "") {
# Conexão com Spark no Databricks
sc <- spark_connect(method = "databricks")
message("Conectado ao Spark no ambiente Databricks")
} else {
# Configuração local
sc <- spark_connect(master = "local")
message("Conectado ao Spark no ambiente local")
}
# Instalar pacotes necessários
if (!requireNamespace("sparklyr", quietly = TRUE)) install.packages("sparklyr")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
if (!requireNamespace("quantmod", quietly = TRUE)) install.packages("quantmod")
if (!requireNamespace("PerformanceAnalytics", quietly = TRUE)) install.packages("PerformanceAnalytics")
if (!requireNamespace("DBI", quietly = TRUE)) install.packages("DBI")
library(sparklyr)
library(dplyr)
library(ggplot2)
library(quantmod)
library(PerformanceAnalytics)
# Configuração do Spark
if (Sys.getenv("DATABRICKS_RUNTIME_VERSION") != "") {
# Conexão com Spark no Databricks
sc <- spark_connect(method = "databricks")
message("Conectado ao Spark no ambiente Databricks")
} else {
# Configuração local
sc <- spark_connect(master = "local")
message("Conectado ao Spark no ambiente local")
}
Sys.getenv("JAVA_HOME")
spark_install(version = "3.4.1")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_431")
# Instalar pacotes necessários
if (!requireNamespace("sparklyr", quietly = TRUE)) install.packages("sparklyr")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
if (!requireNamespace("quantmod", quietly = TRUE)) install.packages("quantmod")
if (!requireNamespace("PerformanceAnalytics", quietly = TRUE)) install.packages("PerformanceAnalytics")
if (!requireNamespace("DBI", quietly = TRUE)) install.packages("DBI")
library(sparklyr)
library(dplyr)
library(ggplot2)
library(quantmod)
library(PerformanceAnalytics)
# Configuração do Spark
if (Sys.getenv("DATABRICKS_RUNTIME_VERSION") != "") {
# Conexão com Spark no Databricks
sc <- spark_connect(method = "databricks")
message("Conectado ao Spark no ambiente Databricks")
} else {
# Configuração local
sc <- spark_connect(master = "local")
message("Conectado ao Spark no ambiente local")
}
Sys.getenv("JAVA_HOME")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_431")
library(sparklyr)
sc <- spark_connect(master = "local")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_431")
system("java -version")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_431")
system("java -version")
Sys.getenv("JAVA_HOME")
system("java -version")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_431")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_431")
system("java -version")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_411")
system("java -version")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_411")
system("java -version")
library(sparklyr)
sc <- spark_connect(master = "local")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_411")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_411")
system("java -version")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk1.8.0_411")
system("java -version")
spark_install()
devtools::install_github("rstudio/sparklyr")
library(sparklyr)
sc <- spark_connect(master = "local")
library(sparklyr)
spark_install(version = "3.0.0")
sc <- spark_connect(master = 'local')
install.packages("rsparkling")
library(rsparkling)
sc <- spark_connect(master = "local")
library(sparklyr)
spark_install()
sc <- spark_connect(master = "local")
devtools::install_github("sparklyr/sparklyr")
devtools::install_github("sparklyr/sparklyr", force = TRUE)
sparklyr::spark_connect(master="local")
sparklyr::spark_connect(master = "local")
sparklyr::spark_connect(master="local")
Sys.setenv(JAVA_HOME=" ")
sparklyr::spark_connect(master="local")
system("java -version")
system("java -version")
# Script de extração de duas colunas adicionais a saber:
# - classificacao_por_palavras
# - contagem_palavras_chave
# - duracao_processo
import sys
reticulate::repl_python()
# Script de extração de duas colunas adicionais a saber:
# - classificacao_por_palavras
# - contagem_palavras_chave
# - duracao_processo
import sys
import os
import pandas as pd
from datetime import datetime
# Adicione o caminho do diretório ao PYTHONPATH (Mateus, troque esse path para rodar localmente, por favor)
custom_path = r"C:\Users\rodri\OneDrive - Grupo Marista\Fleury\datajud-send\datajud-send"
sys.path.append(custom_path)
from extrator_datajud_judiciario_estadual import extract_and_process_tribunal_data
from save_and_load_process import save_extracted_data
from datajud_api import data_mining_datajud
from preprocessing import process_data_datajud
# Configuração dos tribunais e URLs
tribunais = {
"Tribunal de Justiça de Minas Gerais": "https://api-publica.datajud.cnj.jus.br/api_publica_tjmg/_search",
"Tribunal de Justiça de São Paulo": "https://api-publica.datajud.cnj.jus.br/api_publica_tjsp/_search"
}
# Configurações de extração
extraction_type = 'judiciario_estadual'
chunk_size = 1000  # Quantidade máxima permitida de dados por requisição
total_records = 150000  # Total de registros por tribunal solicitado
assuntos_codes = [
9593, 10437, 10434, 10443, 10440, 12481, 12482,
11974, 7771, 7775, 7774, 11810, 11864, 11812,
11811, 7769, 7780, 7779, 8960
]
movimentos_codes = [246, 22, 848, 11385, 12430]
# Função principal para gerenciar o processo de extração e classificação
def main():
for tribunal, url in tribunais.items():
total_extracted = 0
all_resumo = []
all_movimentos = []
all_assuntos = []
while total_extracted < total_records:
# Extrair dados
data = data_mining_datajud(url, tribunal, chunk_size, assuntos_codes, movimentos_codes)
if not data or 'hits' not in data or 'hits' not in data['hits']:
print(f"Nenhum dado retornado para {tribunal}.")
break
resumo, movimentos, assuntos = process_data_datajud(data, tribunal, extraction_type)
# Acumular resultados
if resumo is not None and movimentos is not None and assuntos is not None:
all_resumo.append(resumo)
all_movimentos.append(movimentos)
all_assuntos.append(assuntos)
total_extracted += len(resumo)
else:
break  # Sem mais dados para processar
print(f"{total_extracted} registros extraídos até agora para {tribunal}.")
# Combinar resultados e salvar
if all_resumo:
resumo_df = pd.concat(all_resumo, ignore_index=True)
movimentos_df = pd.concat(all_movimentos, ignore_index=True)
assuntos_df = pd.concat(all_assuntos, ignore_index=True)
# Adicionar colunas adicionais
movimentos_df = adicionar_colunas_adicionais(movimentos_df, resumo_df)
# Salvar os dados
save_extracted_data(extraction_type, resumo_df, movimentos_df, assuntos_df, sample=False)
# Classificar os processos
classificar_processos(movimentos_df)
else:
print(f"Sem dados suficientes para {tribunal}.")
# Função para adicionar colunas adicionais
def adicionar_colunas_adicionais(movimentos_df, resumo_df):
# Classificação por palavras-chave
palavras_chave = {
"improcedente": "Improcedente",
"procedente": "Procedente",
"acordo": "Acordo"
}
def classificar_por_palavra_chave(texto):
for palavra, classificacao in palavras_chave.items():
if palavra in str(texto).lower():
return classificacao
return "Indefinido"
movimentos_df['classificacao_por_palavras'] = movimentos_df['nome'].apply(classificar_por_palavra_chave)
def contar_palavras_chave(texto):
return sum(str(texto).lower().count(palavra) for palavra in palavras_chave)
movimentos_df['contagem_palavras_chave'] = movimentos_df['nome'].apply(contar_palavras_chave)
# Adicionar a duração do processo
def calcular_duracao(numero_processo):
data_inicio = resumo_df.loc[resumo_df['numeroProcesso'] == numero_processo, 'dataAjuizamento']
data_fim = resumo_df.loc[resumo_df['numeroProcesso'] == numero_processo, 'dataHoraUltimaAtualizacao']
if not data_inicio.empty and not data_fim.empty:
try:
data_inicio = datetime.fromisoformat(data_inicio.values[0])
data_fim = datetime.fromisoformat(data_fim.values[0])
return (data_fim - data_inicio).days
except Exception as e:
return None
return None
movimentos_df['duracao_processo'] = movimentos_df['numeroProcesso'].apply(calcular_duracao)
return movimentos_df
# Função para classificar os processos com base nos movimentos
def classificar_processos(movimentos_df):
movimentos_df['classificacao'] = movimentos_df['codigo'].apply(classificar_movimento)
print(movimentos_df[['numeroProcesso', 'codigo', 'nome', 'classificacao', 'classificacao_por_palavras', 'duracao_processo']].head())
# Classificação com base no código do movimento
def classificar_movimento(codigo_movimento):
if codigo_movimento in [246, 848]:  # Exemplo: Procedente
return "Procedente"
elif codigo_movimento in [22, 11385]:  # Exemplo: Improcedente
return "Improcedente"
elif codigo_movimento == 12430:  # Exemplo: Acordo
return "Acordo"
else:
return "Indefinido"
if __name__ == "__main__":
main()
reticulate::repl_python()
