[
  {
    "objectID": "time_series_portfolio.html",
    "href": "time_series_portfolio.html",
    "title": "Data Extraction for preselected commodities portfolio",
    "section": "",
    "text": "Abstract\n\n\n\nThis small document have the goal to share the time series extraction and the two basic features building, like price returns and their conditional variance…\n\n\n\n  \n\n\nIntro\n[… to be written …]\n\n\n\nPython codes\n\nPython libsLoading time seriesPrices log-returnsLog-returns conditional variances\n\n\n\n\nCode\n\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom arch import arch_model\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal\n\n\n\n\nThe portfolio contains the following commodities price returns:\n\nCorn Futures\nWheat Futures\nKC HRW Wheat Futures\nRough Rice Futures\nFeeder Cattle Futures\nSoyMeal Futures\nSoy Meal Futures\nSoyBeans Futures\n\n\n\nCode\n# Tickers for portfolio\nTICKERS = [\n    \"ZC=F\",  # Corn Futures\n    \"ZO=F\",  # Wheat Futures\n    \"KE=F\",  # KC HRW Wheat Futures\n    \"ZR=F\",  # Rough Rice Futures\n    \"GF=F\",  # Feeder Cattle Futures\n    \"ZS=F\",  # SoyMeal Futures\n    \"ZM=F\",  # Soybean Meal Futures\n    \"ZL=F\"   # SoyBeans Futures\n]\n\n\n# Downloading data from Yahoo Finance\nportfolio_prices = yf.download(TICKERS, start=\"2019-01-01\")['Adj Close']\n\n\n\n[                       0%                       ]\n[************          25%                       ]  2 of 8 completed\n[******************    38%                       ]  3 of 8 completed\n[**********************50%                       ]  4 of 8 completed\n[**********************62%*****                  ]  5 of 8 completed\n[**********************75%***********            ]  6 of 8 completed\n[**********************88%*****************      ]  7 of 8 completed\n[*********************100%***********************]  8 of 8 completed\n\n\nCode\nportfolio_prices.dropna(inplace=True)\n\n# Renaming columns for better readability\nportfolio_prices.columns = [\n    \"corn_fut\",\n    \"wheat_fut\",\n    \"KCWheat_fut\",\n    \"rice_fut\",\n    \"Feeder_Cattle\",\n    \"soymeal_fut\",\n    \"soyF_fut\",\n    \"soybeans_fut\"\n]\n\n\nShowing the prices time series side by side: (data in level)\n\n\nCode\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal, theme_void\n\n# Preparar os dados no formato long (necessário para plotnine/ggplot2)\nportfolio_prices_long = portfolio_prices.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Price')\n\ndef plot_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Price', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\np_prices = plot_with_ggplot(portfolio_prices_long, 'Commodity Prices Over Time', 'Price', background='white', fig_height=14, fig_width=8)\n\np_prices\n\n\n&lt;string&gt;:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1400)&gt;\n\n\n\n\n\n\n\n\n\n\n\nObtain the returns time series (first feature):\n\\[\n\\mbox{Price log returns}_t = ln(p_t) - ln(p_{t-1})\n\\]\n\n\nCode\n\n# Calculate log returns\nportfolio_log_returns = np.log(portfolio_prices / portfolio_prices.shift(1)).dropna()\nportfolio_log_returns.columns = [\n    \"ret_corn_fut\",\n    \"ret_wheat_fut\",\n    \"ret_KCWheat_fut\",\n    \"ret_rice_fut\",\n    \"ret_Feeder_Cattle\",\n    \"ret_soymeal_fut\",\n    \"ret_soyF_fut\",\n    \"ret_soybeans_fut\"\n]\n\n\nAnd plot it:\n\n\nCode\n# Preparar os dados no formato long para os log-retornos\nportfolio_log_returns_long = portfolio_log_returns.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Log Return')\n\ndef plot_log_returns_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Log Return', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\np_log_returns = plot_log_returns_with_ggplot(portfolio_log_returns_long, 'Log Returns of Commodities Over Time', 'Log Return', background='white', fig_height=12, fig_width=8)\n\n# Exibir o gráfico\np_log_returns\n\n\n&lt;string&gt;:3: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1200)&gt;\n\n\n\n\n\n\n\n\n\n\n\nAs risk measure, we use the conditional variances (volatilities), to deal better with day by day of the prices log-returns.\nThe GARCH(1,1) model with an asymmetric Student-t distribution is not directly available in most Python libraries. However, we can still use a GARCH(1,1) model with a standard Student-t distribution to estimate the conditional variance. The GARCH(1,1) model is represented as follows:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(r_t\\) is the log-return at time \\(t\\).\n\\(\\mu\\) is the mean of the returns.\n\\(\\epsilon_t\\) is the error term, modeled as conditional on past information.\n\\(\\sigma_t^2\\) is the conditional variance at time \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) are the parameters to be estimated, with \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) follows a Student-t distribution with \\(\\nu\\) degrees of freedom to capture the heavy tails observed in financial returns.\n\n\n\nCode\n# Initialize an empty DataFrame to store conditional variances\ncond_variances = pd.DataFrame(index=portfolio_log_returns.index, columns=portfolio_log_returns.columns)\n\n# Loop through each commodity's log-returns and fit a GARCH(1,1) model\nfor col in portfolio_log_returns.columns:\n    # Fit a GARCH(1,1) model with a Student-t distribution for each series of log returns\n    model = arch_model(portfolio_log_returns[col], vol='Garch', p=1, q=1, dist='t')\n    res = model.fit(disp='off')\n    \n    # Extract conditional variances and store them in the DataFrame\n    cond_variances[col] = res.conditional_volatility\n\n# Show the first few rows of the conditional variances DataFrame\ncond_variances.head()\n\n\n                           ret_corn_fut  ...  ret_soybeans_fut\nDate                                     ...                  \n2019-01-03 00:00:00+00:00      0.007584  ...          0.007990\n2019-01-04 00:00:00+00:00      9.242537  ...          0.008047\n2019-01-07 00:00:00+00:00      9.472461  ...          0.008299\n2019-01-08 00:00:00+00:00      9.432000  ...          0.008250\n2019-01-09 00:00:00+00:00      9.588762  ...          0.008319\n\n[5 rows x 8 columns]\n\n\nand visualizing them:\n\n\nCode\n# Preparar os dados no formato long para as variâncias condicionais\ncond_variances_long = cond_variances.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Conditional Variance')\n\n# Função para criar o gráfico com fundo branco ou transparente e ajustar o tamanho da figura\ndef plot_cond_variances_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Conditional Variance', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\n# Exemplo de uso para as variâncias condicionais das commodities\np_cond_variances = plot_cond_variances_with_ggplot(cond_variances_long, 'Conditional Variances Over Time (GARCH(1,1))', 'Conditional Variance', background='white', fig_height=12, fig_width=8)\n\np_cond_variances\n\n\n&lt;string&gt;:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1200)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR codes\n\nR packagesPortfolio set\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n#library(plotly)\nlibrary(rugarch)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(caTools)\nlibrary(PerformanceAnalytics)\nlibrary(MASS)\nlibrary(PortfolioAnalytics)\nlibrary(ROI)\nrequire(ROI.plugin.glpk)\nrequire(ROI.plugin.quadprog)\nlibrary(quadprog)\nlibrary(corpcor)\nlibrary(DEoptim)\nlibrary(cowplot) # devtools::install_github(\"wilkelab/cowplot/\")\nlibrary(lattice)\nlibrary(timetk)\n\n\n\n\nLoading time series data, for portfolio setting…\n\n\nCode\ntickers &lt;- c(\n         \"ZC=F\", # Corn Futures\n         \"ZO=F\", # Wheat Futures\n         \"KE=F\", # Futuros KC HRW Wheat Futures\n         \"ZR=F\", # Rough Rice Futures\n         \"GF=F\", # Feeder Cattle Futures\n         \"ZS=F\", # SoyMeal Futures \n         \"ZM=F\", # Futuros farelo soja\n         \"ZL=F\"  # SoyBeans Futures\n)\n\n\nObtain daily prices and their returns:\n\n\nCode\nportfolioPrices &lt;- NULL\n  for ( Ticker in tickers )\n    portfolioPrices &lt;- cbind(\n      portfolioPrices, \n      getSymbols.yahoo(\n        Ticker,\n        from = \"2019-01-01\",\n        auto.assign = FALSE\n      )[,4]\n    )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"corn_fut\",\n  \"wheat_fut\",\n  \"KCWheat_fut\",\n  \"rice_fut\",\n  \"Feeder_Cattle\",\n  \"soymeal_fut\",\n  \"soyF_fut\",\n  \"soybeans_fut\"\n)\n\ntail(portfolioPrices)\n\n\n           corn_fut wheat_fut KCWheat_fut rice_fut Feeder_Cattle soymeal_fut\n2025-01-29   497.00    350.00      580.25   1413.0       280.550     1060.50\n2025-01-30   490.25    349.25      588.25   1393.5       281.900     1044.00\n2025-01-31   482.00    347.50      579.25   1384.5       275.725     1042.00\n2025-02-03   488.75    353.00      585.75   1373.0       270.500     1058.25\n2025-02-04   494.50    359.25      594.75   1355.0       268.250     1075.00\n2025-02-05   493.25    367.75      591.75   1350.0       270.725     1057.00\n           soyF_fut soybeans_fut\n2025-01-29    309.8        44.97\n2025-01-30    304.7        44.98\n2025-01-31    301.1        46.11\n2025-02-03    303.7        46.51\n2025-02-04    314.0        45.76\n2025-02-05    308.3        45.09\n\n\nPlotting the time series prices (in level):\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along( corn_fut )\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\nObtain the returns time series (first feature):\n\\[\n\\mbox{Price log returns}_t = ln(p_t) - ln(p_{t-1})\n\\]\n\n\nCode\n# Calculate log returns for the portfolio prices\nportfolioReturs &lt;- na.omit(diff(log(portfolioPrices))) |&gt; as.data.frame()\n\ncolnames(portfolioReturs) &lt;- c(\n  \"ret_corn_fut\",\n  \"ret_wheat_fut\",\n  \"ret_KCWheat_fut\",\n  \"ret_rice_fut\",\n  \"ret_Feeder_Cattle\",\n  \"ret_soymeal_fut\",\n  \"ret_soyF_fut\",\n  \"ret_soybeans_fut\"\n)\n\nglimpse(portfolioReturs)\n\n\nRows: 1,533\nColumns: 8\n$ ret_corn_fut      &lt;dbl&gt; 0.0105891128, 0.0085218477, -0.0019601444, -0.005903…\n$ ret_wheat_fut     &lt;dbl&gt; 0.0008980692, 0.0053715438, -0.0017873106, 0.0115608…\n$ ret_KCWheat_fut   &lt;dbl&gt; 0.0220892515, 0.0049529571, -0.0059464992, 0.0039682…\n$ ret_rice_fut      &lt;dbl&gt; 0.0083930387, 0.0053934919, 0.0174507579, 0.00813596…\n$ ret_Feeder_Cattle &lt;dbl&gt; -0.0096783375, -0.0111522135, 0.0075628145, 0.011068…\n$ ret_soymeal_fut   &lt;dbl&gt; 0.0061281529, 0.0102224954, 0.0030190774, -0.0065988…\n$ ret_soyF_fut      &lt;dbl&gt; 0.0054513913, 0.0076457646, 0.0097900861, -0.0018874…\n$ ret_soybeans_fut  &lt;dbl&gt; 0.0099858421, 0.0081286732, -0.0052938051, -0.002834…\n\n\nCode\n#portfolioReturs &lt;- as.timeSeries(portfolioReturs)\n\n\nPlot all time series and their returns:\n\n\nCode\nportfolioReturs |&gt; \n  mutate(\n    time = seq_along( ret_corn_fut )\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\nPlotting the histograms:\n\n\nCode\nportfolioPrices_df &lt;- as_tibble(portfolioPrices, rownames = \"date\")\nportfolioPrices_df$date &lt;- ymd(portfolioPrices_df$date)\n\nportfolioReturs_df &lt;- na.omit( ROC( portfolioPrices ), type = \"discrete\" ) |&gt;\n  as_tibble(rownames = \"date\")\nportfolioReturs_df$date &lt;- ymd(portfolioReturs_df$date)\ncolnames(portfolioReturs_df) &lt;- c(\n  \"date\",\n  \"ret_corn_fut\",\n  \"ret_wheat_fut\",\n  \"ret_KCWheat_fut\",\n  \"ret_rice_fut\",\n  \"ret_Feeder_Cattle\",\n  \"ret_soymeal_fut\",\n  \"ret_soyF_fut\",\n  \"ret_soybeans_fut\"\n)\n\n# Remover a coluna com nome NA\nportfolioReturs_df &lt;- portfolioReturs_df[, !is.na(colnames(portfolioReturs_df))]\n\n# Verificar novamente os nomes das colunas para garantir que estão corretos\ncolnames(portfolioReturs_df)\n\n\n[1] \"date\"              \"ret_corn_fut\"      \"ret_wheat_fut\"    \n[4] \"ret_KCWheat_fut\"   \"ret_rice_fut\"      \"ret_Feeder_Cattle\"\n[7] \"ret_soymeal_fut\"   \"ret_soyF_fut\"      \"ret_soybeans_fut\" \n\n\nCode\nportfolioReturs_long &lt;- portfolioReturs_df |&gt; \n  pivot_longer(\n    cols = -date, # Exclui a coluna de data\n    names_to = \"fut_type\", \n    values_to = \"returns\"\n  )\n\nggplot(portfolioReturs_long, aes(x = returns)) + \n  geom_histogram(aes(y = ..density..), binwidth = .01, color = \"black\", fill = \"white\") +\n  geom_density(alpha = .2, fill=\"lightgray\") +\n  theme_minimal() +\n  theme(\n    axis.line  = element_line(colour = \"black\"),\n    axis.text  = element_text(colour = \"black\"),  \n    axis.ticks = element_line(colour = \"black\"), \n    legend.position = c(.1,.9), \n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank()\n  ) +\n  theme(plot.title   = element_text(size = 10),  \n        axis.title.x = element_text(size = 7), \n        axis.title.y = element_text(size = 7)) + \n  labs(x = \"Returns\", y = \"Density\") +\n  facet_wrap(~fut_type, scales = \"free\", ncol = 2) \n\n\n\n\n\n\n\n\n\nAnd finnaly, the last feature, is called, the conditional variance (risk measure), obtained by GARCH(1,1) model, formalized as:\nThe GARCH(1,1) model with asymmetric Student-t distribution can be represented mathematically as:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(r_t\\) is the return at time \\(t\\).\n\\(\\mu\\) is the mean of the returns.\n\\(\\epsilon_t\\) is the error term, modeled as conditional on past information.\n\\(\\sigma_t^2\\) is the conditional variance at time \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) are the parameters to be estimated, with \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) follows an asymmetric Student-t distribution with \\(\\nu\\) degrees of freedom to better capture the heavy tails and skewness observed in financial returns.\n\n\n\nCode\n# Load necessary packages\nlibrary(rugarch)\n\n# Define the GARCH(1,1) model specification with Student-t distribution\nspec &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n  mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),\n  distribution.model = \"std\" # Using Student-t distribution\n)\n\n# Estimate the model for each asset in the portfolio and extract conditional variances\ngarch_models &lt;- list()\nconditional_variances &lt;- list()\n\nfor (i in colnames(portfolioReturs)) {\n  garch_models[[i]] &lt;- ugarchfit(spec, data = portfolioReturs[[i]])\n  conditional_variances[[i]] &lt;- sigma(garch_models[[i]])^2\n}\n\n# Convert conditional variances list to a data frame\nconditional_variances_df &lt;- do.call(cbind, conditional_variances) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(time = seq_along(conditional_variances[[1]]))\n\ncolnames(conditional_variances_df) &lt;- c(\n  \"cond_var_corn_fut\",\n  \"cond_var_wheat_fut\",\n  \"cond_var_KCWheat_fut\",\n  \"cond_var_rice_fut\",\n  \"cond_var_Feeder_Cattle\",\n  \"cond_var_soymeal_fut\",\n  \"cond_var_soyF_fut\",\n  \"cond_var_soybeans_fut\",\n  \"time\"\n)\n\n# Reshape data for plotting\nconditional_variances_long &lt;- conditional_variances_df %&gt;%\n  pivot_longer(!time, names_to = \"Variables\", values_to = \"Value\")\n\n\nAnd the plot of the conditional variance (risk):\n\n\nCode\nconditional_variances_long |&gt; \n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\nReferences\nGujarati, D., N. (2004) Basic Econometrics, fourth edition, The McGraw−Hill Companies\nHair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2019). Multivariate Data Analysis. Pearson.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on oct 2023.\n \n \n\n\n\nCode\n# Total timing to compile this Quarto document\n\nend_time = datetime.now()\ntime_diff = end_time - start_time\n\nprint(f\"Total Quarto document compiling time: {time_diff}\")\n\n\nTotal Quarto document compiling time: 0:00:18.944925",
    "crumbs": [
      "About",
      "Predictive Models",
      "Data Extraction for preselected commodities portfolio"
    ]
  },
  {
    "objectID": "mindmap.html",
    "href": "mindmap.html",
    "title": "The Research Mindmap",
    "section": "",
    "text": "Overview\nThe following flowchart illustrates the research workflow, detailing the key methodologies and how they are interconnected. It starts from the problem identification and moves through volatility modeling, predictive modeling, and optimization processes.\n\n\n\n\n\n\n\n\n\n\n\nResearch Proposal: Advanced Techniques for Multiperiod Multiobjective Portfolio Optimization in Commodity Markets\nThis research addresses the significant challenge of modeling and forecasting agricultural commodity prices, which are subject to high volatility and complex dynamics. Agricultural markets are highly sensitive to external factors such as climatic changes, geopolitical events, and supply-demand imbalances, making accurate forecasting and risk management difficult for investors and policymakers.\n\nKey Components of the Research:\n\nVolatility Modeling Using GAMLSS and MSGARCH:\n\nGAMLSS: This technique provides a nuanced understanding of the distributional characteristics of commodity returns, capturing the probabilistic behaviors that traditional models often overlook.\nMSGARCH: By implementing the Markov-Switching GARCH model, the research captures regime shifts in volatility, which are common in commodities due to external shocks and systemic changes.\n\nMulti-Objective Portfolio Optimization:\n\nThis step involves developing a multi-objective optimization framework using evolutionary algorithms such as NSGA-II and Differential Evolution (DEOptim). These algorithms help optimize portfolio allocation by balancing risk, return, and diversification, particularly for portfolios with high-volatility assets like agricultural commodities.\n\nReinforcement Learning for Portfolio Management:\n\nThe research also introduces Reinforcement Learning (RL) methods, such as Q-Learning and K-Bandit algorithms, to adaptively manage portfolio strategies. These techniques are particularly suited for dynamic portfolio management, allowing strategies to evolve as market conditions change.\n\n\n\n\nContribution to Knowledge:\nThe research’s innovative contribution lies in combining these advanced econometric and machine learning techniques to tackle the unique challenges of commodity markets. It offers a comprehensive methodological framework that improves the modeling and forecasting of volatility and returns in agricultural commodities. This work enhances portfolio optimization strategies, offering practical applications for financial markets by providing tools that help portfolio managers make informed, data-driven decisions in the face of volatile market conditions.\nFinal Objective: The primary goal of this research is to develop robust methods for volatility modeling and portfolio optimization that dynamically adapt to market conditions. This approach offers a significant advancement in both academic and professional fields by providing actionable insights for managing portfolios in volatile commodity markets.\nHere we can see an much more complete and interactive mindmap: (if the HTML doesn´t work, see the pic below)\n\nAnother mindmap tryed to be made with networkD3\n\n\nCode\n# Libraries\nlibrary(networkD3)\nlibrary(dplyr)\n\n# Create the dataset (edges) for the mind map\ndata &lt;- data.frame(\n  from = c(\n    # Edges from the root node\n    \"PhD Thesis Proposal\", \"PhD Thesis Proposal\", \"PhD Thesis Proposal\", \n    \"PhD Thesis Proposal\", \"PhD Thesis Proposal\", \"PhD Thesis Proposal\", \"PhD Thesis Proposal\",\n    \n    # Edges under \"Research Motivation\"\n    \"Research Motivation\", \"Research Motivation\",\n    \n    # Edges under \"Methodological Framework\"\n    \"Methodological Framework\", \"Methodological Framework\",\n    \n    # Edges under \"Econometric Modeling Techniques & Volatility Modeling Approaches\"\n    \"Econometric Modeling Techniques & Volatility Modeling Approaches\", \"Econometric Modeling Techniques & Volatility Modeling Approaches\",\n    \n    # Edges under \"Portfolio Optimization & Allocation\"\n    \"Portfolio Optimization & Allocation\", \"Portfolio Optimization & Allocation\",\n    \n    # Edges under \"Multi-Objective Portfolio Optimization Framework\"\n    \"Multi-Objective Portfolio Optimization Framework\", \"Multi-Objective Portfolio Optimization Framework\",\n    \n    # Edges under \"Optimization Algorithms\"\n    \"Optimization Algorithms\", \"Optimization Algorithms\",\n    \n    # Edges under \"Expected Contributions\"\n    \"Expected Contributions\", \"Expected Contributions\",\n    \n    # Edges under \"Work Plan & Timeline\"\n    \"Work Plan & Timeline\", \"Work Plan & Timeline\", \"Work Plan & Timeline\", \n    \"Work Plan & Timeline\", \"Work Plan & Timeline\", \"Work Plan & Timeline\"\n  ),\n  to = c(\n    # Children of \"PhD Thesis Proposal\"\n    \"Research Motivation\", \"Methodological Framework\", \"Dynamic Programming for Multi-Period Optimization\", \n    \"Empirical Validation\", \"Practical Guidelines\", \"Expected Contributions\", \"Work Plan & Timeline\",\n    \n    # Children of \"Research Motivation\"\n    \"Challenges in Commodity Markets\", \"Need for Improved Forecasting & Risk Management\",\n    \n    # Children of \"Methodological Framework\"\n    \"Econometric Modeling Techniques & Volatility Modeling Approaches\", \"Portfolio Optimization & Allocation\",\n    \n    # Children of \"Econometric Modeling Techniques & Volatility Modeling Approaches\"\n    \"Distributional Analysis (GAMLSS Framework)\", \"Volatility & Regime Switching (MSGARCH Models)\",\n    \n    # Children of \"Portfolio Optimization & Allocation\"\n    \"Reinforcement Learning for Allocation (K-Bandit & Q-Learning)\", \"Multi-Objective Portfolio Optimization Framework\",\n    \n    # Children of \"Multi-Objective Portfolio Optimization Framework\"\n    \"Objective Functions (Return, Risk, Entropy)\", \"Optimization Algorithms\",\n    \n    # Children of \"Optimization Algorithms\"\n    \"Differential Evolution (DE)\", \"NSGA-II\",\n    \n    # Children of \"Expected Contributions\"\n    \"Advances in Volatility Modeling\", \"Innovative Portfolio Optimization\",\n    \n    # Children of \"Work Plan & Timeline\"\n    \"1-3 Months: Data Collection\", \"4-6 Months: Develop GAMLSS & MSGARCH\", \n    \"7-9 Months: Model Comparison & Optimization Framework\", \"10-12 Months: Integrate Reinforcement Learning\", \n    \"13-15 Months: Empirical Validation\", \"16-18 Months: Final Analysis & Thesis\"\n  ),\n  stringsAsFactors = FALSE\n)\n\n# Plot the network using simpleNetwork from networkD3\nsimpleNetwork(data, \n              height = \"800px\", \n              width = \"100%\", \n              Source = 1,      # column number for source nodes\n              Target = 2,      # column number for target nodes\n              linkDistance = 100,  # adjust as needed for spacing\n              charge = -300,       # node repulsion (tweak for layout)\n              fontSize = 14,       # font size for node labels\n              fontFamily = \"serif\",\n              linkColour = \"#666\", # color of the links\n              nodeColour = \"#69b3a2\",  # color of the nodes\n              opacity = 0.9,       # overall opacity\n              zoom = TRUE          # allow zooming\n)",
    "crumbs": [
      "About",
      "Intro",
      "The Research Mindmap"
    ]
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#data-collection-and-preprocessing",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#data-collection-and-preprocessing",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Data Collection and Preprocessing",
    "text": "Data Collection and Preprocessing\nPython libs\n\n\nCode\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, Dense, Flatten, MaxPooling1D, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport time\nimport warnings\nimport psutil\nimport os\nfrom keras_tuner import RandomSearch\n\nwarnings.filterwarnings('ignore')\n\n\nFirst of all, we need to check the hardware availability:\n\n\nCode\n# Collecting hardware information\ndef get_system_info():\n    return {\n        'CPU_cores': psutil.cpu_count(logical=True),\n        'CPU_freq_MHz': psutil.cpu_freq().current,\n        'Total_RAM_GB': round(psutil.virtual_memory().total / (1024 ** 3), 2),\n        'Available_RAM_GB': round(psutil.virtual_memory().available / (1024 ** 3), 2),\n    }\n\nsystem_info = get_system_info()\nprint(\"System Information:\", system_info)\n\n\nSystem Information: {'CPU_cores': 12, 'CPU_freq_MHz': 1800.0, 'Total_RAM_GB': 31.69, 'Available_RAM_GB': 15.36}\n\n\nLoading corn time series data:\n\n\nCode\n# Helper function to prepare LSTM input-output pairs\ndef prepare_data(series, time_steps):\n    X, y = [], []\n    for i in range(len(series) - time_steps):\n        X.append(series[i:(i + time_steps)])\n        y.append(series[i + time_steps][0])  # Extract only the target variable (returns)\n    return np.array(X), np.array(y)\n\n# Define ticker for Corn Futures\nticker = \"ZC=F\"\n\n# Downloading price data\ndata = yf.download(ticker, start=\"2015-01-01\")[[\"Close\"]].fillna(method='ffill').dropna()\n\n# Calculating logarithmic returns\nreturns = np.log(data / data.shift(1)).dropna()\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\nCNN+LSTM preprocessing step:\nNormalizing time series, to deal much better with outliers via QuantileTransformer function:\n\n\nCode\n# Normalizing the data using QuantileTransformer to handle outliers\nscaler = QuantileTransformer(output_distribution='normal')\nscaled_data = scaler.fit_transform(returns)\n\n# Preparing the dataset\ntime_steps = 30\nX, y = prepare_data(scaled_data, time_steps)"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#train-x-test-split",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#train-x-test-split",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Train x test split",
    "text": "Train x test split\nWe use 80% for train data x 20% test, and then we can reshape the data (samples, time steps and features)\n\n\nCode\n# Splitting the data into training and testing sets\nsplit_ratio = 0.8\ntrain_size  = int(len(X) * split_ratio)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# Reshaping the input to be 3D [samples, time steps, features]\nX_train = np.expand_dims(X_train, axis=-1)\nX_test = np.expand_dims(X_test, axis=-1)"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#cnnlstm-setting",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#cnnlstm-setting",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "CNN+LSTM setting",
    "text": "CNN+LSTM setting\nWe can set an grid search for the CNN+LSTM time series forecasting\n\n\nCode\n# Building the CNN+LSTM model with additional GRU layer and Dropout for regularization\ndef build_model(include_conv=True, conv_filters=64, include_lstm=True, lstm_units=50, gru_units=0, activation='relu', include_dropout=True, dropout_rate=0.1, l2_regularization=0.01):\n    model = Sequential()\n    if include_conv:\n        model.add(Conv1D(filters=conv_filters, kernel_size=3, activation=activation, kernel_regularizer=l2(l2_regularization), input_shape=(time_steps, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n    if include_lstm:\n        model.add(LSTM(lstm_units, activation=activation, kernel_regularizer=l2(l2_regularization), return_sequences=True if gru_units &gt; 0 else False))\n    if gru_units &gt; 0:\n        model.add(GRU(gru_units, activation=activation, kernel_regularizer=l2(l2_regularization), return_sequences=False))\n    model.add(Flatten())\n    model.add(Dense(50, activation=activation, kernel_regularizer=l2(l2_regularization)))\n    if include_dropout:\n        model.add(Dropout(dropout_rate))  # Added dropout layer to reduce overfitting\n    model.add(Dense(1, kernel_regularizer=l2(l2_regularization)))\n    return model\n\n\nThen we can set the Adam optimizer and the early stopping for the model:\n\n\nCode\n# Defining optimizer and callbacks\ndef get_optimizer():\n    return Adam(learning_rate=0.001)\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n\nBy using the Keras tunner, we can set the hyperparameters tunning:\n\n\nCode\n# Hyperparameter search using Keras Tuner\ndef hyperparameter_search():\n    def build_model_kt(hp):\n        model = Sequential()\n        include_conv = hp.Boolean('include_conv')\n        if include_conv:\n            conv_filters = hp.Choice('conv_filters', values=[32, 64, 128])\n            model.add(Conv1D(filters=conv_filters, kernel_size=3, activation='relu', input_shape=(time_steps, 1)))\n            model.add(MaxPooling1D(pool_size=2))\n        \n        include_lstm = hp.Boolean('include_lstm')\n        if include_lstm:\n            lstm_units = hp.Choice('lstm_units', values=[50, 100])\n            model.add(LSTM(lstm_units, activation='relu', return_sequences=True))\n        \n        include_gru = hp.Boolean('include_gru')\n        if include_gru:\n            gru_units = hp.Choice('gru_units', values=[50, 100])\n            model.add(GRU(gru_units, activation='relu', return_sequences=False))\n        \n        model.add(Flatten())\n        model.add(Dense(50, activation='relu'))\n        \n        include_dropout = hp.Boolean('include_dropout')\n        if include_dropout:\n            dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n            model.add(Dropout(dropout_rate))\n        \n        model.add(Dense(1))\n        \n        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n        return model\n\n    tuner = RandomSearch(\n        build_model_kt,\n        objective='val_mse',\n        max_trials=10,\n        executions_per_trial=1,\n        directory='my_dir',\n        project_name='corn_forecast_tuning'\n    )\n\n    tuner.search(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n    return best_hps\n\n# Running hyperparameter search\nbest_params = hyperparameter_search()\n\n\nReloading Tuner from my_dir\\corn_forecast_tuning\\tuner0.json"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#setting-the-best-model-before-hyp-tunning",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#setting-the-best-model-before-hyp-tunning",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Setting the best model before hyp tunning:",
    "text": "Setting the best model before hyp tunning:\n\n\nCode\ndef build_best_model(best_params):\n    model = Sequential()\n    \n    # Adiciona camada convolucional se ativada\n    if 'include_conv' in best_params.values and best_params.values['include_conv']:\n        conv_filters = best_params.values['conv_filters']\n        model.add(Conv1D(filters=conv_filters, kernel_size=3, activation='relu', input_shape=(time_steps, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n    \n    # Adiciona camada LSTM se ativada\n    if 'include_lstm' in best_params.values and best_params.values['include_lstm']:\n        lstm_units = best_params.values['lstm_units']\n        return_sequences = True if 'include_gru' in best_params.values and best_params.values['include_gru'] else False\n        model.add(LSTM(lstm_units, activation='relu', return_sequences=return_sequences))\n    \n    # Adiciona camada GRU se ativada\n    if 'include_gru' in best_params.values and best_params.values['include_gru']:\n        gru_units = best_params.values['gru_units']\n        model.add(GRU(gru_units, activation='relu', return_sequences=False))\n    \n    # Camadas adicionais\n    model.add(Flatten())\n    model.add(Dense(50, activation='relu'))\n    \n    # Adiciona camada Dropout se ativada\n    if 'include_dropout' in best_params.values and best_params.values['include_dropout']:\n        dropout_rate = best_params.values.get('dropout_rate', 0.1)\n        model.add(Dropout(dropout_rate))\n    \n    # Camada de saída\n    model.add(Dense(1))\n    \n    return model\n\n\nAnd then the history fit:\n\n\nCode\n# Certifique-se de que best_params foi definido corretamente\nprint(\"Best hyperparameters:\", best_params.values)\n\n# Construa o modelo\nmodel = build_best_model(best_params)\n\n# Compile o modelo\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n# Confirme que o modelo foi construído corretamente\nprint(\"Model summary:\")\nmodel.summary()\n\n# Treine o modelo\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n\nBest hyperparameters: {'include_conv': True, 'include_lstm': False, 'include_gru': True, 'include_dropout': False, 'conv_filters': 64, 'lstm_units': 50, 'dropout_rate': 0.30000000000000004, 'gru_units': 50}\nModel summary:\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d (Conv1D)                 │ (None, 28, 64)         │           256 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d (MaxPooling1D)    │ (None, 14, 64)         │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru (GRU)                       │ (None, 50)             │        17,400 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 50)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 50)             │         2,550 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │            51 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 20,257 (79.13 KB)\n\n\n\n Trainable params: 20,257 (79.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nEpoch 1/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1:08 1s/step - loss: 1.2850 2/63 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - loss: 1.1999\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.0864 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0585\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 1.0578 - val_loss: 1.0375\nEpoch 2/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.8427\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8980 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9371\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9514\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9630 - val_loss: 1.0372\nEpoch 3/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.9068\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9385 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9824 - val_loss: 1.0400\nEpoch 4/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.9736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0295 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0240\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0100\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9951 - val_loss: 1.0337\nEpoch 5/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: 0.3063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8799 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9158\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9287\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b60/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9392\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9431 - val_loss: 1.0317\nEpoch 6/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.5503\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.1716 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.1285\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0997\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0702 - val_loss: 1.0346\nEpoch 7/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 0.8511\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9834 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0058 - val_loss: 1.0328\nEpoch 8/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.6796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0799 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0338\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b56/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0098\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0085 - val_loss: 1.0245\nEpoch 9/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.8550\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9303 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9423\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9524\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b56/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9630\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9673 - val_loss: 1.0267\nEpoch 10/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: 0.9402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9745 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9730\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9552\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9683 - val_loss: 1.0581\nEpoch 11/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.6234\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0699 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0204\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9977\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9959 - val_loss: 1.0361\nEpoch 12/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.0651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.0411 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0439\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0378\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.0222 - val_loss: 1.0432\nEpoch 13/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.8871\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0297 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0144\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9788\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b61/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9748 - val_loss: 1.0658\nEpoch 14/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.0064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9454 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9841\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9793 - val_loss: 1.0207\nEpoch 15/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.9187\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9735 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9779\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9700\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9703 - val_loss: 1.0157\nEpoch 16/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - loss: 0.7292\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7873 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8567\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8830\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b62/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9162 - val_loss: 1.0800\nEpoch 17/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 0.8483\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7909 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8824\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9047 - val_loss: 1.0194\nEpoch 18/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 1.1324\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9649 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9571\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9547 - val_loss: 1.0270\nEpoch 19/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 0.7409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8275 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8665\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9146 - val_loss: 1.0140\nEpoch 20/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.9388\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9421 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9268\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9176 - val_loss: 1.0415\nEpoch 21/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.0534\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0351 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9509 - val_loss: 1.1168\nEpoch 22/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.0725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0047 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9163\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9151\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9150 - val_loss: 1.0167\nEpoch 23/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.1516\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0398 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9630\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b62/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9524\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9506 - val_loss: 1.0923\nEpoch 24/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.4731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.7419 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7905\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8176\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b46/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8290\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8377\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.8405 - val_loss: 1.0485\nEpoch 25/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - loss: 0.9536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8086 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8167\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8237\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8270 - val_loss: 1.0314\nEpoch 26/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 0.9687\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9127 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8774\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8736 - val_loss: 1.0872\nEpoch 27/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - loss: 0.5574\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.7478 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7450\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b46/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7692\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7776\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.7845 - val_loss: 1.0884\nEpoch 28/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.6348\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.6944 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7172\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.7411\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.7613\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7723 - val_loss: 1.2496\nEpoch 29/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.7269\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8679 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8459\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8374\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8332 - val_loss: 1.1209"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#model-backtest",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#model-backtest",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Model backtest",
    "text": "Model backtest\n\n\nCode\n# Predicting and evaluating the model\ny_pred = model.predict(X_test)\ny_test_inverse = scaler.inverse_transform(y_test.reshape(-1, 1))\ny_pred_inverse = scaler.inverse_transform(y_pred)\n\nmse = mean_squared_error(y_test_inverse, y_pred_inverse)\nmape = mean_absolute_percentage_error(y_test_inverse, y_pred_inverse) * 100  # Convert to percentage\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"Mean Absolute Percentage Error: {mape}%\")\n\n\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 133ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\nMean Squared Error: 0.0002890161366973252\nMean Absolute Percentage Error: 3200378914379.584%\n\n\nAblation Study: Evaluating the Impact of Each Layer\nThe ablation study aims to assess the influence of different architectural components and configurations on the performance of the CNN+LSTM model for time series forecasting. By systematically altering specific layers and hyperparameters, we gain insights into the contribution of each element to the overall model accuracy and robustness.\n\nConfigurations\nThe following configurations were tested as part of the ablation study:\n\nConfiguration 1:\n\nConvolutional layer included with 64 filters.\nLSTM layer included with 50 units.\nGRU layer included with 50 units.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 2:\n\nConvolutional layer included with 32 filters.\nLSTM layer included with 50 units.\nGRU layer excluded.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 3:\n\nConvolutional layer included with 128 filters.\nLSTM layer included with 50 units.\nGRU layer included with 50 units.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 4:\n\nConvolutional layer included with 64 filters.\nLSTM layer excluded.\nGRU layer included with 50 units.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 5:\n\nConvolutional layer excluded.\nLSTM layer included with 50 units.\nGRU layer excluded.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 6:\n\nConvolutional layer included with 64 filters.\nLSTM layer included with 50 units.\nGRU layer included with 50 units.\nActivation function: Tanh.\nDropout layer included.\n\n\nFor each configuration, the model was:\n\nCompiled with the Adam optimizer and mean squared error (MSE) loss function.\nTrained on the dataset with early stopping to prevent overfitting.\nEvaluated on the test set to calculate the mean squared error (MSE) and mean absolute percentage error (MAPE).\n\n\n\nCode\n# Ablation Study: Evaluating the impact of each layer\ndef ablation_study():\n    results = []\n    configurations = [\n        {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': True, 'conv_filters': 32, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': True, 'conv_filters': 128, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': True, 'conv_filters': 64, 'include_lstm': False, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': False, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'tanh', 'include_dropout': True},\n    ]\n    for config in configurations:\n        optimizer = get_optimizer()  # Recreate optimizer for each configuration\n        model = build_model(**config)\n        model.build(input_shape=(None, time_steps, 1))  # Explicitly build the model with correct input shape\n        model.compile(optimizer=optimizer, loss='mse')\n        model.fit(\n            X_train, y_train,\n            epochs=50,\n            batch_size=32,\n            validation_data=(X_test, y_test),\n            callbacks=[early_stopping],\n            verbose=0\n        )\n        y_pred = model.predict(X_test)\n        y_pred_inverse = scaler.inverse_transform(y_pred)\n        mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n        mape = mean_absolute_percentage_error(y_test_inverse, y_pred_inverse) * 100  # Convert to percentage\n        results.append({'config': config, 'mse': mse, 'mape': mape})\n    return results\n\ndef generate_ablation_report(results):\n    report = \"\\nAblation Study Report\\n\"\n    report += \"===================\\n\"\n    for result in results:\n        report += (f\"Configuration: {result['config']}, MSE: {result['mse']}, MAPE: {result['mape']}%\\n\")\n    report += \"===================\\n\"\n    return report\n\nablation_results = ablation_study()\n\nprint(generate_ablation_report(ablation_results))\n\n\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 175ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 177ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 179ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 100ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 180ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\n\nAblation Study Report\n===================\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}, MSE: 0.0002981824980357916, MAPE: 99.20318725099602%\nConfiguration: {'include_conv': True, 'conv_filters': 32, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True}, MSE: 0.00029739035256384154, MAPE: 415885659688.06445%\nConfiguration: {'include_conv': True, 'conv_filters': 128, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}, MSE: 0.00029949885368034694, MAPE: 2521000988657.1562%\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': False, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}, MSE: 0.00029937068511983806, MAPE: 2454989630722.6016%\nConfiguration: {'include_conv': False, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True}, MSE: 0.00029963638029014024, MAPE: 2339832904717.413%\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'tanh', 'include_dropout': True}, MSE: 0.0002986295145228721, MAPE: 305550686135.86865%\n===================\n\n\n\nThe results of the ablation study include detailed performance metrics for each configuration, providing a quantitative basis for identifying the most impactful layers and hyperparameters.\nA summary of the ablation results is presented in the report generated by the following function:"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#final-analysis-ablation-study-report",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#final-analysis-ablation-study-report",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Final Analysis: Ablation Study Report",
    "text": "Final Analysis: Ablation Study Report\nThe ablation study was conducted to evaluate the impact of different architectural configurations on the CNN+LSTM model’s performance in time series forecasting. The results are summarized below, focusing on the Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE) for each configuration.\n\nKey Observations\n\nBest Configuration:\n\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000296357\nMAPE: 99.18%\n\nThis configuration achieved the lowest MSE and the lowest MAPE among all setups tested, indicating a strong balance between convolutional and recurrent layers with sufficient filters and neurons.\n\nPoor Generalization with Higher Filters:\n\nConfiguration: {'include_conv': True, 'conv_filters': 128, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000297815\nMAPE: 2460438667962.732%\n\nWhile MSE remained competitive, the extremely high MAPE indicates poor generalization, likely caused by overfitting due to the increased complexity of convolutional filters.\n\nEffect of Removing LSTM Layers:\n\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': False, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000296818\nMAPE: 545753800735.74896%\n\nRemoving LSTM layers slightly increased MSE and drastically impacted MAPE, demonstrating that the absence of temporal dependency modeling reduces the network’s ability to forecast accurately.\n\nEffect of Excluding Convolutional Layers:\n\nConfiguration: {'include_conv': False, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000297667\nMAPE: 2250906755637.2246%\n\nThe exclusion of convolutional layers increased both MSE and MAPE significantly, reinforcing the importance of feature extraction via convolution for improved performance.\n\nEffect of Activation Function Change:\n\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'tanh', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000297963\nMAPE: 2525076505075.051%\n\nThe use of tanh instead of relu led to a slight increase in both MSE and MAPE, suggesting that relu performs better for this dataset and task.\n\nLower Filter Configurations:\n\nConfiguration: {'include_conv': True, 'conv_filters': 32, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000298109\nMAPE: 2649331642167.829%\n\nWhile a lower filter count in convolutional layers reduced complexity, it also led to poorer performance in both MSE and MAPE.\n\n\n\n\nConclusion\nThe ablation study highlights the importance of balancing the network’s architecture for time series forecasting: - A combination of convolutional layers (conv_filters=64) and LSTM layers (lstm_units=50) enhanced temporal feature extraction while maintaining efficient generalization. - Over-complex architectures (e.g., excessive filters or layers) resulted in higher errors, likely due to overfitting. - Proper activation functions (relu) contributed to improved performance compared to alternative configurations.\nThe findings provide actionable insights for tuning CNN+LSTM models in future applications."
  },
  {
    "objectID": "resmindmap.html",
    "href": "resmindmap.html",
    "title": "Research proposal",
    "section": "",
    "text": "Advanced Techniques for Multiperiod Multiobjective Portfolio Optimization in Agricultural Commodity Markets:\nCombining Volatility Modeling and Reinforcement Learning and Exploring Strategic Asset Allocation",
    "crumbs": [
      "About",
      "Intro",
      "Research proposal"
    ]
  },
  {
    "objectID": "resmindmap.html#mindmap",
    "href": "resmindmap.html#mindmap",
    "title": "Research proposal",
    "section": "MindMap",
    "text": "MindMap\n\nAdvanced Techniques for Multiperiod Multiobjective Portfolio Optimization in Agricultural Commodity Markets:\nCombining Volatility Modeling and Reinforcement Learning and Exploring Strategic Asset Allocation",
    "crumbs": [
      "About",
      "Intro",
      "Research proposal"
    ]
  },
  {
    "objectID": "projects.html#project-1",
    "href": "projects.html#project-1",
    "title": "Projects",
    "section": "Project 1",
    "text": "Project 1\n\nOur first Colab Notebook  here\n\n This project is related with the step News and Impact on price and volatilities dynamics"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2\n This project aims to find how and why the corn prices flows down for the zero in 2024-jun-16.\n\nMERCADO GLOBAL DE ARROZ ENFRENTA FORTE VOLATILIDADE EM JUNHO DE 2024\nPreço do Arroz Marca Zero no Gráfico em 16 de Junho: Erro Técnico ou Anomalia no Mercado?\nO Yahoo Finance, uma plataforma amplamente utilizada para acessar dados financeiros, cotações em tempo real e gráficos históricos, apresentou um dado intrigante sobre as variações do preço do arroz. No dia 16 de junho de 2024, o preço do alimento foi registrado como zero. Essa informação incomum levanta questionamentos sobre sua origem, com duas possíveis explicações: um erro técnico na plataforma ou uma anomalia no mercado de commodities.\nErro Técnico\nEspecialistas sugerem que a hipótese de um erro técnico não deve ser descartada. Falhas em plataformas financeiras podem ocorrer por problemas no endpoint, parâmetros incorretos ou dificuldades relacionadas ao processamento de dados por meio de APIs. Vale destacar que a data em questão cai em um domingo, dia em que muitos mercados permanecem fechados, o que pode ter gerado inconsistências nos registros.\nAnomalia no Mercado de Commodities\nPor outro lado, o mercado de commodities agrícolas, como o arroz, vive constante volatilidade devido a fatores como mudanças climáticas, safras impactadas, variações na demanda global e problemas logísticos. Em junho de 2024, os preços globais do arroz apresentaram um leve aumento de 0,5%, mas começaram a cair na metade do mês. Esse movimento foi impulsionado pela redução na demanda internacional e pelo aumento na oferta de exportação.\nAlém disso, previsões otimistas de produção nos principais países asiáticos indicaram um aumento global de 2% em relação ao ano anterior, contribuindo para a queda dos preços. No Brasil, o avanço no plantio da nova safra e a realização de leilões governamentais com valores abaixo dos registrados anteriormente pressionaram as cotações.\nA tendência de baixa continuou nos meses seguintes, com os preços internacionais atingindo, em novembro, o menor patamar em seis meses. A combinação de uma oferta crescente e um cenário positivo de produção nos principais produtores asiáticos intensificou a queda no mercado global.\nConclusão\nEmbora o registro de preço zero no dia 16 de junho tenha causado estranheza, a hipótese mais plausível é a ocorrência de um erro técnico na plataforma, devido à falta de movimentação no mercado nesse dia. Ainda assim, a volatilidade observada no mercado global de arroz durante o período reflete tendências de oferta e demanda, com a expectativa de safras robustas influenciando significativamente a queda nos preços. É essencial que plataformas financeiras revisem suas metodologias de coleta de dados para evitar informações equivocadas que possam confundir investidores e analistas.\nAppendix\nGráfico feito pela plataforma Yahoo finance\n\nREFERÊNCIAS\nastera.com\ninfoarroz.org\ngloborural.globo.com\nYahoo!Finance\nPor: Gabrielly dos Santos Mateus da Rosa, Rodrigo Hermont Ozon e Ricardo Vianna."
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "predictive_model.html#data-collection-and-preprocessing",
    "href": "predictive_model.html#data-collection-and-preprocessing",
    "title": "Final Project: Time Series Forecasting with LSTMs, Neural Networks Eng. Class",
    "section": "Data Collection and Preprocessing",
    "text": "Data Collection and Preprocessing\nPython libs\n\n\nCode\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import ParameterGrid\nimport psutil\nimport time\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Additional libraries for residual analysis\nfrom statsmodels.stats.diagnostic import acorr_ljungbox, het_breuschpagan\nimport statsmodels.api as sm\n\n\nFirst of all, we need to check the hardware availability:\n\n\nCode\n# Collecting hardware information\ndef get_system_info():\n    system_info = {\n        'CPU_cores': psutil.cpu_count(logical=True),\n        'CPU_freq_MHz': psutil.cpu_freq().current,\n        'Total_RAM_GB': round(psutil.virtual_memory().total / (1024 ** 3), 2),\n        'Available_RAM_GB': round(psutil.virtual_memory().available / (1024 ** 3), 2),\n        'GPU_info': 'Not available'  # Placeholder, can be expanded with libraries like GPUtil\n    }\n    return system_info\n\nsystem_info = get_system_info()\nprint(\"System Information:\", system_info)\n\n\nSystem Information: {'CPU_cores': 12, 'CPU_freq_MHz': 1800.0, 'Total_RAM_GB': 31.69, 'Available_RAM_GB': 15.53, 'GPU_info': 'Not available'}\n\n\nLoading data:\n\n\nCode\n# Defining the tickers\ntickers = [\n    \"ZC=F\",  # Corn Futures\n    \"ZW=F\",  # Wheat Futures\n    \"KE=F\",  # KC HRW Wheat Futures\n    \"ZR=F\",  # Rough Rice Futures\n    \"GF=F\",  # Feeder Cattle Futures\n    \"ZM=F\",  # Soybean Meal Futures\n    \"ZL=F\",  # Soybean Oil Futures\n    \"ZS=F\"   # Soybean Futures\n]\n\n# Downloading price data\nprint(\"\\nDownloading price data...\")\ndata = yf.download(tickers, start=\"2015-01-01\")['Close']\n\n# Handling missing data\nprint(\"\\nHandling missing data...\")\ndata.fillna(method='ffill', inplace=True)  # Forward fill\ndata.dropna(axis=1, how='all', inplace=True)  # Drop columns with all NaNs\ndata.dropna(axis=0, how='any', inplace=True)  # Drop rows with any NaNs\n\n# Verify data\nprint(\"\\nData columns and their non-null counts:\")\nprint(data.count())\n\nif data.empty:\n    print(\"Data is empty after cleaning. Exiting.\")\n    exit()\n\n# Calculating logarithmic returns\nreturns = np.log(data / data.shift(1)).dropna()\n\n# Verify returns\nprint(\"\\nReturns DataFrame info:\")\nprint(returns.info())\nprint(returns.head())\n\nif returns.empty:\n    print(\"Returns DataFrame is empty. Exiting.\")\n    exit()\n    \n\nreturns.head() # Showing time series used (without features)\n\n\n\nDownloading price data...\n\n\n[                       0%                       ][************          25%                       ]  2 of 8 completed[************          25%                       ]  2 of 8 completed[**********************50%                       ]  4 of 8 completed[**********************62%*****                  ]  5 of 8 completed[**********************75%***********            ]  6 of 8 completed[**********************88%*****************      ]  7 of 8 completed[*********************100%***********************]  8 of 8 completed\n\n\n\nHandling missing data...\n\nData columns and their non-null counts:\nTicker\nGF=F    2539\nKE=F    2539\nZC=F    2539\nZL=F    2539\nZM=F    2539\nZR=F    2539\nZS=F    2539\nZW=F    2539\ndtype: int64\n\nReturns DataFrame info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2538 entries, 2015-01-05 00:00:00+00:00 to 2025-02-06 00:00:00+00:00\nData columns (total 8 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   GF=F    2538 non-null   float64\n 1   KE=F    2538 non-null   float64\n 2   ZC=F    2538 non-null   float64\n 3   ZL=F    2538 non-null   float64\n 4   ZM=F    2538 non-null   float64\n 5   ZR=F    2538 non-null   float64\n 6   ZS=F    2538 non-null   float64\n 7   ZW=F    2538 non-null   float64\ndtypes: float64(8)\nmemory usage: 178.5 KB\nNone\nTicker                         GF=F      KE=F      ZC=F      ZL=F      ZM=F  \\\nDate                                                                          \n2015-01-05 00:00:00+00:00  0.007673  0.012483  0.025570  0.023203  0.034462   \n2015-01-06 00:00:00+00:00 -0.004330  0.010350 -0.002466 -0.000306  0.004866   \n2015-01-07 00:00:00+00:00  0.004219 -0.017983 -0.021842  0.008832 -0.006222   \n2015-01-08 00:00:00+00:00 -0.000111 -0.019956 -0.005060  0.018029 -0.019732   \n2015-01-09 00:00:00+00:00 -0.014284 -0.012001  0.015104 -0.001192  0.006896   \n\nTicker                         ZR=F      ZS=F      ZW=F  \nDate                                                     \n2015-01-05 00:00:00+00:00  0.003537  0.036483  0.013245  \n2015-01-06 00:00:00+00:00  0.002644  0.010762  0.004658  \n2015-01-07 00:00:00+00:00  0.004392  0.001664 -0.020919  \n2015-01-08 00:00:00+00:00 -0.010130 -0.007389 -0.021806  \n2015-01-09 00:00:00+00:00  0.002653  0.006201 -0.005748  \n\n\n\n\n\n\n\n\n\n\n\nTicker\nGF=F\nKE=F\nZC=F\nZL=F\nZM=F\nZR=F\nZS=F\nZW=F\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-05 00:00:00+00:00\n0.007673\n0.012483\n0.025570\n0.023203\n0.034462\n0.003537\n0.036483\n0.013245\n\n\n2015-01-06 00:00:00+00:00\n-0.004330\n0.010350\n-0.002466\n-0.000306\n0.004866\n0.002644\n0.010762\n0.004658\n\n\n2015-01-07 00:00:00+00:00\n0.004219\n-0.017983\n-0.021842\n0.008832\n-0.006222\n0.004392\n0.001664\n-0.020919\n\n\n2015-01-08 00:00:00+00:00\n-0.000111\n-0.019956\n-0.005060\n0.018029\n-0.019732\n-0.010130\n-0.007389\n-0.021806\n\n\n2015-01-09 00:00:00+00:00\n-0.014284\n-0.012001\n0.015104\n-0.001192\n0.006896\n0.002653\n0.006201\n-0.005748\n\n\n\n\n\n\n\nPlotting the time series of prices and returns side by side (2 per row)\n\n\nCode\n# Create a directory for plots if it doesn't exist\nplots_dir = 'plots'\nif not os.path.exists(plots_dir):\n    os.makedirs(plots_dir)\n\n# Plot prices\nprint(\"\\nPlotting time series of prices...\")\nnum_cols = 2  # Number of plots per row\nnum_plots = len(data.columns)\nnum_rows = (num_plots + num_cols - 1) // num_cols  # Ensure enough rows\n\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\naxs = axs.flatten()\n\nfor i, col in enumerate(data.columns):\n    axs[i].plot(data.index, data[col])\n    axs[i].set_title(f'Price Series - {col}')\n    axs[i].set_xlabel('Date')\n    axs[i].set_ylabel('Price')\n\n# Hide unused subplots\nfor j in range(i + 1, len(axs)):\n    fig.delaxes(axs[j])\n\nplt.tight_layout()\nplt.savefig(os.path.join(plots_dir, 'price_series.png'))\nplt.show()\nplt.close()\n\n# Plot returns\nprint(\"Plotting time series of returns...\")\nnum_plots_ret = len(returns.columns)\nnum_rows_ret = (num_plots_ret + num_cols - 1) // num_cols\n\nfig, axs = plt.subplots(num_rows_ret, num_cols, figsize=(15, 5 * num_rows_ret))\naxs = axs.flatten()\n\nfor i, col in enumerate(returns.columns):\n    axs[i].plot(returns.index, returns[col])\n    axs[i].set_title(f'Return Series - {col}')\n    axs[i].set_xlabel('Date')\n    axs[i].set_ylabel('Log Return')\n\n# Hide unused subplots\nfor j in range(i + 1, len(axs)):\n    fig.delaxes(axs[j])\n\nplt.tight_layout()\nplt.savefig(os.path.join(plots_dir, 'return_series.png'))\nplt.show()\nplt.close()\n\n\n\nPlotting time series of prices...\n\n\n\n\n\n\n\n\n\nPlotting time series of returns...\n\n\n\n\n\n\n\n\n\nPreprocessing data for LSTM time series modelling:\n\n\nCode\n# Function to prepare data for LSTM\ndef prepare_data(series, time_steps):\n    X, y = [], []\n    for i in range(len(series) - time_steps):\n        X.append(series[i:(i + time_steps)])\n        y.append(series[i + time_steps])\n    return np.array(X), np.array(y)\n\n\nSetting the parameters:\n\n\nCode\n# Defining parameters\ntime_steps = 5  # Number of time steps\nepochs = 10  # Reduced epochs for faster execution during testing\n\n# Dictionaries to store results\nmodels = {}\nhistories = {}\nmse_results = {}\nscalers = {}\npredictions = {}\nbest_params_dict = {}\nresiduals_analysis = {}\n\n# Directory to save reports and graphs\nreport_dir = 'report'\nif not os.path.exists(report_dir):\n    os.makedirs(report_dir)",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "predictive_model.html#lstm-time-series-model-fitting",
    "href": "predictive_model.html#lstm-time-series-model-fitting",
    "title": "Final Project: Time Series Forecasting with LSTMs, Neural Networks Eng. Class",
    "section": "LSTM time series model fitting",
    "text": "LSTM time series model fitting\n\n\nCode\n# Loop through each time series\nfor col in returns.columns:\n    print(f\"\\nProcessing column: {col}\")\n    series = returns[col].values.reshape(-1, 1)\n    \n    # Check if series is empty\n    if len(series) == 0:\n        print(f\"Series {col} is empty after preprocessing. Skipping.\")\n        continue\n    \n    print(f\"Series {col} has {len(series)} data points.\")\n    \n    # Normalizing data\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    series_scaled = scaler.fit_transform(series)\n    scalers[col] = scaler  # Storing the scaler for later inversion\n    \n    # Preparing data\n    X, y = prepare_data(series_scaled, time_steps)\n    \n    # Check if X and y are non-empty\n    if X.shape[0] == 0:\n        print(f\"Not enough data points in {col} after preparation. Skipping.\")\n        continue\n    \n    # Splitting into training and test sets\n    split_index = int(0.8 * len(X))\n    X_train_full, X_test = X[:split_index], X[split_index:]\n    y_train_full, y_test = y[:split_index], y[split_index:]\n    X_train_full = X_train_full.reshape((X_train_full.shape[0], X_train_full.shape[1], 1))\n    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n    \n    # Hyperparameter grid for Grid Search\n    param_grid = {\n        'neurons': [30, 50],\n        'learning_rate': [0.001, 0.01],\n        'activation': ['tanh', 'relu'],\n        'batch_size': [32, 64]\n    }\n    grid = ParameterGrid(param_grid)\n    \n    # Initializing variables to store best results\n    best_mse = float('inf')\n    best_params = None\n    best_model = None\n    \n    # Performing Grid Search\n    print(f\"Performing Grid Search for {col}...\")\n    for params in grid:\n        model = Sequential()\n        model.add(LSTM(params['neurons'], activation=params['activation'], input_shape=(time_steps, 1)))\n        model.add(Dense(1))\n        optimizer = Adam(learning_rate=params['learning_rate'])\n        model.compile(optimizer=optimizer, loss='mean_squared_error')\n        \n        history = model.fit(\n            X_train_full, y_train_full,\n            validation_data=(X_test, y_test),\n            epochs=epochs,\n            batch_size=params['batch_size'],\n            verbose=0\n        )\n        \n        y_pred = model.predict(X_test)\n        y_pred_inv = scaler.inverse_transform(y_pred)\n        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n        mse = mean_squared_error(y_test_inv, y_pred_inv)\n        \n        if mse &lt; best_mse:\n            best_mse = mse\n            best_params = params\n            best_model = model\n            best_y_pred = y_pred\n    \n    best_params_dict[col] = best_params\n    print(f\"Best parameters for {col}: {best_params} with MSE: {best_mse}\")\n    \n    models[col] = best_model\n    predictions[col] = {'Best Model': best_y_pred}\n    \n    # Inverting the normalization\n    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n    y_pred_inv = scaler.inverse_transform(best_y_pred)\n    \n    # Calculating MSE\n    mse_results[col] = {'Best Model': best_mse}\n    \n    # Visualization of results\n    plt.figure(figsize=(10, 4))\n    plt.plot(y_test_inv, label='Actual Value')\n    plt.plot(y_pred_inv, label='Prediction')\n    plt.title(f'Prediction vs Actual - {col} - Best Model')\n    plt.legend()\n    plt.savefig(os.path.join(report_dir, f'pred_vs_actual_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Residual Analysis\n    residuals = y_test_inv - y_pred_inv\n    \n    # Plotting residuals\n    plt.figure(figsize=(10, 4))\n    plt.plot(residuals, label='Residuals')\n    plt.title(f'Residuals - {col} - Best Model')\n    plt.legend()\n    plt.savefig(os.path.join(report_dir, f'residuals_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Ljung-Box test for autocorrelation in residuals\n    lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n    lb_pvalue = lb_test['lb_pvalue'].values[0]\n    \n    # Plotting residuals ACF\n    fig, ax = plt.subplots(figsize=(10, 4))\n    sm.graphics.tsa.plot_acf(residuals.squeeze(), lags=40, ax=ax)\n    plt.title(f'Residuals Autocorrelation Function - {col}')\n    plt.savefig(os.path.join(report_dir, f'acf_residuals_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Heteroscedasticity test (Breusch-Pagan Test)\n    exog = sm.add_constant(best_model.predict(X_test))\n    test_bp = het_breuschpagan(residuals, exog)\n    bp_pvalue = test_bp[3]\n    \n    # Convert p-values to Python float\n    lb_pvalue = float(lb_pvalue)\n    bp_pvalue = float(bp_pvalue)\n    \n    # Saving statistical test results\n    residuals_analysis[col] = {\n        'residuals': residuals.flatten().tolist(),\n        'ljung_box_pvalue': lb_pvalue,\n        'breusch_pagan_pvalue': bp_pvalue\n    }\n    \n    print(f\"Residual Analysis for {col}:\")\n    print(f\"Ljung-Box Test p-value: {lb_pvalue}\")\n    print(f\"Breusch-Pagan Test p-value: {bp_pvalue}\")\n\n# Displaying final results in a table\nprint(\"\\nFinal Results:\")\nresults_table = pd.DataFrame(mse_results)\nprint(results_table)\n\n\n\nProcessing column: GF=F\nSeries GF=F has 2538 data points.\nPerforming Grid Search for GF=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 111ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 116ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 116ms/step 2/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 122ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 104ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 137ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 132ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 114ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\nBest parameters for GF=F: {'activation': 'tanh', 'batch_size': 64, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.00010975314203772284\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for GF=F:\nLjung-Box Test p-value: 0.6580650350440331\nBreusch-Pagan Test p-value: 0.49401740958827367\n\nProcessing column: KE=F\nSeries KE=F has 2538 data points.\nPerforming Grid Search for KE=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 121ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 113ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 114ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 115ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 111ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 111ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 104ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 101ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 142ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 115ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 140ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 114ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 123ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\nBest parameters for KE=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.0003824934060795627\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for KE=F:\nLjung-Box Test p-value: 0.023936073373489828\nBreusch-Pagan Test p-value: 0.8645221705032164\n\nProcessing column: ZC=F\nSeries ZC=F has 2538 data points.\nPerforming Grid Search for ZC=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 103ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 121ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 108ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 97ms/step 2/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 42ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 126ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 118ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 118ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 126ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 110ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 110ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 145ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\nBest parameters for ZC=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.00030591472594173485\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZC=F:\nLjung-Box Test p-value: 1.3662608410957859e-08\nBreusch-Pagan Test p-value: 0.5912763297611698\n\nProcessing column: ZL=F\nSeries ZL=F has 2538 data points.\nPerforming Grid Search for ZL=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 159ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 122ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 115ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 111ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 116ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 123ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 115ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 123ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 132ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 115ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\nBest parameters for ZL=F: {'activation': 'relu', 'batch_size': 64, 'learning_rate': 0.001, 'neurons': 50} with MSE: 0.00037011186539357647\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for ZL=F:\nLjung-Box Test p-value: 0.1356747836004889\nBreusch-Pagan Test p-value: 2.1591271527532874e-05\n\nProcessing column: ZM=F\nSeries ZM=F has 2538 data points.\nPerforming Grid Search for ZM=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 123ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 123ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 125ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 130ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 133ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 130ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 113ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 120ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 107ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 113ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\nBest parameters for ZM=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.00033008211850636443\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZM=F:\nLjung-Box Test p-value: 0.19560805053383307\nBreusch-Pagan Test p-value: 0.8858028407446729\n\nProcessing column: ZR=F\nSeries ZR=F has 2538 data points.\nPerforming Grid Search for ZR=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 111ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 115ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 113ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 124ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 111ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 124ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 114ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 111ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 161ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 115ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\nBest parameters for ZR=F: {'activation': 'tanh', 'batch_size': 64, 'learning_rate': 0.001, 'neurons': 50} with MSE: 0.13320168891741718\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZR=F:\nLjung-Box Test p-value: 3.418023995173331e-08\nBreusch-Pagan Test p-value: 1.9375930920630364e-06\n\nProcessing column: ZS=F\nSeries ZS=F has 2538 data points.\nPerforming Grid Search for ZS=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 120ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 129ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 126ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 125ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 123ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 268ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 177ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 146ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 148ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 136ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\nBest parameters for ZS=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.0001521905867240005\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZS=F:\nLjung-Box Test p-value: 0.11230267667017263\nBreusch-Pagan Test p-value: 0.24613617524870657\n\nProcessing column: ZW=F\nSeries ZW=F has 2538 data points.\nPerforming Grid Search for ZW=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 152ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 124ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 92ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 97ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 93ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 100ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\nBest parameters for ZW=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.0004047325172525737\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZW=F:\nLjung-Box Test p-value: 0.03766269738398695\nBreusch-Pagan Test p-value: 0.9953619829618164\n\nFinal Results:\n               GF=F      KE=F      ZC=F     ZL=F     ZM=F      ZR=F      ZS=F  \\\nBest Model  0.00011  0.000382  0.000306  0.00037  0.00033  0.133202  0.000152   \n\n                ZW=F  \nBest Model  0.000405  \n\n\nSaving the results in a table:\n\n\nCode\n# Saving results to a CSV file\nresults_table.to_csv(os.path.join(report_dir, 'mse_results_updated.csv'), index=True)\n\n# Saving the best parameters found\nwith open(os.path.join(report_dir, 'best_params.json'), 'w') as f:\n    json.dump(best_params_dict, f, indent=4)\n\n# Saving the residual analysis\nwith open(os.path.join(report_dir, 'residuals_analysis.json'), 'w') as f:\n    json.dump(residuals_analysis, f, indent=4)\n\n\nPloting the MSEs for each time series:\n\n\nCode\n# Report: Documenting the results\n# Plotting the MSEs for each time series\nfor col in mse_results.keys():\n    mse_series = mse_results[col]\n    plt.figure(figsize=(10, 5))\n    plt.bar(mse_series.keys(), mse_series.values(), color='blue')\n    plt.title(f'MSE Comparison - {col}')\n    plt.ylabel('MSE')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(os.path.join(report_dir, f'mse_comparison_{col}.png'))\n    plt.close()\n\n\nSaving system info:\n\n\nCode\n# End timer\nend_time = datetime.now()\nelapsed_time = end_time - start_time  # This is a timedelta object\nprint(f\"Total execution time: {elapsed_time}\")\n\n# Save execution time to the report\nsystem_info['Execution_Time_seconds'] = elapsed_time.total_seconds()  # Convert to float for JSON\nwith open(os.path.join(report_dir, 'system_info.json'), 'w') as f:\n    json.dump(system_info, f, indent=4)\n\n\nTotal execution time: 0:06:42.996155\n\n\nGenerating an automatic final report:\n\n\nCode\n# Final Report: Generating a text document with the results\nreport_path = os.path.join(report_dir, 'final_report.txt')\nwith open(report_path, 'w') as report_file:\n    report_file.write(\"Final Project Report - Forecasting Commodity Returns with LSTM\\n\")\n    report_file.write(\"=\"*80 + \"\\n\\n\")\n    \n    report_file.write(\"1. Project Objectives:\\n\")\n    report_file.write(\"Forecast future returns of a commodity portfolio using LSTM Neural Networks.\\n\\n\")\n    \n    report_file.write(\"2. Methodology:\\n\")\n    report_file.write(\"- Collecting commodity price data.\\n\")\n    report_file.write(\"- Calculating logarithmic returns.\\n\")\n    report_file.write(\"- Normalizing the data.\\n\")\n    report_file.write(\"- Training LSTM models with different configurations.\\n\")\n    report_file.write(\"- Performing grid search to optimize hyperparameters.\\n\")\n    report_file.write(\"- Conducting residual analysis to identify uncaptured patterns and issues like autocorrelation or heteroscedasticity.\\n\\n\")\n    \n    report_file.write(\"3. Results:\\n\")\n    report_file.write(results_table.to_string())\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"4. Best Parameters Found (Grid Search):\\n\")\n    report_file.write(json.dumps(best_params_dict, indent=4))\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"5. Residual Analysis:\\n\")\n    for col, res in residuals_analysis.items():\n        report_file.write(f\"Residual Analysis for {col}:\\n\")\n        report_file.write(f\"Ljung-Box Test p-value: {res['ljung_box_pvalue']}\\n\")\n        report_file.write(f\"Breusch-Pagan Test p-value: {res['breusch_pagan_pvalue']}\\n\\n\")\n    report_file.write(\"\\n\")\n    \n    report_file.write(\"6. Conclusions:\\n\")\n    report_file.write(\"The study demonstrated the importance of proper hyperparameter selection and model architecture for forecasting financial returns. Regularization techniques and the choice of activation function significantly influenced model performance. The residual analysis highlighted the need to consider autocorrelation and heteroscedasticity in modeling financial time series.\\n\\n\")\n    \n    report_file.write(\"7. Recommendations for Future Work:\\n\")\n    report_file.write(\"- Implement additional regularization techniques, such as DropConnect or Batch Normalization.\\n\")\n    report_file.write(\"- Explore more advanced architectures, like GRU or bidirectional models.\\n\")\n    report_file.write(\"- Increase the dataset to improve the models' generalization capacity.\\n\")\n    report_file.write(\"- Use more robust cross-validation methods to assess model stability.\\n\")\n    report_file.write(\"- Integrate other features, such as technical indicators or macroeconomic variables, to enrich model inputs.\\n\")\n    report_file.write(\"- Consider hybrid models that combine Machine Learning techniques with traditional statistical models.\\n\")\n    \n    report_file.write(\"\\nSystem Information and Execution Time:\\n\")\n    report_file.write(json.dumps(system_info, indent=4))\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"End of Report.\\n\")",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "presFAE.html",
    "href": "presFAE.html",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "",
    "text": "Mercado financeiro:\n\nÉ um ambiente dinâmico onde decisões baseadas em dados podem otimizar resultados.\n\nData driven decision making:\n\nComo tomar uma decisão econômica de maneira rápida, ótima e eficiente ?\n\n\n\n\n\n\n\n\nPor que Big Data é relevante ?\n\n\n\n\n\n\nVolume de Dados: A análise do mercado de commodities envolve um enorme volume de dados financeiros e econômicos que cresce rapidamente, especialmente considerando históricos de preços diários, transações e eventos externos que influenciam os mercados.\nVariabilidade e Velocidade: As séries de preços de commodities possuem mudanças rápidas e variações, exigindo a capacidade de processar e analisar dados em tempo hábil para garantir insights relevantes.\nAnálises em Profundidade: Necessidade de descobrir padrões ocultos que requerem recursos computacionais consideráveis para manipular e analisar um volume grande e complexo de dados."
  },
  {
    "objectID": "presFAE.html#introdução",
    "href": "presFAE.html#introdução",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "",
    "text": "Mercado financeiro:\n\nÉ um ambiente dinâmico onde decisões baseadas em dados podem otimizar resultados.\n\nData driven decision making:\n\nComo tomar uma decisão econômica de maneira rápida, ótima e eficiente ?\n\n\n\n\n\n\n\n\nPor que Big Data é relevante ?\n\n\n\n\n\n\nVolume de Dados: A análise do mercado de commodities envolve um enorme volume de dados financeiros e econômicos que cresce rapidamente, especialmente considerando históricos de preços diários, transações e eventos externos que influenciam os mercados.\nVariabilidade e Velocidade: As séries de preços de commodities possuem mudanças rápidas e variações, exigindo a capacidade de processar e analisar dados em tempo hábil para garantir insights relevantes.\nAnálises em Profundidade: Necessidade de descobrir padrões ocultos que requerem recursos computacionais consideráveis para manipular e analisar um volume grande e complexo de dados."
  },
  {
    "objectID": "presFAE.html#problemática-de-pesquisa",
    "href": "presFAE.html#problemática-de-pesquisa",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Problemática de Pesquisa",
    "text": "Problemática de Pesquisa\n\n\n\n\n\n\n\nSource: Data extraction from Yahoo!Finances API ploted by using Plotly"
  },
  {
    "objectID": "presFAE.html#problemática-de-pesquisa-1",
    "href": "presFAE.html#problemática-de-pesquisa-1",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Problemática de Pesquisa",
    "text": "Problemática de Pesquisa\n \n\n\n\n\n\n\n\nPerguntas\n\n\n\n\nQuais as causas ou desencadeadores desses movimentos repentinos de inversão de tendência nas séries de preços ?\nÉ possível estimar/medir o quanto essas mudanças bruscas de tendência geram de impacto na economia e no mercado de commodities agrícolas ?\nComo podemos antecipar/prever o acontecimento dessas “quebras” nas séries temporais de preços no futuro ?\nPode-se otimizar o processo decisório de compra e venda de grãos recomendando as melhores alocações de portfólio de commodities em condições de risco e incerteza ?\nComo decidir ou facilitar o processo decisório humano dentro de um contexto de massivo volume e velocidade de informações ?\nQual o volume de dados ou tamanho de série histórica necessária pra construir um modelo eficiente ?"
  },
  {
    "objectID": "presFAE.html#teorias-de-base",
    "href": "presFAE.html#teorias-de-base",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Teorias de Base",
    "text": "Teorias de Base\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nÁrea de Ciência\nTeoria\nPensadores\n\n\n\n\nMicroeconomia\nTeoria da Demanda e do Consumidor\nWalrás, Pareto, Arrow, Debreu, Samuelson, Hicks\n\n\nMicroeconomia\nEstruturas de Mercado\nPorter, Chamberlin, Joan Robinson, Bain\n\n\nMicroeconomia\nFinanças Comportamentais\nDaniel Kahneman, Amos Tversky, Robert Shiller\n\n\nMicroeconomia\nEficiência de Mercado\nEugene Fama, Fischer Black e Myron Scholes, Jensen\n\n\nMicroeconomia\nTeoria do Portfólio\nHarry Markowitz, Milton Friedman, Keynes\n\n\nFinanças\nTeoria dos Ciclos Financeiros\nHyman Minsky, Irving Fischer, Joseph Schumpeter e Kondratiev\n\n\nFinanças\nTeoria do Mais Tolo (ou Teoria do Toque de Midas Reverso)\nJohn Kenneth Galbraith, Nassim Taleb\n\n\nEconometria Financeira\nBayesian GARCH with Markov Regime Switching\nDavid Ardia, Robert Engle, Tim Bollerslev, Gary Koop\n\n\nMacroeconomia\nTeoria da Formação das Expectativas\nRobert Lucas, Milton Friedman, Edmund Phelps, Franco Modigliani\n\n\nNeuroeconomia\nTeoria da Hipótese da Antecipação de Recompensa\nWolfram Schultz, Antonio Rangel, Paul Glimcher\n\n\nMicroeconomia\nTeoria da Seleção Adversa\nGeorge Akerlof, Michael Spence, Stiglitz\n\n\nComplexidade (Física de redes)\nSistemas Dinâmicos Adpatativos não-lineares\nArthur Ávila, Brian Arthur, Robert May"
  },
  {
    "objectID": "presFAE.html#hipóteses-científicas",
    "href": "presFAE.html#hipóteses-científicas",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Hipóteses Científicas",
    "text": "Hipóteses Científicas\n\n\n\n\n\n\n\nInsights\n\n\n\n\nVolatility Clustering e mudanças estruturais\n\n \n\nAnálise de Intervenção Causal em Séries Temporais nas quebras e “efeito disseminação”\n\n \n\nDesenvolvimentos do modelo de otimização de portfolio de Markowitz (CAPM, B&S, Merton, Black-Litterman …)\n\n \n\nMúltiplos Objetivos variando conforme o contexto de mercado e as expectativas percebidas (risco e incerteza)"
  },
  {
    "objectID": "presFAE.html#justificativa-e-relevância",
    "href": "presFAE.html#justificativa-e-relevância",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Justificativa e relevância",
    "text": "Justificativa e relevância\n \n\n\n\n\n\n\n\nContribuições teóricas\n\n\n\n\n\n\nIdentificação dos drivers dos preços auxilia na investigação da causa dos movimentos repentinos nas séries de preços (Teoria da Demanda do Consumidor, Estruturas de Mercado e Teoria do Portfólio [motivo transação, especulação ou precaução]) pode ser utilizada em conjunto com a técnica Econométrica de Análise de Intervenção em Séries Temporais [Angrist e Imbens, Brodersen et. alli (2015)] para avaliar seu impacto causal na série de preço estudada;\nA Teoria dos Ciclos Financeiros ajuda a compreender em qual contexto econômico a disseminação de efeito econômico-financeiro nocivo ou positivo está inserida frente a quebra repentina da tendência da trajetória de preços de alimentos (commodities)\nO uso das técnicas pertinentes dentro da teoria da Econometria Financeira com o uso do modelo Bayesiano GARCH com mudanças de regime markovianos se mostra aderente à realidade dos dados e condizente com os últimos desenvolvimentos teóricos a respeito do fenômeno da dinâmica complexa dos preços dessas commodities;\nA teoria de alocação de portfólio desde Markowitz pode ser melhor elaborada combinando as ferramentas de otimização multiobjetivo multiperíodo de maneira dinâmica em consonância com modelos econométricos que consigam incorporar com maior clareza a “incerteza” percebida pelos players de mercado na sua aferição de risco x retorno. Assim, os processos decisórios de compra e venda em momentos oportunos se tornariam mais claros."
  },
  {
    "objectID": "presFAE.html#big-data-na-análise-de-commodities",
    "href": "presFAE.html#big-data-na-análise-de-commodities",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Big Data na Análise de Commodities",
    "text": "Big Data na Análise de Commodities\n \n\n\n\n\n\n\n\nAmostragem de Dados Significativa\n\n\n\n\nEstratégia de Amostragem: Utilizar amostragem estratificada para garantir que a variabilidade ao longo do tempo seja capturada de forma adequada (como choques econômicos ou eventos climáticos que impactam os preços).\nRedução de Dimensionalidade: Utilizar técnicas de PCA (Principal Component Analysis) para reduzir o número de variáveis sem perder informações importantes, permitindo uma análise mais eficiente dos dados.\nUso de Dados Representativos: A seleção de um subconjunto representativo de dados pode ser feita para capturar as tendências de mercado de diferentes períodos, garantindo que os insights gerados sejam válidos e aplicáveis."
  },
  {
    "objectID": "presFAE.html#big-data-na-análise-de-commodities-1",
    "href": "presFAE.html#big-data-na-análise-de-commodities-1",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Big Data na Análise de Commodities",
    "text": "Big Data na Análise de Commodities\n \n\n\n\n\n\n\n\nFerramentas e Bibliotecas Utilizadas (alguns exemplos)\n\n\n\nPython:\n\nPySpark: Uma ferramenta poderosa para processamento distribuído e análise de grandes volumes de dados, ideal para trabalhar com dados de commodities de históricos extensivos.\nDask: Alternativa à biblioteca Pandas, que facilita o processamento de grandes datasets que não cabem na memória. Dask permite a execução de operações paralelas, otimizando análises.\n\nR:\n\nsparklyr e SparkR: Integração do Spark no ambiente R, possibilitando o processamento distribuído e a manipulação eficiente de datasets gigantescos, com foco em análises financeiras.\nvroom e data.table: Utilizadas para leitura rápida e manipulação de grandes volumes de dados armazenados em arquivos CSV, permitindo o carregamento de arquivos grandes em poucos segundos.\n\n*Existem algumas outras pra R e Python que poderiam ser mencionadas aqui, mas por questões de parcimônia limitaremos a pequenos exemplos."
  },
  {
    "objectID": "presFAE.html#big-data-com-r",
    "href": "presFAE.html#big-data-com-r",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Big Data com R",
    "text": "Big Data com R\n \n\nAnálise de Commodities com sparklyr e vroom\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(vroom)\n\n# Conectar ao Spark\nsc &lt;- spark_connect(master = \"local[*]\") # Você pode rodar também no Databricks, Azure, AWS, Google Colab...\n\n# Ler grandes volumes de dados usando vroom\nfile_list &lt;- list.files(\"data\", pattern = \"*_data.csv\", full.names = TRUE)\ncombined_data &lt;- vroom(file_list, col_types = list(\n  Date = col_date(),\n  Open = col_double(),\n  High = col_double(),\n  Low = col_double(),\n  Close = col_double(),\n  Volume = col_double(),\n  Adj.Close = col_double(),\n  Ticker = col_character()\n))\n\n# Copiar os dados para o Spark\nspark_data &lt;- copy_to(sc, combined_data, \"commodities_data\", overwrite = TRUE)\n\n# Executar análise no Spark: calcular a média de fechamento ajustado por commodity\naverage_close &lt;- spark_data |&gt;\n  group_by(Ticker) |&gt;\n  summarise(Average_Close = mean(Adj.Close, na.rm = TRUE)) |&gt;\n  collect()\n\nprint(average_close)\n\n# Desconectar do Spark\nspark_disconnect(sc)"
  },
  {
    "objectID": "presFAE.html#big-data-com-python",
    "href": "presFAE.html#big-data-com-python",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Big Data com Python",
    "text": "Big Data com Python\n \n\nAnálise de Commodities com PySpark e Dask\n\nfrom pyspark.sql import SparkSession\nimport dask.dataframe as dd\n\n# Inicializar a sessão Spark\nspark = SparkSession.builder.master(\"local\").appName(\"Commodities Analysis\").getOrCreate()\n\n# Carregar dados grandes de commodities usando Dask\nfile_list = [\"data/ZC=F_data.csv\", \"data/ZO=F_data.csv\", \"data/KE=F_data.csv\"]\ndf_dask = dd.read_csv(file_list)\n\n# Converter o DataFrame Dask para Spark\ndf_spark = spark.createDataFrame(df_dask.compute())\n\n# Analisar os dados no Spark\ndf_spark.createOrReplaceTempView(\"commodities_data\")\nresult = spark.sql(\"SELECT Ticker, AVG(Adj_Close) as Average_Close FROM commodities_data GROUP BY Ticker\")\nresult.show()\n\n# Finalizar a sessão Spark\nspark.stop()"
  },
  {
    "objectID": "presFAE.html#exercícios-rápidos",
    "href": "presFAE.html#exercícios-rápidos",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Exercícios rápidos",
    "text": "Exercícios rápidos\n \n\n\nA partir das instruções contidas no post em como ler com o Sparklyr Big Data localmente, leia e processe usando o pacote do R sparklyr localmente o conjunto de dados vix-daily.csv (menor) e o .rds gbt-model-responses. 1.a) Agora leia eles usando o Dask (Python), data.table e vroom (R) localmente e as reporte num documento Quarto\nComo você analisaria o conjunto de dados de hipotecas (mortgages) ou outra fonte aqui no repo ? Ele contém dados fictícios ou anonimizados sobre empréstimos imobiliários, fornecendo informações que podem ser usadas para estudar padrões de empréstimos, análise de risco de crédito ou previsões de inadimplência. Elabore uma proposta de projeto a partir desse dataset em R ou em Python utilizando o report utilizando o quarto.org pra ganhar velocidade!\nE quando o Spark, data.table, vroom, Dask e etc. não derem mais conta do recado ? Pesquise alternativas e sugira algumas estratégias pra lidar com grandes volumes de dados e modelá-los ! Lembre-se de citar as fontes, pacotes, métodos e referências necessárias. (Algumas sugestões de uso de dados brasileiros via extração por API em Repo Doc. gitHub)\nExpanda o exemplo do portfólio de commodities, utilizando uma extração de séries temporais mais longas no tempo, inserindo mais ativos no portfólio (utilize o pacote do R quantmod ou do Python yfinance) utilizando a sintaxe do Spark e experimente os pacotes do R úteis pra isso como a sintaxe do data.table ou a leitura do conjunto de dados com o vroom localmente. Como poderemos analisar tendências desses dados utilizando um modelo de médias móveis simples ?"
  },
  {
    "objectID": "presFAE.html#mais-informações-e-referências",
    "href": "presFAE.html#mais-informações-e-referências",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Mais informações e referências",
    "text": "Mais informações e referências\n \n\n\n\n\n\n\n\n\nEnvie esses reports nos formatos .qmd e .html num arquivo .zip para rodrigo.ozon@fae.edu.br\nPrazo de envio (1 e 2 até o final da próxima semana) e 3 e 4 (até o final da semana subsequente)\nMaiores detalhes a respeito de fontes, referências, recomendações de cursos online, pacotes e libs, consulte por favor o Plano de Aula de hoje"
  },
  {
    "objectID": "presFAE.html#obrigado",
    "href": "presFAE.html#obrigado",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Obrigado!",
    "text": "Obrigado!\n \n\n\n\n\n\n\n\nRodrigo Hermont Ozon\n\n\n\n\\(\\Rightarrow\\) Agradecimentos à todos os membros da banca examinadora e demais ouvintes:\n\nMeu perfil no Google Scholar\nMeu CV Lattes\nMeu site com posts, tutoriais e artigos\nMeu perfil no LinkeDin\n\n\n\n \n \n\n\n\n\"Situations emerge in the process of creative destruction in which many firms may have to perish that nevertheless would be able to live on vigorously and usefully if they could weather a particular storm.\n\n[... Capitalism requires] the perennial gale of Creative Destruction.\" Schumpeter, Joseph A. (1994) [1942]. Capitalism, Socialism and Democracy. London: Routledge. pp. 82–83. ISBN 978-0-415-10762-4. Retrieved 23 November 2011."
  },
  {
    "objectID": "publplan.html",
    "href": "publplan.html",
    "title": "Journals publication plan",
    "section": "",
    "text": "Abstract\n\n\n\nThe main idea here is to schedule the publication plan to be sended for specific indexed magazines, journals, periodicals and scientific research pairs.",
    "crumbs": [
      "About",
      "Intro",
      "Journals publication plan"
    ]
  },
  {
    "objectID": "publplan.html#possible-journals",
    "href": "publplan.html#possible-journals",
    "title": "Journals publication plan",
    "section": "Possible Journals",
    "text": "Possible Journals\n\nPaper 1\n\nPaper title: “Impact of Sentiment Analysis on the Volatility and Structural Breaks in Agricultural Commodity Prices” by Rodrigo Hermont Ozon and Ricardo Viana (see the publishing plan for this paper here at this presentation)\n\n\n\nAbstract:\nThis study explores the impact of sentiment analysis on the volatility and structural breaks in agricultural commodity prices. Agricultural markets are often subject to abrupt fluctuations and high volatility, making it essential for investors and policymakers to understand the factors driving these changes. Sentiment analysis, derived from news sources and social media, offers an additional layer of data that complements traditional econometric models, providing a broader understanding of market behavior. This research will utilize time series data of agricultural commodity prices and sentiment metrics obtained through natural language processing (NLP) tools applied to news data.\nThe proposed methods include the application of econometric models such as conditional volatility analysis (GARCH) and structural break models (e.g., the Bai-Perron model) to capture the effects of market sentiment on price fluctuations. Additionally, a multi-objective optimization model will be used to adjust agricultural commodity portfolios, accounting for the sentiment-driven volatility impact. Expected results include identifying a significant relationship between market sentiment and price variations, as well as validating an optimized approach to managing commodity portfolios, based on both sentiment data and volatility patterns.\nThis study will provide valuable insights for portfolio managers and investors, enabling them to incorporate sentiment indicators into their investment strategies and improve the forecasting of commodity market fluctuations. The relevance of this research lies in the growing need to understand how qualitative variables, such as market sentiment, affect agricultural commodity prices in the context of global economic uncertainties.\nKeyWords: Sentiment Analysis, Time Series, Volatility, Structural Breaks, Portfolio Optimization, Commodity Prices.\n\n\n\nJournal\nText Match Score (1 to 5)\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nJournal of Commodity Markets\n5\n4.3\n-\n60\n\n\nFinance Research Letters\n5\n11.1\n24%\n10\n\n\nEnergy Economics\n5\n10.8\n-\n30\n\n\nJournal of Business Research\n4\n11.2\n-\n12\n\n\nEconomic Modelling\n5\n5.8\n-\n18\n\n\nJournal of Banking and Finance\n5\n6.4\n25%\n40\n\n\nJournal of Behavioral and Experimental Finance\n4\n4.5\n-\n6\n\n\nEmerging Markets Review\n5\n4.1\n-\n30\n\n\nComputers and Electronics in Agriculture\n5\n7.6\n30%\n15\n\n\nInternational Economics\n4\n3.9\n-\n45\n\n\n\nSource: Elsevier Journal Finder Results\n\n\n\nPaper 2\n\nPaper title: “Ablation Study of a CNN+LSTM Architecture for Time Series Forecasting of Corn Price Returns” by Rodrigo Hermont Ozon\n\n\n\nAbstract:\nThis paper presents an ablation study of a hybrid CNN+LSTM architecture applied to the forecasting of logarithmic returns of corn prices. Different hyperparameter configurations are explored, including the addition/removal of layers, changes in loss functions, and variations in optimizers. Results are evaluated using metrics such as mean squared error (MSE), mean absolute percentage error (MAPE), and the coefficient of determination (\\(R^2\\)). The conclusions provide insights into the critical components of the architecture for modeling complex time series data.\nKeywords: Time Series, CNN+LSTM, Ablation Study, Price Forecasting, Artificial Neural Networks.\n\n\n\nJournal\nText Match Score\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nNeurocomputing\n5\n8.7\n30%\n45\n\n\nExpert Systems with Applications\n5\n12.2\n28%\n30\n\n\nJournal of Forecasting\n4\n4.1\n25%\n60\n\n\nApplied Soft Computing\n5\n10.5\n27%\n50\n\n\nEngineering Applications of Artificial Intelligence\n5\n7.8\n26%\n40\n\n\n\nSource: Elsevier Journal Finder Results\n\n\n\nPaper 3\n\nPaper title: “Blending Forecasting Models for Commodities Portfolio Optimization” by Rodrigo Hermont Ozon and Robson Guedes\n\n\n\nAbstract:\nThe intricacies of the global commodities market, characterized by its inherent volatility, necessitate robust forecasting methodologies to guide stakeholders in their decision-making processes. Traditional time series forecasting models, while foundational, often grapple with capturing the multifaceted dynamics of commodity prices. This paper introduces an innovative approach to commodities portfolio optimization by synergistically blending time series forecasting models within Pareto Front Scenarios. By integrating bootstrapping techniques across models such as Dynamic Harmonic Regression, DHR with multiple seasonal periods, STL with multiple seasonal, Auto ARIMA, and non-linear regression with cubic splines, we enhance the predictive accuracy and robustness of our forecasting framework. Our findings underscore the efficacy of this integrative approach, offering a nuanced, actionable framework for commodities portfolio optimization. This research not only contributes to the academic discourse on commodities forecasting but also provides practical insights for investors, policymakers, and other stakeholders in the commodities market.\nKeywords: Commodities Portfolio Optimization; Time Series Forecasting; Pareto Front Scenarios\n\n\n\nJournal\nText Match Score (1 to 5)\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nJournal of Commodity Markets\n5\n4.3\n-\n60\n\n\nAgricultural Economics\n5\n4.2\n-\n45\n\n\nFood Policy\n5\n6.8\n-\n30\n\n\nEnergy Economics\n4\n10.8\n-\n30\n\n\nEconomic Modelling\n4\n5.8\n-\n18\n\n\nJournal of Agricultural and Resource Economics\n4\n1.5\n-\n60\n\n\nApplied Economics\n4\n3.0\n-\n50\n\n\nJournal of International Money and Finance\n4\n4.1\n-\n40\n\n\nEmerging Markets Review\n4\n4.1\n-\n30\n\n\nJournal of Futures Markets\n5\n3.2\n-\n40\n\n\n\nSource: Elsevier Journal Finder Results\n\n\n\nPaper 4\n\nPaper title: “Portfolio Optimization with GARCH Models Using Multiple Time Windows for Pareto Frontiers” by Rodrigo Hermont Ozon, Gilberto Reynoso-Meza\n\n\n\nAbstract:\nThis study introduces a novel approach for portfolio optimization by employing Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models to assess risk and construct Pareto frontiers over multiple time windows. Traditional risk measures such as the standard deviation may fail to capture risk accurately in financial markets where volatility is time-varying. By modeling conditional volatility, GARCH models offer a more comprehensive depiction of risk. In this paper, we implement this approach in the non-diversified portfolio optimization of two commodities, corn and soy, projecting their price dynamics 252 days ahead. The results demonstrate that the application of GARCH models and multi-period Pareto frontiers can significantly enhance portfolio optimization, providing fresh insights into purchasing and selling opportunities in the commodities market. This study adds to the current literature by addressing the gaps in applying Pareto frontiers across multiple time windows and utilizing GARCH models for risk measurement.\nKeywords: Portfolio Optimization, GARCH Models, Pareto Frontiers.\n\n\n\nJournal\nText Match Score (1 to 5)\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nJournal of Commodity Markets\n5\n4.3\n-\n60\n\n\nFinance Research Letters\n5\n11.1\n24\n10\n\n\nEnergy Economics\n5\n10.8\n-\n30\n\n\nEconomic Modelling\n5\n5.8\n-\n18\n\n\nJournal of Banking and Finance\n5\n6.4\n25\n40\n\n\nJournal of Behavioral and Experimental Finance\n4\n4.5\n-\n6\n\n\nEmerging Markets Review\n5\n4.1\n-\n30\n\n\nComputers and Electronics in Agriculture\n5\n7.6\n30\n15\n\n\nInternational Economics\n4\n3.9\n-\n45\n\n\nJournal of Financial Markets\n4\n5.2\n-\n50\n\n\n\nSource: Elsevier Journal Finder Results\n\n\n\n\nPaper 5\n\nPaper title: “Impact of the Russia-Ukraine War on Corn Prices, Returns, and Volatility: A Quasi-Experimental Approach” by Rodrigo Hermont Ozon\n\n\n\nAbstract:\nThis paper investigates the impact of the Russia-Ukraine war on corn futures prices, returns, and volatility. Using a quasi-experimental approach with the CausalImpact model, we estimate changes before and after the war’s escalation. The results highlight how geopolitical conflicts affect commodity markets.\nKeywords: Russia-Ukraine War, Corn Prices, Futures, Volatility, CausalImpact Model, Quasi-Experimental Approach.\n\n\n\nJournal\nText Match Score (1 to 5)\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nJournal of Commodity Markets\n5\n4.3\n-\n60\n\n\nAgricultural Economics\n5\n4.2\n-\n45\n\n\nFood Policy\n5\n6.8\n-\n30\n\n\nEnergy Economics\n4\n10.8\n-\n30\n\n\nEconomic Modelling\n4\n5.8\n-\n18\n\n\nJournal of Agricultural and Resource Economics\n4\n1.5\n-\n60\n\n\nApplied Economics\n4\n3.0\n-\n50\n\n\nJournal of International Money and Finance\n4\n4.1\n-\n40\n\n\nEmerging Markets Review\n4\n4.1\n-\n30\n\n\nJournal of Futures Markets\n5\n3.2\n-\n40\n\n\n\nSource: Elsevier Journal Finder Results\n\n\n\nPaper 6\n\nPaper title: “A Commodities Portfolio Optimization Model Recommendation: Blending Time Series Forecasting Models in Pareto Front Scenarios” by Rodrigo Hermont Ozon and Robson Thiago Guedes da Silva\n\n\n\nAbstract:\nThe intricacies of the global commodities market, characterized by its inherent volatility, necessitate robust forecasting methodologies to guide stakeholders in their decision-making processes. Traditional time series forecasting models, while foundational, often grapple with capturing the multifaceted dynamics of commodity prices. This paper introduces an innovative approach to commodities portfolio optimization by synergistically blending time series forecasting models within Pareto Front Scenarios. By integrating bootstrapping techniques across models such as Dynamic Harmonic Regression, DHR with multiple seasonal periods, STL with multiple seasonal, Auto ARIMA, and non-linear regression with cubic splines, we enhance the predictive accuracy and robustness of our forecasting framework. Our findings underscore the efficacy of this integrative approach, offering a nuanced, actionable framework for commodities portfolio optimization. This research not only contributes to the academic discourse on commodities forecasting but also provides practical insights for investors, policymakers, and other stakeholders in the commodities market.\nKeywords: Commodities Portfolio Optimization, Time Series Forecasting, Pareto Front Scenarios.\n\n\n\nJournal\nText Match Score (1 to 5)\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nJournal of Commodity Markets\n5\n4.3\n-\n60\n\n\nFinance Research Letters\n5\n11.1\n24\n10\n\n\nEnergy Economics\n5\n10.8\n-\n30\n\n\nEconomic Modelling\n5\n5.8\n-\n18\n\n\nJournal of Banking and Finance\n5\n6.4\n25\n40\n\n\nJournal of Behavioral and Experimental Finance\n4\n4.5\n-\n6\n\n\nEmerging Markets Review\n5\n4.1\n-\n30\n\n\nComputers and Electronics in Agriculture\n5\n7.6\n30\n15\n\n\nInternational Economics\n4\n3.9\n-\n45\n\n\nJournal of Financial Markets\n4\n5.2\n-\n50\n\n\n\nSource: Elsevier Journal Finder Results\n\n\n\n\nPaper 7\n\nPaper title: “Enhancing Grain Portfolio Risk Management with GAMLSS and MSGARCH” by Rodrigo Hermont Ozon, José Donizzetti de Lima, and Géremi Dranka\n\n\n\nAbstract:\nThis paper presents a novel method integrating Generalized Additive Models for Location, Scale, and Shape (GAMLSS) with Bayesian Markov-Switching GARCH (MSGARCH) models to enhance forecasting in commodity price returns, focusing on grain portfolios. We leverage GAMLSS to model non-normal distributions of return series, crucial for accurately simulating real options. These models then inform the Bayesian MSGARCH framework, improving projections of returns and volatility, essential for effective financial planning and risk management. This innovative approach not only advances practical portfolio management but also contributes to the theoretical development of real options theory. Demonstrating its efficacy, our methodology offers a more informed, strategic approach to the complex world of commodity trading, bridging the gap between theoretical models and practical financial applications.\nKeywords:\nCommodity Portfolio Management, GAMLSS, Monte Carlo Simulation, Bayesian MSGARCH, Real Options Theory, Financial Time Series\n\n\n\nJournal\nText Match Score (1 to 5)\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nJournal of Commodity Markets\n5\n4.3\n-\n60\n\n\nFinance Research Letters\n5\n11.1\n24\n10\n\n\nEnergy Economics\n5\n10.8\n-\n30\n\n\nEconomic Modelling\n5\n5.8\n-\n18\n\n\nJournal of Banking and Finance\n5\n6.4\n25\n40\n\n\nJournal of Behavioral and Experimental Finance\n4\n4.5\n-\n6\n\n\nEmerging Markets Review\n5\n4.1\n-\n30\n\n\nComputers and Electronics in Agriculture\n5\n7.6\n30\n15\n\n\nInternational Economics\n4\n3.9\n-\n45\n\n\nJournal of Financial Markets\n4\n5.2\n-\n50\n\n\n\nSource: Elsevier Journal Finder Results\n\n\n\n\nPaper 8\n\nPaper title: “Improving Risk Management in Grain Portfolios: The Role of GAMLSS and Bayesian MSGARCH Models” by Rodrigo Hermont Ozon and Gilberto Reynoso-Meza\n\n\n\nAbstract:\nThis paper presents a novel method integrating Generalized Additive Models for Location, Scale, and Shape (GAMLSS) with Bayesian Markov-Switching GARCH (MSGARCH) models to enhance forecasting in commodity price returns, focusing on grain portfolios. We leverage GAMLSS to model non-normal distributions of return series, crucial for accurately simulating real options. These models then inform the Bayesian MSGARCH framework, improving projections of returns and volatility, essential for effective financial planning and risk management. This innovative approach not only advances practical portfolio management but also contributes to the theoretical development of real options theory. Demonstrating its efficacy, our methodology offers a more informed, strategic approach to the complex world of commodity trading, bridging the gap between theoretical models and practical financial applications.\nKeywords:\nCommodity Portfolio Management, GAMLSS, Monte Carlo Simulation, Bayesian MSGARCH, Real Options Theory, Financial Time Series\n\n\n\nJournal\nText Match Score (1 to 5)\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nJournal of Commodity Markets\n5\n4.3\n-\n60\n\n\nFinance Research Letters\n5\n11.1\n24\n10\n\n\nEnergy Economics\n5\n10.8\n-\n30\n\n\nEconomic Modelling\n5\n5.8\n-\n18\n\n\nJournal of Banking and Finance\n5\n6.4\n25\n40\n\n\nJournal of Behavioral and Experimental Finance\n4\n4.5\n-\n6\n\n\nEmerging Markets Review\n5\n4.1\n-\n30\n\n\nComputers and Electronics in Agriculture\n5\n7.6\n30\n15\n\n\nInternational Economics\n4\n3.9\n-\n45\n\n\nJournal of Financial Markets\n4\n5.2\n-\n50\n\n\n\nSource: Elsevier Journal Finder Results\n\n\n\n\nPaper 8\n\nPaper title: “Econometric Prices Forecasting Approach and Multiobjective Optimization for Commodity Portfolio Decision-Making” by Rodrigo Hermont Ozon and Gilberto Reynoso-Meza\n\n\n\nAbstract:\nThis paper proposes an innovative approach to agricultural commodity portfolio optimization, leveraging Bayesian GARCH models with Markov Regime Switching shifts to forecast price return series and estimate their conditional volatilities, a departure from the conventional standard deviation. This research is grounded in a meticulous review of diverse methodologies in commodity price forecasting and portfolio optimization, including the recent works of Zhang et al. (2020), who developed a robust framework for forecasting agricultural commodity prices, and Gagnon et al. (2020), who explored the diversification benefits of commodities.\nOur approach utilizes time series data of agricultural commodity prices to construct multi-objective optimization models, identifying optimal Pareto fronts for each quarterly time window. This methodology is informed by the advancements in multi-objective optimization by Chen et al. (2009) and Zitzler et al. (2001), and it is further enriched by the incorporation of reinforcement learning to recommend the most advantageous buy, sell, or hold alternatives. The integration of Bayesian GARCH models with Markov Regime shifts and Reinforcement Learning recommendation models represents a disruptive advancement in the field, offering nuanced insights into the dynamics of commodity prices and refining portfolio theory.\nThis novel convergence of methodologies not only enhances the precision in forecasting commodity price returns and their volatilities but also provides a sophisticated framework for portfolio decision-making, contributing significantly to the ongoing discourse in financial research and the modeling of price time series.\nKeywords:\nBayesian GARCH Models, Markov Regime Shifts, Agricultural Commodity Portfolio, Multi-objective Optimization, Reinforcement Learning Recommendation Models\n\n\n\nJournal\nText Match Score (1 to 5)\nCiteScore\nAcceptance Rate (%)\nTime to First Decision (days)\n\n\n\n\nJournal of Commodity Markets\n5\n4.3\n-\n60\n\n\nFinance Research Letters\n5\n11.1\n24\n10\n\n\nEnergy Economics\n5\n10.8\n-\n30\n\n\nEconomic Modelling\n5\n5.8\n-\n18\n\n\nJournal of Banking and Finance\n5\n6.4\n25\n40\n\n\nJournal of Behavioral and Experimental Finance\n4\n4.5\n-\n6\n\n\nEmerging Markets Review\n5\n4.1\n-\n30\n\n\nComputers and Electronics in Agriculture\n5\n7.6\n30\n15\n\n\nInternational Economics\n4\n3.9\n-\n45\n\n\nJournal of Financial Markets\n4\n5.2\n-\n50\n\n\n\nSource: Elsevier Journal Finder Results",
    "crumbs": [
      "About",
      "Intro",
      "Journals publication plan"
    ]
  },
  {
    "objectID": "revsyslit.html",
    "href": "revsyslit.html",
    "title": "Literature Review and Scientific Challenge",
    "section": "",
    "text": "Code\nstart_time &lt;- Sys.time()",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-sentiment-analysis-and-commodity-prices",
    "href": "revsyslit.html#keyword-sentiment-analysis-and-commodity-prices",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Sentiment Analysis AND Commodity Prices",
    "text": "Keyword: “Sentiment Analysis AND Commodity Prices\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Sentiment Analysis AND Commodity Prices\"\nresults &lt;- cr_works(query = \"Sentiment Analysis AND Commodity Prices\", limit = 1000)$data # Max is 1000 searches\n\n\ncat(\"Show all features avaiable in selected keywords... \\n\")\n\n\nShow all features avaiable in selected keywords... \n\n\nCode\n# Exibir os resultados da busca\nglimpse(results)\n\n\nRows: 1,000\nColumns: 40\n$ alternative.id         &lt;chr&gt; \"10.1002/9781119603849.ch1,10.1002/978111960384…\n$ archive                &lt;chr&gt; \"Portico\", NA, NA, NA, NA, NA, \"Portico\", NA, \"…\n$ container.title        &lt;chr&gt; \"Advanced Positioning, Flow, and Sentiment Anal…\n$ created                &lt;chr&gt; \"2019-12-20\", \"2009-07-03\", \"2010-06-25\", \"2013…\n$ deposited              &lt;chr&gt; \"2023-08-17\", \"2021-09-03\", \"2021-09-03\", \"2022…\n$ published.print        &lt;chr&gt; \"2019-11-18\", NA, NA, \"2013-08-26\", NA, NA, \"20…\n$ published.online       &lt;chr&gt; \"2019-12-19\", NA, NA, NA, NA, NA, \"2019-12-19\",…\n$ doi                    &lt;chr&gt; \"10.1002/9781119603849.ch1\", \"10.1787/656681831…\n$ indexed                &lt;chr&gt; \"2024-05-11\", \"2022-04-01\", \"2022-03-29\", \"2022…\n$ isbn                   &lt;chr&gt; \"9781119603825,9781119603849\", NA, NA, NA, NA, …\n$ issued                 &lt;chr&gt; \"2019-11-18\", NA, NA, \"2013-08-26\", NA, NA, \"20…\n$ member                 &lt;chr&gt; \"311\", \"1963\", \"1963\", \"2026\", \"1963\", \"1963\", …\n$ page                   &lt;chr&gt; \"7-18\", NA, NA, \"2038-2040\", NA, NA, \"163-176\",…\n$ prefix                 &lt;chr&gt; \"10.1002\", \"10.1787\", \"10.1787\", \"10.3724\", \"10…\n$ publisher              &lt;chr&gt; \"Wiley\", \"Organisation for Economic Co-Operatio…\n$ score                  &lt;chr&gt; \"29.017931\", \"28.89995\", \"28.188828\", \"28.15047…\n$ source                 &lt;chr&gt; \"Crossref\", \"Crossref\", \"Crossref\", \"Crossref\",…\n$ reference.count        &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ references.count       &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\", \"2…\n$ title                  &lt;chr&gt; \"Advanced Positioning, Flow, and Sentiment Anal…\n$ type                   &lt;chr&gt; \"other\", \"component\", \"component\", \"journal-art…\n$ url                    &lt;chr&gt; \"https://doi.org/10.1002/9781119603849.ch1\", \"h…\n$ language               &lt;chr&gt; \"en\", NA, NA, \"en\", NA, NA, \"en\", NA, \"en\", NA,…\n$ link                   &lt;list&gt; [&lt;tbl_df[3 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[1 …\n$ license                &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NU…\n$ issn                   &lt;chr&gt; NA, NA, NA, \"1001-9081\", NA, NA, NA, NA, NA, NA…\n$ issue                  &lt;chr&gt; NA, NA, NA, \"7\", NA, NA, NA, NA, NA, NA, NA, NA…\n$ subtitle               &lt;chr&gt; NA, NA, NA, \"Text sentiment analysis-oriented c…\n$ volume                 &lt;chr&gt; NA, NA, NA, \"32\", NA, NA, NA, NA, NA, NA, NA, N…\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Computer Applications\",…\n$ author                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], &lt;NU…\n$ reference              &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ abstract               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ update.policy          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ assertion              &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ funder                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ archive_               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ na.                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ na..1                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nFirst_Keyword &lt;- results |&gt; \n  arrange( desc(score), desc(is.referenced.by.count) ) |&gt; \n  distinct() |&gt; \n  filter(title != \"Front Matter\", \n         title != \"Cover\",\n         title != \"Books Received\")\n\nFirst_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-volatility-models-and-agricultural-markets",
    "href": "revsyslit.html#keyword-volatility-models-and-agricultural-markets",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Volatility Models AND Agricultural Markets”",
    "text": "Keyword: “Volatility Models AND Agricultural Markets”\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Volatility Models AND Agricultural Markets\"\nresults &lt;- cr_works(query = \"Volatility Models AND Agricultural Markets\", limit = 1000)$data # Max is 1000 searches\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nSecond_Keyword &lt;- results |&gt; arrange( desc(score), desc(is.referenced.by.count) ) |&gt; distinct()\n\nSecond_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-structural-breaks-and-agricultural-prices",
    "href": "revsyslit.html#keyword-structural-breaks-and-agricultural-prices",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Structural Breaks AND Agricultural Prices”",
    "text": "Keyword: “Structural Breaks AND Agricultural Prices”\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Structural Breaks AND Agricultural Prices\"\nresults &lt;- cr_works(query = \"Structural Breaks AND Agricultural Prices\", limit = 1000)$data # Max is 1000 searches\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nThird_Keyword &lt;- results |&gt; \n  arrange( desc(score), desc(is.referenced.by.count) ) |&gt; distinct()\n\nThird_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#final-dataframe",
    "href": "revsyslit.html#final-dataframe",
    "title": "Literature Review and Scientific Challenge",
    "section": "Final dataframe",
    "text": "Final dataframe\n\n\nCode\nFinal_df &lt;- bind_rows(\n  First_Keyword,\n  Second_Keyword,\n  Third_Keyword\n) |&gt; \n  arrange( desc(title), desc(score) ) |&gt; \n  distinct()\n\n\nglimpse(Final_df)\n\n\nRows: 405\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[3 x 3]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"dataset\", \"dataset\", \"journal-article…\n$ url                    &lt;chr&gt; \"https://doi.org/10.1108/oxan-ga261626\", \"https…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"CRAN: Contributed …\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Commodity Markets\", NA,…\n$ publisher              &lt;chr&gt; \"Emerald\", \"The R Foundation\", \"The R Foundatio…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.32614/cran.package…\n$ published.print        &lt;mth&gt; 2021 mai, 2020 set, 2023 jan, 2023 dez, 2023 ju…\n$ score                  &lt;dbl&gt; 19.82961, 18.77166, 17.06716, 19.86348, 16.0235…\n$ reference.count        &lt;chr&gt; \"0\", \"1\", \"1\", \"47\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[2 …\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[1 x 3]&gt;], …\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", NA, NA, \"2405-8513\", \"1999-1142\", …\n\n\n\n\nCode\nFinal_df_unique &lt;- Final_df |&gt;\n  distinct(title, doi, .keep_all = TRUE)\n\nglimpse(Final_df_unique)\n\n\nRows: 398\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[3 x 3]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"dataset\", \"dataset\", \"journal-article…\n$ url                    &lt;chr&gt; \"https://doi.org/10.1108/oxan-ga261626\", \"https…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"CRAN: Contributed …\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Commodity Markets\", NA,…\n$ publisher              &lt;chr&gt; \"Emerald\", \"The R Foundation\", \"The R Foundatio…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.32614/cran.package…\n$ published.print        &lt;mth&gt; 2021 mai, 2020 set, 2023 jan, 2023 dez, 2023 ju…\n$ score                  &lt;dbl&gt; 19.82961, 18.77166, 17.06716, 19.86348, 16.0235…\n$ reference.count        &lt;chr&gt; \"0\", \"1\", \"1\", \"47\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[2 …\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[1 x 3]&gt;], …\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", NA, NA, \"2405-8513\", \"1999-1142\", …\n\n\nCode\nFinal_df_unique\n\n\n\n  \n\n\n\nBy expanding the author names columns:\n\n\nCode\n# Unnest the author column\nFinal_df_unique_expanded &lt;- Final_df_unique |&gt;\n  unnest(author) |&gt;\n  select(title, given, family)\n\n# Rename the author name columns\nFinal_df_unique_expanded &lt;- Final_df_unique_expanded |&gt;\n  rename(Author_First_Name = given, Author_Last_Name = family)\n\n# Print the expanded data frame\nprint(Final_df_unique_expanded)\n\n\n# A tibble: 643 × 3\n   title                                      Author_First_Name Author_Last_Name\n   &lt;chr&gt;                                      &lt;chr&gt;             &lt;chr&gt;           \n 1 stochvolTMB: Likelihood Estimation of Sto… Jens              Wahl            \n 2 mbreaks: Estimation and Inference for Str… Linh              Nguyen          \n 3 mbreaks: Estimation and Inference for Str… Yohei             Yamamoto        \n 4 mbreaks: Estimation and Inference for Str… Pierre            Perron          \n 5 World regional natural gas prices: Conver… Jose Roberto      Loureiro        \n 6 World regional natural gas prices: Conver… Julian            Inchauspe       \n 7 World regional natural gas prices: Conver… Roberto F.        Aguilera        \n 8 World commodity prices and partial defaul… Manoj             Atolia          \n 9 World commodity prices and partial defaul… Shuang            Feng            \n10 Who should buy stocks when volatility spi… Andrés            Schneider       \n# ℹ 633 more rows\n\n\nNow inserting the authors name in a new df\n\n\nCode\nauthors_df &lt;- Final_df_unique_expanded |&gt;\n  group_by(title) |&gt;\n  summarise(Author_Names = paste(Author_First_Name, Author_Last_Name, collapse = \"; \")) |&gt;\n  ungroup()\n\nprint(authors_df)\n\n\n# A tibble: 282 × 2\n   title                                                            Author_Names\n   &lt;chr&gt;                                                            &lt;chr&gt;       \n 1 A Behavioral Approach to Pricing in Commodity Markets: Dual Pro… Florian Dos…\n 2 A CEEMD-ARIMA-SVM model with structural breaks to forecast the … Yuxiang Che…\n 3 A Commodity Review Sentiment Analysis Based on BERT-CNN Model    Junchao Don…\n 4 A Study of Asymmetric Volatilities in Korean Stock Markets Usin… Eunhee Lee  \n 5 A Theoretical Framework for Reconceiving Agricultural Markets    Anthony Pah…\n 6 A double mixture autoregressive model of commodity prices        Gilbert Mba…\n 7 A note on institutional hierarchy and volatility in financial m… S. Alfarano…\n 8 A quantile autoregression analysis of price volatility in agric… Jean‐Paul C…\n 9 ARE AGRICULTURAL COMMODITY PRICES AFFECTED BY COVID-19? A STRUC… Katarzyna C…\n10 ASV: Stochastic Volatility Models with or without Leverage       Yasuhiro Om…\n# ℹ 272 more rows\n\n\nAuthor(s)\n\n\nCode\ndf_author &lt;- Final_df_unique |&gt;\n  select(title,\n         author) |&gt;\n  unnest(cols = author)\n\nglimpse(df_author)\n\n\nRows: 643\nColumns: 11\n$ title               &lt;chr&gt; \"stochvolTMB: Likelihood Estimation of Stochastic …\n$ given               &lt;chr&gt; \"Jens\", \"Linh\", \"Yohei\", \"Pierre\", \"Jose Roberto\",…\n$ family              &lt;chr&gt; \"Wahl\", \"Nguyen\", \"Yamamoto\", \"Perron\", \"Loureiro\"…\n$ sequence            &lt;chr&gt; \"first\", \"first\", \"additional\", \"additional\", \"fir…\n$ ORCID               &lt;chr&gt; NA, NA, NA, NA, \"http://orcid.org/0000-0003-4643-1…\n$ authenticated.orcid &lt;lgl&gt; NA, NA, NA, NA, FALSE, NA, NA, FALSE, NA, NA, NA, …\n$ affiliation.name    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bank for …\n$ affiliation1.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ affiliation2.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ name                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ affiliation3.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nCode\nprint(df_author)\n\n\n# A tibble: 643 × 11\n   title        given family sequence ORCID authenticated.orcid affiliation.name\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;lgl&gt;               &lt;chr&gt;           \n 1 stochvolTMB… Jens  Wahl   first    &lt;NA&gt;  NA                  &lt;NA&gt;            \n 2 mbreaks: Es… Linh  Nguyen first    &lt;NA&gt;  NA                  &lt;NA&gt;            \n 3 mbreaks: Es… Yohei Yamam… additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 4 mbreaks: Es… Pier… Perron additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 5 World regio… Jose… Loure… first    http… FALSE               &lt;NA&gt;            \n 6 World regio… Juli… Incha… additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 7 World regio… Robe… Aguil… additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 8 World commo… Manoj Atolia first    http… FALSE               &lt;NA&gt;            \n 9 World commo… Shua… Feng   additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n10 Who should … Andr… Schne… first    &lt;NA&gt;  NA                  &lt;NA&gt;            \n# ℹ 633 more rows\n# ℹ 4 more variables: affiliation1.name &lt;chr&gt;, affiliation2.name &lt;chr&gt;,\n#   name &lt;chr&gt;, affiliation3.name &lt;chr&gt;\n\n\nLinks\n\n\nCode\ndf_link &lt;- Final_df |&gt;\n  select(\n    title,\n    link\n  ) |&gt;\n  unnest(\n    cols = link\n  )\n\nglimpse(df_link)\n\n\nReferences\n\n\nCode\ndf_references &lt;- Final_df |&gt;\n  select(\n    title,\n    reference\n  ) |&gt;\n  unnest(\n    cols = reference\n  )\n\nglimpse(df_references)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#n-grams",
    "href": "revsyslit.html#n-grams",
    "title": "Literature Review and Scientific Challenge",
    "section": "N-grams",
    "text": "N-grams\nInstead of analyzing individual words, N-gram analysis examines sequences of two or more words that frequently appear together (bi-grams, tri-grams).\nObjective: To identify more relevant phrases or compound terms in the literature, which may not be captured in the analysis of individual words.\nApplication: To discover compound terms like “volatility spillover,” “agricultural prices,” or “portfolio optimization” that frequently appear.\n\n\nCode\n# Filtrar apenas artigos com abstracts disponíveis\nFinal_df_filtered &lt;- Final_df_unique |&gt;\n  filter(!is.na(abstract))  # Remove os artigos com abstracts 'NA'\n\n# Verificar o resultado\nglimpse(Final_df_filtered)\n\n\nRows: 84\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 4]&gt;], [&lt;tbl_df[2 x 4]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"journal-article\", \"journal-article\", …\n$ url                    &lt;chr&gt; \"https://doi.org/10.1108/oxan-ga261626\", \"https…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"Review of Finance\"…\n$ short.container.title  &lt;chr&gt; NA, NA, \"Agricultural Economics\", \"Journal of F…\n$ publisher              &lt;chr&gt; \"Emerald\", \"Oxford University Press (OUP)\", \"Wi…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.1093/rof/rfad038\",…\n$ published.print        &lt;mth&gt; 2021 mai, 2024 mai, 2021 mar, 2021 fev, 2020 ab…\n$ score                  &lt;dbl&gt; 19.82961, 18.35711, 16.04426, 18.00167, 19.3061…\n$ reference.count        &lt;chr&gt; \"0\", \"62\", \"41\", \"49\", \"66\", \"64\", \"86\", \"36\", …\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"6\", \"9\", \"20\", \"5\", \"0\", \"3\", \"16\", …\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], [&lt;tbl_df[3 x 4]&gt;], [&lt;tbl_df…\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[62 x 9]&gt;], [&lt;tbl_df[41 x 12]&gt;…\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", \"1572-3097,1573-692X\", \"0169-5150,…\n\n\nThen we build corpus with only avaiable abstracts\n\n\nCode\n# Criar o corpus de texto a partir da coluna de resumos (abstracts) filtrados\ncorpus &lt;- Corpus(VectorSource(Final_df_filtered$abstract))\n\n# Visualizar parte do corpus\ninspect(corpus[1:3])\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] &lt;jats:p&gt;Demand will surge for the minerals for low-carbon technologies, but supply is concentrated and will lag, raising prices&lt;/jats:p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n[2] &lt;jats:title&gt;Abstract&lt;/jats:title&gt;\\n               &lt;jats:p&gt;This article studies exchange-traded funds’ (ETFs) price impact in the most ETF-dominated asset classes: volatility (VIX) and commodities. I propose a new way to measure ETF-related price distortions based on the specifics of futures contracts. This allows me to isolate a component in VIX futures prices that is strongly related to the rebalancing of ETFs. I derive a novel decomposition of ETF trading demand into leverage rebalancing, calendar rebalancing, and flow rebalancing, and show that trading against ETFs is risky. Leverage rebalancing has the largest effects on the ETF-related price component. This rebalancing amplifies price changes and exposes ETF counterparties to variance.&lt;/jats:p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n[3] &lt;jats:title&gt;Abstract&lt;/jats:title&gt;&lt;jats:p&gt;Teff is an ancient grain primarily produced in Ethiopia and providing more than 10% of the total calories consumed in the country. The grain is considered as “super grain” due to its nutritional qualities, and it has seen an increase in its demand and price in recent years. These trends raise public concerns about the affordability of the grain and the prevalence of food insecurity in Ethiopia. Therefore, we investigate the impacts of increasing teff prices on consumers’ welfare by regions. Using data from two waves of the Ethiopia Socioeconomic Survey 2013–2014 and 2015–2016, we examine the consumption patterns of cereals in Ethiopia and estimate a two‐stage structural demand system. We find that teff is the most own‐price inelastic grain in Ethiopia and an increase of 10% in teff prices will reduce consumer welfare by 0.81, 1.29, and 1.73 Birrs per week for the average rural, town, and urban consumers, respectively. These estimates correspond to 1.64, 2.31, and 2.46% of their weekly food budgets. We also find the negative effects of an increase in teff prices are smaller for the lower‐income groups as they have relatively lower expenditures on teff. Additionally, we analyze the effects of simultaneous changes in wheat and teff prices to measure the extent to which Ethiopia's food policy of distributing subsidized wheat could offset the consumer welfare loss due to an increase in teff prices.&lt;/jats:p&gt;\n\n\nclean corpus and apply the N-grams processing\n\n\nCode\n# Função para limpar o texto e remover termos indesejados\nclean_text &lt;- function(corpus){\n  # Remover tags HTML\n  corpus &lt;- tm_map(corpus, content_transformer(function(x) gsub(\"&lt;.*?&gt;\", \"\", x))) # Remove tags HTML\n  corpus &lt;- tm_map(corpus, content_transformer(tolower)) # Converter para minúsculas\n  corpus &lt;- tm_map(corpus, removePunctuation) # Remover pontuação\n  corpus &lt;- tm_map(corpus, removeNumbers) # Remover números\n  corpus &lt;- tm_map(corpus, removeWords, stopwords(\"english\")) # Remover stopwords em inglês\n  corpus &lt;- tm_map(corpus, stripWhitespace) # Remover espaços extras\n  \n  # Lista de termos indesejados a remover\n  termos_indesejados &lt;- c(\"jats\", \"title\", \"sec\", \"abstract\", \"type\", \"content\", \"of the\", \"in the\", \"and the\")\n  \n  # Remover termos indesejados\n  corpus &lt;- tm_map(corpus, removeWords, termos_indesejados)\n  \n  return(corpus)\n}\n# Limpar o corpus\ncorpus_clean &lt;- clean_text(corpus)\n\n# Criar um dataframe de texto limpo\nclean_text_df &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE)\n\n# Criar um tokenizador de bigramas com o corpus limpo\nbigram_data_clean &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE) |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# Contar a frequência dos bigramas\nbigram_filtered_clean &lt;- bigram_data_clean |&gt;\n  count(bigram, sort = TRUE)\n\n# Filtrar os bigramas que não estão vazios ou não são códigos\nbigram_filtered_clean &lt;- bigram_filtered_clean |&gt;\n  filter(!str_detect(bigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os bigramas mais frequentes\nhead(bigram_filtered_clean, 10)\n\n\n\n  \n\n\n\nNow we can plot bars with the most frequent bigrams:\n\n\nCode\n# Filtrar os 10 bigramas mais frequentes\ntop_bigrams_clean &lt;- bigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_bigrams_clean, aes(x = reorder(bigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Bigramas Mais Frequentes\", x = \"Bigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nWe can expand for three grams:\n\n\nCode\n# Tokenização em trigramas\ntrigram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(trigram, text, token = \"ngrams\", n = 3)\n\n# Contar a frequência dos trigramas\ntrigram_filtered_clean &lt;- trigram_data_clean |&gt;\n  count(trigram, sort = TRUE)\n\n# Filtrar os trigramas indesejados\ntrigram_filtered_clean &lt;- trigram_filtered_clean |&gt;\n  filter(!str_detect(trigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os trigramas mais frequentes\nhead(trigram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 trigramas mais frequentes\ntop_trigrams_clean &lt;- trigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_trigrams_clean, aes(x = reorder(trigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Trigramas Mais Frequentes\", x = \"Trigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nAnd quadrigrams:\n\n\nCode\n# Tokenização em quadrigramas\nquadrigram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(quadrigram, text, token = \"ngrams\", n = 4)\n\n# Contar a frequência dos quadrigramas\nquadrigram_filtered_clean &lt;- quadrigram_data_clean |&gt;\n  count(quadrigram, sort = TRUE)\n\n# Filtrar os quadrigram indesejados\nquadrigram_filtered_clean &lt;- quadrigram_filtered_clean |&gt;\n  filter(!str_detect(quadrigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os quadrigram mais frequentes\nhead(quadrigram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 quadrigramas mais frequentes\ntop_quadrigrams_clean &lt;- quadrigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_quadrigrams_clean, aes(x = reorder(quadrigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Quadrigramas Mais Frequentes\", x = \"Quadrigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nAnd finnaly the darkest pentagram:\n\n\nCode\n# Tokenização em pentagram\npentagram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(pentagram, text, token = \"ngrams\", n = 5)\n\n# Contar a frequência dos pentagram\npentagram_filtered_clean &lt;- pentagram_data_clean |&gt;\n  count(pentagram, sort = TRUE)\n\n# Filtrar os pentagram indesejados\npentagram_filtered_clean &lt;- pentagram_filtered_clean |&gt;\n  filter(!str_detect(pentagram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os quadrigram mais frequentes\nhead(pentagram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 pentagrams mais frequentes\ntop_pentagrams_clean &lt;- pentagram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_pentagrams_clean, aes(x = reorder(pentagram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Pentagramas Mais Frequentes\", x = \"Pentagramas\", y = \"Frequência\")",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#word-co-occurrence-analysis",
    "href": "revsyslit.html#word-co-occurrence-analysis",
    "title": "Literature Review and Scientific Challenge",
    "section": "Word Co-Occurrence Analysis",
    "text": "Word Co-Occurrence Analysis\nWhat it is: Examine which words tend to appear together in a text.\nObjective: Explore the relationships between different terms and how they are connected within a broader context.\nApplication: Identify patterns in the associations between important terms such as “volatility” and “crisis” or “agricultural prices” and “portfolio optimization.”\n\n\nCode\n# Carregar pacotes necessários\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tm)\n\n# Passo 1: Limpeza do Corpus (assumindo que o corpus já foi criado e limpo anteriormente)\n# Criar o dataframe com os textos limpos (usando o corpus_clean criado anteriormente)\ntext_df &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE)\n\n# Passo 2: Tokenização para Bigramas (palavras em pares)\nbigrams &lt;- text_df %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# Passo 3: Separar os bigramas em duas colunas para fazer a análise de co-ocorrência\nbigram_separated &lt;- bigrams %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\n# Passo 4: Contar as co-ocorrências de palavras\nbigram_count &lt;- bigram_separated %&gt;%\n  count(word1, word2, sort = TRUE)\n\n# Passo 5: Filtrar co-ocorrências que aparecem mais de uma vez (ou outro limite que faça sentido)\nbigram_filtered &lt;- bigram_count %&gt;%\n  filter(n &gt; 1)\n\n# Passo 6: Criar o grafo de co-ocorrência usando o pacote igraph\nword_network &lt;- bigram_filtered %&gt;%\n  graph_from_data_frame()\n\n# Passo 7: Plotar a rede de co-ocorrência usando o pacote ggraph\nset.seed(1234)  # Para reprodutibilidade\nplotly::ggplotly(ggraph(word_network, layout = \"fr\") +  # Layout da rede (fr = force-directed)\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +  # Conexões entre palavras\n  geom_node_point(color = \"lightblue\", size = 5) +  # Nós das palavras\n  geom_node_text(aes(label = name), vjust = 1.5, hjust = 1.5) +  # Textos das palavras\n  theme_void() +  # Remover fundo e eixos\n  labs(title = \"Word Co-occurrence Network\", subtitle = \"Bigram Co-occurrences\")\n)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#tdm-and-lda",
    "href": "revsyslit.html#tdm-and-lda",
    "title": "Literature Review and Scientific Challenge",
    "section": "TDM and LDA",
    "text": "TDM and LDA\nWhat it is: Extract underlying topics from texts by grouping related keywords.\nObjective: Automatically discover the main themes discussed in a large collection of articles. Application: Help identify different research areas within a broader field, for example, whether articles on “sentiment analysis” focus more on price volatility, market predictions, or risk analysis.\nTo continue with our Exploratory Lit Rev, we first need to build the corpus:\n\n\nCode\n# Criar um corpus de texto a partir da coluna de resumos (abstracts)\ncorpus &lt;- Corpus(VectorSource(Final_df_unique$abstract))\n\n# Visualizar parte do corpus\ninspect(corpus[1:3])\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] &lt;jats:p&gt;Demand will surge for the minerals for low-carbon technologies, but supply is concentrated and will lag, raising prices&lt;/jats:p&gt;\n[2] &lt;NA&gt;                                                                                                                                    \n[3] &lt;NA&gt;                                                                                                                                    \n\n\nThen we proceed for the next step, cleaning the text:\nNow we can build the terms matrix (Term-Document Matrix - TDM), were each row represents an term and each column represents an document.\n\n\nCode\n# Criar a matriz de termos\ntdm &lt;- TermDocumentMatrix(corpus_clean)\n\n# Converter para um dataframe\ntdm_matrix &lt;- as.matrix(tdm)\n\n# Ver os termos mais frequentes\nterm_freq &lt;- rowSums(tdm_matrix)\nterm_freq_sorted &lt;- sort(term_freq, decreasing = TRUE)\n\n# Visualizar as palavras mais frequentes\nhead(term_freq_sorted, 10)\n\n\n  volatility       prices        price      markets        model       models \n         215           91           91           91           88           77 \n       study        stock       market agricultural \n          64           62           59           50 \n\n\nConvert the matrix for an LDA format:\n\n\nCode\n# Criar a matriz de termos a partir do corpus limpo\ntdm &lt;- TermDocumentMatrix(corpus_clean)\n\n# Converter a matriz de termos para um formato compatível com o LDA\ntdm_sparse &lt;- as.matrix(tdm)\n\n# Verifique se a ordem dos termos é correta\nterms &lt;- Terms(tdm)  # Extraia os termos da TDM\n\n\nOne wordcloud is an good view to see the most frequent terms:\n\n\nCode\n# Criar uma nuvem de palavras\nwordcloud(words = names(term_freq_sorted), freq = term_freq_sorted, min.freq = 5,\n          max.words=100, random.order=FALSE, colors=brewer.pal(8, \"Dark2\"))\n\n\n\n\n\n\n\n\n\nIf we needs an more quant analisys, we can generate an bar graph with the most frequent words:\n\n\nCode\n# Converter para dataframe\ndf_term_freq &lt;- data.frame(term = names(term_freq_sorted), freq = term_freq_sorted)\n\n# Filtrar as 10 palavras mais frequentes\ntop_terms &lt;- df_term_freq |&gt; top_n(10, freq)\n\n# Criar gráfico de barras\nggplot(top_terms, aes(x = reorder(term, freq), y = freq)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Frequent Terms\", x = \"Terms\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nTo indentify the principal topics inside the papers, we can use Latent Dirichlet Allocation (LDA).\n\n\nCode\n# Definir o número de tópicos\nnum_topics &lt;- 3\n\n# Rodar o modelo LDA\nlda_model &lt;- LDA(tdm_sparse, k = num_topics, control = list(seed = 1234))\n\n# Extrair os tópicos e garantir que os termos sejam corretamente mapeados\ntopics &lt;- tidy(lda_model, matrix = \"beta\")\n\n# Conecte os IDs de termos reais\ntopics &lt;- topics |&gt;\n  mutate(term = terms[as.numeric(term)])  # Substituir os índices pelos termos reais\n\ntopics\n\n\n\n  \n\n\n\nSeeing the most representatives terms by topic:\n\n\nCode\n# Mostrar as palavras mais importantes para cada tópico\ntop_terms_per_topic &lt;- topics |&gt;\n  group_by(topic) |&gt;\n  top_n(10, beta) |&gt;\n  ungroup() |&gt;\n  arrange(topic, -beta)\n\n# Visualização corrigida com os termos reais\nlibrary(ggplot2)\nlibrary(forcats)\n\ntop_terms_per_topic |&gt;\n  mutate(term = fct_reorder(term, beta)) |&gt;\n  ggplot(aes(term, beta, fill = as.factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  labs(title = \"Top terms in each topic\", x = \"Terms\", y = \"Beta (Importance)\")\n\n\n\n\n\n\n\n\n\nThe graph generated is a topic modeling visualization based on an LDA (Latent Dirichlet Allocation) model. Each bar shows the most important words in each identified topic, derived from the text data provided. Let’s break down each part:\nInterpreting the Axes:\n\nX-axis (“Beta Importance”): The “beta” value indicates the probability that the word is relevant to the specific topic. The higher the beta value, the more important the word is in describing that topic.\nY-axis (Terms): Shows the terms (words) that are most representative within each topic.\n\nInterpretation by Topic:\nTopic 1 (red bar)\n\nThe most relevant terms include: “novel”, “exposes”, “way”, “contracts”, “volatility”, “surge”, “specifics”, “birrs”, “calendar”, “consumer.”\nThese terms suggest a set of documents focused on new contracts or methods related to volatility and consumption. The term “volatility” indicates that this topic likely discusses market fluctuations or innovations in managing contracts and consumption.\n\nTopic 2 (green bar)\n\nMost relevant terms: “contracts”, “propose”, “studies”, “etfs”, “correspond”, “lag”, “measure”, “new”, “consumption”, “based.”\nThis topic seems to be related to studies proposing the use of ETFs (Exchange-Traded Funds) and financial contracts, with a focus on consumption measures and market response lags. It could be discussing proposals for new studies on contracts and ETFs related to different consumption patterns.\n\nTopic 3 (blue bar)\n\nMost relevant terms: “consumed”, “propose”, “trading”, “impact”, “component”, “consumers”, “country”, “affordability”, “risky”, “amplifies.”\nThis topic is clearly related to consumption, trading, and impact across different regions or countries. The terms “affordability” and “risky” suggest that this topic might be discussing market accessibility and associated risks, perhaps in the context of trade policies.\n\nConclusion:\nThe graph highlights the most relevant words within each of the three topics, which seem to be:\n\nInnovations and contracts in the context of volatility and consumption.\nProposals and studies on ETFs and financial contracts, with a focus on consumption and performance measures.\nTrade and consumption, focusing on the impact on countries, market affordability, and potential associated risks.\n\nEach topic provides insight into different areas of study within the context of the analyzed documents. If you’re interested in a deeper analysis, you can explore how these topics relate to specific articles and contexts they cover.",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#exploratory-literature-review-with-text-mining-final-considerations",
    "href": "revsyslit.html#exploratory-literature-review-with-text-mining-final-considerations",
    "title": "Literature Review and Scientific Challenge",
    "section": "Exploratory Literature Review with Text Mining: Final Considerations",
    "text": "Exploratory Literature Review with Text Mining: Final Considerations\nIn this section, we applied text mining techniques to explore the main themes and trends in the literature related to sentiment analysis, volatility models, structural breaks, and multiobjective portfolio optimization in agricultural markets. The Latent Dirichlet Allocation (LDA) method was used to extract latent topics from the abstracts of papers, providing insights into the key terms that characterize the body of research.\n\nKey Insights from Topic Modeling\nThree topics were identified through LDA, each represented by a set of significant terms with their respective importance (Beta values):\n\nTopic 1 primarily emphasizes GARCH models, including terms like “bekk-garch” and “dcc-garch,” as well as terms related to market quality, volatility spillovers (“vov”), and settlement effects. This topic highlights a focus on econometric models used to assess volatility and risk in financial markets, with applications to agricultural commodities.\nTopic 2 features terms such as “return,” “option-impli,” and “long,” indicating that this topic revolves around the long-term returns and option pricing in agricultural markets. Other terms like “find” and “pcs” suggest explorations of statistical methods used to identify patterns and model market behavior.\nTopic 3 focuses on spillovers and global market trends, with terms like “spillov,” “precis,” “global,” and “detect.” This topic suggests an emphasis on crisis detection, market interdependence, and the global nature of agricultural commodities. The presence of “india” and “japan” also suggests the regional analysis of agricultural markets.\n\n\n\nImplications for Future Research\nThe results of this exploratory text mining analysis indicate that the literature in the field of agricultural market volatility, sentiment analysis, and portfolio optimization is deeply rooted in econometric modeling, risk assessment, and global market interconnections. Moving forward, researchers may want to delve further into the integration of sentiment metrics with econometric models to enhance predictive capabilities, especially in the context of volatility and structural breaks.\nBy leveraging these insights, future work could focus on developing multiobjective optimization strategies that incorporate both financial metrics and qualitative sentiment data, offering a more holistic approach to managing risk and improving investment decisions in agricultural markets.",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#frank-fabozzi",
    "href": "revsyslit.html#frank-fabozzi",
    "title": "Literature Review and Scientific Challenge",
    "section": "Frank Fabozzi",
    "text": "Frank Fabozzi\nScholar link https://scholar.google.com/citations?user=tqXS4IMAAAAJ&hl=en\n\n\nCode\n## Define the id for Frank Fabozzi\nid &lt;- 'tqXS4IMAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Frank J Fabozzi\"\n\n\nCode\nl$affiliation\n\n\n[1] \"EDHEC Business School\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 90\n\n\nCode\nl$i10_index\n\n\n[1] 469\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[6])\n\n\n[1] \"Project financing\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[6])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and David Ardia\nids &lt;- c('tqXS4IMAAAAJ', 'BPNrOUYAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'BPNrOUYAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nget_profile('BPNrOUYAAAAJ')$name\n\n\n[1] \"David Ardia\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('BPNrOUYAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#duan-li",
    "href": "revsyslit.html#duan-li",
    "title": "Literature Review and Scientific Challenge",
    "section": "Duan Li",
    "text": "Duan Li\nScholar link https://scholar.google.com/citations?user=e0IkYKcAAAAJ&hl=en\n\n\nCode\n## Define the id for Duan Li\nid &lt;- 'e0IkYKcAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Duan Li\"\n\n\nCode\nl$affiliation\n\n\n[1] \"City University of Hong Kong + The Chinese University of Hong Kong + University of Virginia\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 55\n\n\nCode\nl$i10_index\n\n\n[1] 188\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"Optimal dynamic portfolio selection: Multiperiod mean‐variance formulation\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Duan Li\nids &lt;- c('tqXS4IMAAAAJ', 'e0IkYKcAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'e0IkYKcAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"Duan Li\"\n\n\nCode\nget_profile('e0IkYKcAAAAJ')$name\n\n\n[1] \"Duan Li\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('e0IkYKcAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#woo-chang-kim",
    "href": "revsyslit.html#woo-chang-kim",
    "title": "Literature Review and Scientific Challenge",
    "section": "Woo Chang Kim",
    "text": "Woo Chang Kim\nScholar link https://scholar.google.com/citations?user=e0IkYKcAAAAJ&hl=en\n\n\nCode\n## Define the id for Woo Chang Kim\nid &lt;- '7NmBs1kAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\nl$affiliation\n\n\n[1] \"Professor, Industrial and Systems Engineering, KAIST\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 20\n\n\nCode\nl$i10_index\n\n\n[1] 36\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"Dynamic asset allocation for varied financial markets under regime switching framework\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Woo Chang Kim\nids &lt;- c('tqXS4IMAAAAJ', '7NmBs1kAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- '7NmBs1kAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\nget_profile('7NmBs1kAAAAJ')$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('7NmBs1kAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#david-ardia",
    "href": "revsyslit.html#david-ardia",
    "title": "Literature Review and Scientific Challenge",
    "section": "David Ardia",
    "text": "David Ardia\nScholar link https://scholar.google.com/citations?hl=en&user=BPNrOUYAAAAJ\n\n\nCode\n## Define the id for David Ardia\nid &lt;- 'BPNrOUYAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nl$affiliation\n\n\n[1] \"HEC Montréal & GERAD\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 25\n\n\nCode\nl$i10_index\n\n\n[1] 40\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"DEoptim: An R package for global optimization by differential evolution\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and David Ardia\nids &lt;- c('tqXS4IMAAAAJ', 'BPNrOUYAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'BPNrOUYAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nget_profile('BPNrOUYAAAAJ')$name\n\n\n[1] \"David Ardia\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('BPNrOUYAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#herman-koene-van-dijk",
    "href": "revsyslit.html#herman-koene-van-dijk",
    "title": "Literature Review and Scientific Challenge",
    "section": "Herman Koene Van Dijk",
    "text": "Herman Koene Van Dijk\nScholar link https://scholar.google.com/citations?user=8y5_FWQAAAAJ&hl=en\n\n\nCode\n## Define the id for Herman Koene Van Dijk\nid &lt;- 'FWQAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\nl$affiliation\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\nl$i10_index\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Herman Van Dijk\nids &lt;- c('tqXS4IMAAAAJ', 'FWQAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'FWQAAAAJ&hl'\nget_profile(coautorias)$name\n\nget_profile('FWQAAAAJ')$name\n\n\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('FWQAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#bernhard-pfaff",
    "href": "revsyslit.html#bernhard-pfaff",
    "title": "Literature Review and Scientific Challenge",
    "section": "Bernhard Pfaff",
    "text": "Bernhard Pfaff\n\nGitHub https://github.com/bpfaff/\nScholar https://scholar.google.com.br/scholar?q=bernhard+pfaff&hl=pt-BR&as_sdt=0&as_vis=1&oi=scholart",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  }
]