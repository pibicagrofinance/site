[
  {
    "objectID": "predictive_model.html#data-collection-and-preprocessing",
    "href": "predictive_model.html#data-collection-and-preprocessing",
    "title": "Final Project: Time Series Forecasting with LSTMs, Neural Networks Eng. Class",
    "section": "Data Collection and Preprocessing",
    "text": "Data Collection and Preprocessing\nPython libs\n\n\nCode\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\n#import yfinance as yf\nfrom yahooquery import Ticker\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import ParameterGrid\nimport psutil\nimport time\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Additional libraries for residual analysis\nfrom statsmodels.stats.diagnostic import acorr_ljungbox, het_breuschpagan\nimport statsmodels.api as sm\n\n\nFirst of all, we need to check the hardware availability:\n\n\nCode\n# Collecting hardware information\ndef get_system_info():\n    system_info = {\n        'CPU_cores': psutil.cpu_count(logical=True),\n        'CPU_freq_MHz': psutil.cpu_freq().current,\n        'Total_RAM_GB': round(psutil.virtual_memory().total / (1024 ** 3), 2),\n        'Available_RAM_GB': round(psutil.virtual_memory().available / (1024 ** 3), 2),\n        'GPU_info': 'Not available'  # Placeholder, can be expanded with libraries like GPUtil\n    }\n    return system_info\n\nsystem_info = get_system_info()\nprint(\"System Information:\", system_info)\n\n\nSystem Information: {'CPU_cores': 12, 'CPU_freq_MHz': 1800.0, 'Total_RAM_GB': 31.69, 'Available_RAM_GB': 10.41, 'GPU_info': 'Not available'}\n\n\nLoading data:\n\n\nCode\nfrom yahooquery import Ticker\nimport pandas as pd\nimport numpy as np\n\n# Definindo os tickers\ntickers = [\n    \"ZC=F\",  # Corn Futures\n    \"ZW=F\",  # Wheat Futures\n    \"KE=F\",  # KC HRW Wheat Futures\n    \"ZR=F\",  # Rough Rice Futures\n    \"GF=F\",  # Feeder Cattle Futures\n    \"ZM=F\",  # Soybean Meal Futures\n    \"ZL=F\",  # Soybean Oil Futures\n    \"ZS=F\"   # Soybean Futures\n]\n\nprint(\"\\nDownloading price data using yahooquery...\")\nticker_obj = Ticker(tickers)\ndata = ticker_obj.history(start=\"2015-01-01\")\n\n# Se o DataFrame possuir índice MultiIndex (com 'symbol' e 'date'), reinicializamos o índice e pivotamos\nif isinstance(data.index, pd.MultiIndex):\n    data = data.reset_index()\n    if 'close' in data.columns:\n        data = data.pivot(index='date', columns='symbol', values='close')\n    else:\n        print(\"Column 'close' not found in data.\")\nelse:\n    # Caso não possua MultiIndex, verifica se há a coluna 'close'\n    data = data['close'] if 'close' in data.columns else data\n\n# Converter o índice para datetime (tz-aware para UTC e, em seguida, remover a informação de fuso, ficando tz-naive)\ndata.index = pd.to_datetime(data.index, utc=True).tz_convert(None)\n\n# Se data for uma Series, converte para DataFrame\nif isinstance(data, pd.Series):\n    data = data.to_frame()\n\n# Tratamento de dados ausentes\nprint(\"\\nHandling missing data...\")\ndata.fillna(method='ffill', inplace=True)  # Preenchimento forward\ndata.dropna(axis=1, how='all', inplace=True)  # Remove colunas com todos os valores NaN\ndata.dropna(axis=0, how='any', inplace=True)  # Remove linhas com qualquer valor NaN\n\n# Verificando os dados\nprint(\"\\nData columns and their non-null counts:\")\nprint(data.count())\n\nif data.empty:\n    print(\"Data is empty after cleaning. Exiting.\")\n    exit()\n\n# Calculando os retornos logarítmicos\nreturns = np.log(data / data.shift(1)).dropna()\n\n# Verificando os retornos\nprint(\"\\nReturns DataFrame info:\")\nprint(returns.info())\nprint(returns.head())\n\nif returns.empty:\n    print(\"Returns DataFrame is empty. Exiting.\")\n    exit()\n\nreturns.head()  # Exibindo as séries temporais utilizadas (sem features adicionais)\n\n\n\nDownloading price data using yahooquery...\n\nHandling missing data...\n\nData columns and their non-null counts:\nsymbol\nGF=F    2563\nKE=F    2563\nZC=F    2563\nZL=F    2563\nZM=F    2563\nZR=F    2563\nZS=F    2563\nZW=F    2563\ndtype: int64\n\nReturns DataFrame info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2562 entries, 2015-01-05 00:00:00 to 2025-03-10 18:19:56\nData columns (total 8 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   GF=F    2562 non-null   float64\n 1   KE=F    2562 non-null   float64\n 2   ZC=F    2562 non-null   float64\n 3   ZL=F    2562 non-null   float64\n 4   ZM=F    2562 non-null   float64\n 5   ZR=F    2562 non-null   float64\n 6   ZS=F    2562 non-null   float64\n 7   ZW=F    2562 non-null   float64\ndtypes: float64(8)\nmemory usage: 180.1 KB\nNone\nsymbol          GF=F      KE=F      ZC=F      ZL=F      ZM=F      ZR=F  \\\ndate                                                                     \n2015-01-05  0.007673  0.012483  0.025570  0.023203  0.034462  0.003537   \n2015-01-06 -0.004330  0.010350 -0.002466 -0.000306  0.004866  0.002644   \n2015-01-07  0.004219 -0.017983 -0.021842  0.008832 -0.006222  0.004392   \n2015-01-08 -0.000111 -0.019956 -0.005060  0.018029 -0.019732 -0.010130   \n2015-01-09 -0.014284 -0.012001  0.015104 -0.001192  0.006896  0.002653   \n\nsymbol          ZS=F      ZW=F  \ndate                            \n2015-01-05  0.036483  0.013245  \n2015-01-06  0.010762  0.004658  \n2015-01-07  0.001664 -0.020919  \n2015-01-08 -0.007389 -0.021806  \n2015-01-09  0.006201 -0.005748  \n\n\n\n\n\n\n\n\nsymbol\nGF=F\nKE=F\nZC=F\nZL=F\nZM=F\nZR=F\nZS=F\nZW=F\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-05\n0.007673\n0.012483\n0.025570\n0.023203\n0.034462\n0.003537\n0.036483\n0.013245\n\n\n2015-01-06\n-0.004330\n0.010350\n-0.002466\n-0.000306\n0.004866\n0.002644\n0.010762\n0.004658\n\n\n2015-01-07\n0.004219\n-0.017983\n-0.021842\n0.008832\n-0.006222\n0.004392\n0.001664\n-0.020919\n\n\n2015-01-08\n-0.000111\n-0.019956\n-0.005060\n0.018029\n-0.019732\n-0.010130\n-0.007389\n-0.021806\n\n\n2015-01-09\n-0.014284\n-0.012001\n0.015104\n-0.001192\n0.006896\n0.002653\n0.006201\n-0.005748\n\n\n\n\n\n\n\nPlotting the time series of prices and returns side by side (2 per row)\n\n\nCode\n# Create a directory for plots if it doesn't exist\nplots_dir = 'plots'\nif not os.path.exists(plots_dir):\n    os.makedirs(plots_dir)\n\n# Plot prices\nprint(\"\\nPlotting time series of prices...\")\nnum_cols = 2  # Number of plots per row\nnum_plots = len(data.columns)\nnum_rows = (num_plots + num_cols - 1) // num_cols  # Ensure enough rows\n\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\naxs = axs.flatten()\n\nfor i, col in enumerate(data.columns):\n    axs[i].plot(data.index, data[col])\n    axs[i].set_title(f'Price Series - {col}')\n    axs[i].set_xlabel('Date')\n    axs[i].set_ylabel('Price')\n\n# Hide unused subplots\nfor j in range(i + 1, len(axs)):\n    fig.delaxes(axs[j])\n\nplt.tight_layout()\nplt.savefig(os.path.join(plots_dir, 'price_series.png'))\nplt.show()\nplt.close()\n\n# Plot returns\nprint(\"Plotting time series of returns...\")\nnum_plots_ret = len(returns.columns)\nnum_rows_ret = (num_plots_ret + num_cols - 1) // num_cols\n\nfig, axs = plt.subplots(num_rows_ret, num_cols, figsize=(15, 5 * num_rows_ret))\naxs = axs.flatten()\n\nfor i, col in enumerate(returns.columns):\n    axs[i].plot(returns.index, returns[col])\n    axs[i].set_title(f'Return Series - {col}')\n    axs[i].set_xlabel('Date')\n    axs[i].set_ylabel('Log Return')\n\n# Hide unused subplots\nfor j in range(i + 1, len(axs)):\n    fig.delaxes(axs[j])\n\nplt.tight_layout()\nplt.savefig(os.path.join(plots_dir, 'return_series.png'))\nplt.show()\nplt.close()\n\n\n\nPlotting time series of prices...\n\n\n\n\n\n\n\n\n\nPlotting time series of returns...\n\n\n\n\n\n\n\n\n\nPreprocessing data for LSTM time series modelling:\n\n\nCode\n# Function to prepare data for LSTM\ndef prepare_data(series, time_steps):\n    X, y = [], []\n    for i in range(len(series) - time_steps):\n        X.append(series[i:(i + time_steps)])\n        y.append(series[i + time_steps])\n    return np.array(X), np.array(y)\n\n\nSetting the parameters:\n\n\nCode\n# Defining parameters\ntime_steps = 5  # Number of time steps\nepochs = 10  # Reduced epochs for faster execution during testing\n\n# Dictionaries to store results\nmodels = {}\nhistories = {}\nmse_results = {}\nscalers = {}\npredictions = {}\nbest_params_dict = {}\nresiduals_analysis = {}\n\n# Directory to save reports and graphs\nreport_dir = 'report'\nif not os.path.exists(report_dir):\n    os.makedirs(report_dir)",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "predictive_model.html#lstm-time-series-model-fitting",
    "href": "predictive_model.html#lstm-time-series-model-fitting",
    "title": "Final Project: Time Series Forecasting with LSTMs, Neural Networks Eng. Class",
    "section": "LSTM time series model fitting",
    "text": "LSTM time series model fitting\n\n\nCode\n# Loop through each time series\nfor col in returns.columns:\n    print(f\"\\nProcessing column: {col}\")\n    series = returns[col].values.reshape(-1, 1)\n    \n    # Check if series is empty\n    if len(series) == 0:\n        print(f\"Series {col} is empty after preprocessing. Skipping.\")\n        continue\n    \n    print(f\"Series {col} has {len(series)} data points.\")\n    \n    # Normalizing data\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    series_scaled = scaler.fit_transform(series)\n    scalers[col] = scaler  # Storing the scaler for later inversion\n    \n    # Preparing data\n    X, y = prepare_data(series_scaled, time_steps)\n    \n    # Check if X and y are non-empty\n    if X.shape[0] == 0:\n        print(f\"Not enough data points in {col} after preparation. Skipping.\")\n        continue\n    \n    # Splitting into training and test sets\n    split_index = int(0.8 * len(X))\n    X_train_full, X_test = X[:split_index], X[split_index:]\n    y_train_full, y_test = y[:split_index], y[split_index:]\n    X_train_full = X_train_full.reshape((X_train_full.shape[0], X_train_full.shape[1], 1))\n    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n    \n    # Hyperparameter grid for Grid Search\n    param_grid = {\n        'neurons': [30, 50],\n        'learning_rate': [0.001, 0.01],\n        'activation': ['tanh', 'relu'],\n        'batch_size': [32, 64]\n    }\n    grid = ParameterGrid(param_grid)\n    \n    # Initializing variables to store best results\n    best_mse = float('inf')\n    best_params = None\n    best_model = None\n    \n    # Performing Grid Search\n    print(f\"Performing Grid Search for {col}...\")\n    for params in grid:\n        model = Sequential()\n        model.add(LSTM(params['neurons'], activation=params['activation'], input_shape=(time_steps, 1)))\n        model.add(Dense(1))\n        optimizer = Adam(learning_rate=params['learning_rate'])\n        model.compile(optimizer=optimizer, loss='mean_squared_error')\n        \n        history = model.fit(\n            X_train_full, y_train_full,\n            validation_data=(X_test, y_test),\n            epochs=epochs,\n            batch_size=params['batch_size'],\n            verbose=0\n        )\n        \n        y_pred = model.predict(X_test)\n        y_pred_inv = scaler.inverse_transform(y_pred)\n        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n        mse = mean_squared_error(y_test_inv, y_pred_inv)\n        \n        if mse &lt; best_mse:\n            best_mse = mse\n            best_params = params\n            best_model = model\n            best_y_pred = y_pred\n    \n    best_params_dict[col] = best_params\n    print(f\"Best parameters for {col}: {best_params} with MSE: {best_mse}\")\n    \n    models[col] = best_model\n    predictions[col] = {'Best Model': best_y_pred}\n    \n    # Inverting the normalization\n    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n    y_pred_inv = scaler.inverse_transform(best_y_pred)\n    \n    # Calculating MSE\n    mse_results[col] = {'Best Model': best_mse}\n    \n    # Visualization of results\n    plt.figure(figsize=(10, 4))\n    plt.plot(y_test_inv, label='Actual Value')\n    plt.plot(y_pred_inv, label='Prediction')\n    plt.title(f'Prediction vs Actual - {col} - Best Model')\n    plt.legend()\n    plt.savefig(os.path.join(report_dir, f'pred_vs_actual_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Residual Analysis\n    residuals = y_test_inv - y_pred_inv\n    \n    # Plotting residuals\n    plt.figure(figsize=(10, 4))\n    plt.plot(residuals, label='Residuals')\n    plt.title(f'Residuals - {col} - Best Model')\n    plt.legend()\n    plt.savefig(os.path.join(report_dir, f'residuals_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Ljung-Box test for autocorrelation in residuals\n    lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n    lb_pvalue = lb_test['lb_pvalue'].values[0]\n    \n    # Plotting residuals ACF\n    fig, ax = plt.subplots(figsize=(10, 4))\n    sm.graphics.tsa.plot_acf(residuals.squeeze(), lags=40, ax=ax)\n    plt.title(f'Residuals Autocorrelation Function - {col}')\n    plt.savefig(os.path.join(report_dir, f'acf_residuals_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Heteroscedasticity test (Breusch-Pagan Test)\n    exog = sm.add_constant(best_model.predict(X_test))\n    test_bp = het_breuschpagan(residuals, exog)\n    bp_pvalue = test_bp[3]\n    \n    # Convert p-values to Python float\n    lb_pvalue = float(lb_pvalue)\n    bp_pvalue = float(bp_pvalue)\n    \n    # Saving statistical test results\n    residuals_analysis[col] = {\n        'residuals': residuals.flatten().tolist(),\n        'ljung_box_pvalue': lb_pvalue,\n        'breusch_pagan_pvalue': bp_pvalue\n    }\n    \n    print(f\"Residual Analysis for {col}:\")\n    print(f\"Ljung-Box Test p-value: {lb_pvalue}\")\n    print(f\"Breusch-Pagan Test p-value: {bp_pvalue}\")\n\n# Displaying final results in a table\nprint(\"\\nFinal Results:\")\nresults_table = pd.DataFrame(mse_results)\nprint(results_table)\n\n\n\nProcessing column: GF=F\nSeries GF=F has 2562 data points.\nPerforming Grid Search for GF=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 139ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 153ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 121ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 120ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 921us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 140ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 956us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 132ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 133ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 152ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 121ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 11s 743ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step   \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 125ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \nBest parameters for GF=F: {'activation': 'relu', 'batch_size': 64, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.001863558358285238\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for GF=F:\nLjung-Box Test p-value: 3.114439298300653e-19\nBreusch-Pagan Test p-value: 5.572021567735004e-12\n\nProcessing column: KE=F\nSeries KE=F has 2562 data points.\nPerforming Grid Search for KE=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 130ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 152ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 880us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 172ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 153ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 151ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 139ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 716us/step\nBest parameters for KE=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.00041234203904946966\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for KE=F:\nLjung-Box Test p-value: 0.006590268761731124\nBreusch-Pagan Test p-value: 0.415827361757533\n\nProcessing column: ZC=F\nSeries ZC=F has 2562 data points.\nPerforming Grid Search for ZC=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 146ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 136ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 153ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 139ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 124ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 126ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 16s 1s/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 142ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 129ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 124ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 141ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \nBest parameters for ZC=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.00031450223427219255\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZC=F:\nLjung-Box Test p-value: 1.2706819946105328e-08\nBreusch-Pagan Test p-value: 0.8329847721504908\n\nProcessing column: ZL=F\nSeries ZL=F has 2562 data points.\nPerforming Grid Search for ZL=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 132ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 662us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 134ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 140ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 158ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 129ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 147ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 149ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 165ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 160ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 191ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \nBest parameters for ZL=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.0010778309310390621\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step 2/16 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step\nResidual Analysis for ZL=F:\nLjung-Box Test p-value: 4.039168937925409e-11\nBreusch-Pagan Test p-value: 0.0004951263920116207\n\nProcessing column: ZM=F\nSeries ZM=F has 2562 data points.\nPerforming Grid Search for ZM=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 159ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 150ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 176ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 160ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 167ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 990us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 136ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 151ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 133ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \nBest parameters for ZM=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.0003333248517181665\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for ZM=F:\nLjung-Box Test p-value: 0.159790669133358\nBreusch-Pagan Test p-value: 0.4065472611473179\n\nProcessing column: ZR=F\nSeries ZR=F has 2562 data points.\nPerforming Grid Search for ZR=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 137ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 148ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 145ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 134ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 154ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 122ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 150ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 142ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 151ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 151ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 122ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 136ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \nBest parameters for ZR=F: {'activation': 'tanh', 'batch_size': 64, 'learning_rate': 0.001, 'neurons': 50} with MSE: 0.12913776207143213\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for ZR=F:\nLjung-Box Test p-value: 1.2371562402040067e-10\nBreusch-Pagan Test p-value: 4.876451554339863e-08\n\nProcessing column: ZS=F\nSeries ZS=F has 2562 data points.\nPerforming Grid Search for ZS=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 134ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 706us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 142ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 148ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 139ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 193ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 131ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 5s 388ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 401ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 460ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 9s 622ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 415ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 5s 399ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 432ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step\nBest parameters for ZS=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.00020035861546976582\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 46ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for ZS=F:\nLjung-Box Test p-value: 0.0030306191622350835\nBreusch-Pagan Test p-value: 0.0002490492748648036\n\nProcessing column: ZW=F\nSeries ZW=F has 2562 data points.\nPerforming Grid Search for ZW=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 413ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 5s 370ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 432ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 406ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 12s 840ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/16 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 10s 702ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 11ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 459ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 7s 472ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 404ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 310ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 8s 555ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 316ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 326ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 5s 361ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 311ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 279ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \nBest parameters for ZW=F: {'activation': 'tanh', 'batch_size': 64, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.00041271055702721423\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step \nResidual Analysis for ZW=F:\nLjung-Box Test p-value: 0.002700318474036124\nBreusch-Pagan Test p-value: 0.5546230357603605\n\nFinal Results:\n                GF=F      KE=F      ZC=F      ZL=F      ZM=F      ZR=F  \\\nBest Model  0.001864  0.000412  0.000315  0.001078  0.000333  0.129138   \n\n              ZS=F      ZW=F  \nBest Model  0.0002  0.000413  \n\n\nSaving the results in a table:\n\n\nCode\n# Saving results to a CSV file\nresults_table.to_csv(os.path.join(report_dir, 'mse_results_updated.csv'), index=True)\n\n# Saving the best parameters found\nwith open(os.path.join(report_dir, 'best_params.json'), 'w') as f:\n    json.dump(best_params_dict, f, indent=4)\n\n# Saving the residual analysis\nwith open(os.path.join(report_dir, 'residuals_analysis.json'), 'w') as f:\n    json.dump(residuals_analysis, f, indent=4)\n\n\nPloting the MSEs for each time series:\n\n\nCode\n# Report: Documenting the results\n# Plotting the MSEs for each time series\nfor col in mse_results.keys():\n    mse_series = mse_results[col]\n    plt.figure(figsize=(10, 5))\n    plt.bar(mse_series.keys(), mse_series.values(), color='blue')\n    plt.title(f'MSE Comparison - {col}')\n    plt.ylabel('MSE')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(os.path.join(report_dir, f'mse_comparison_{col}.png'))\n    plt.close()\n\n\nSaving system info:\n\n\nCode\n# End timer\nend_time = datetime.now()\nelapsed_time = end_time - start_time  # This is a timedelta object\nprint(f\"Total execution time: {elapsed_time}\")\n\n# Save execution time to the report\nsystem_info['Execution_Time_seconds'] = elapsed_time.total_seconds()  # Convert to float for JSON\nwith open(os.path.join(report_dir, 'system_info.json'), 'w') as f:\n    json.dump(system_info, f, indent=4)\n\n\nTotal execution time: 0:10:16.011476\n\n\nGenerating an automatic final report:\n\n\nCode\n# Final Report: Generating a text document with the results\nreport_path = os.path.join(report_dir, 'final_report.txt')\nwith open(report_path, 'w') as report_file:\n    report_file.write(\"Final Project Report - Forecasting Commodity Returns with LSTM\\n\")\n    report_file.write(\"=\"*80 + \"\\n\\n\")\n    \n    report_file.write(\"1. Project Objectives:\\n\")\n    report_file.write(\"Forecast future returns of a commodity portfolio using LSTM Neural Networks.\\n\\n\")\n    \n    report_file.write(\"2. Methodology:\\n\")\n    report_file.write(\"- Collecting commodity price data.\\n\")\n    report_file.write(\"- Calculating logarithmic returns.\\n\")\n    report_file.write(\"- Normalizing the data.\\n\")\n    report_file.write(\"- Training LSTM models with different configurations.\\n\")\n    report_file.write(\"- Performing grid search to optimize hyperparameters.\\n\")\n    report_file.write(\"- Conducting residual analysis to identify uncaptured patterns and issues like autocorrelation or heteroscedasticity.\\n\\n\")\n    \n    report_file.write(\"3. Results:\\n\")\n    report_file.write(results_table.to_string())\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"4. Best Parameters Found (Grid Search):\\n\")\n    report_file.write(json.dumps(best_params_dict, indent=4))\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"5. Residual Analysis:\\n\")\n    for col, res in residuals_analysis.items():\n        report_file.write(f\"Residual Analysis for {col}:\\n\")\n        report_file.write(f\"Ljung-Box Test p-value: {res['ljung_box_pvalue']}\\n\")\n        report_file.write(f\"Breusch-Pagan Test p-value: {res['breusch_pagan_pvalue']}\\n\\n\")\n    report_file.write(\"\\n\")\n    \n    report_file.write(\"6. Conclusions:\\n\")\n    report_file.write(\"The study demonstrated the importance of proper hyperparameter selection and model architecture for forecasting financial returns. Regularization techniques and the choice of activation function significantly influenced model performance. The residual analysis highlighted the need to consider autocorrelation and heteroscedasticity in modeling financial time series.\\n\\n\")\n    \n    report_file.write(\"7. Recommendations for Future Work:\\n\")\n    report_file.write(\"- Implement additional regularization techniques, such as DropConnect or Batch Normalization.\\n\")\n    report_file.write(\"- Explore more advanced architectures, like GRU or bidirectional models.\\n\")\n    report_file.write(\"- Increase the dataset to improve the models' generalization capacity.\\n\")\n    report_file.write(\"- Use more robust cross-validation methods to assess model stability.\\n\")\n    report_file.write(\"- Integrate other features, such as technical indicators or macroeconomic variables, to enrich model inputs.\\n\")\n    report_file.write(\"- Consider hybrid models that combine Machine Learning techniques with traditional statistical models.\\n\")\n    \n    report_file.write(\"\\nSystem Information and Execution Time:\\n\")\n    report_file.write(json.dumps(system_info, indent=4))\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"End of Report.\\n\")",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html",
    "href": "garch_hawkes_simmodel.html",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "",
    "text": "Forecasting asset prices is a crucial topic in both academic research and risk management. Among the widely used models, the GARCH(1,1) model is popular for modeling the conditional volatility of returns, while the Hawkes process is effective in modeling the occurrence of extreme events (or “jumps”) that are not captured by traditional volatility measures.\nIn this paper, we integrate both models. The GARCH(1,1) model with a skewed Student‑t distribution is employed to capture the dynamics of conditional volatility. The model is specified as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha_1 \\varepsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2,\n\\]\nwhere \\(\\varepsilon_t\\) are the shocks and \\(\\sigma_t\\) is the conditional volatility. The use of a skewed Student‑t distribution (denoted as sstd in the rugarch package) allows us to capture heavy tails and asymmetry—characteristics often observed in financial return data.\nNext, standardized residuals are computed as:\n\\[\n\\tilde{\\varepsilon}_t = \\frac{\\varepsilon_t}{\\sigma_t},\n\\]\nand extreme events are defined as those for which \\(|\\tilde{\\varepsilon}_t| &gt; 2\\). These extreme events are then used to calibrate a Hawkes process with the intensity function:\n\\[\n\\lambda(t) = \\mu_h + \\alpha_h \\sum_{t_j &lt; t} e^{-\\beta_h (t-t_j)},\n\\]\nwhere \\(\\mu_h\\) is the baseline intensity, \\(\\alpha_h\\) quantifies the influence of past events, and \\(\\beta_h\\) is the decay rate.\nFinally, the forecasted baseline returns from the GARCH model are combined with the simulated jumps from the Hawkes process to generate three future price trajectories for the next 252 trading days.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#systematic-literature-review",
    "href": "garch_hawkes_simmodel.html#systematic-literature-review",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Systematic Literature Review",
    "text": "Systematic Literature Review\nForecasting financial time series has been extensively studied using econometric models and machine learning techniques. Among these models, Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models are widely used to model conditional volatility, while the Hawkes process is employed to capture the occurrence of extreme events and self-exciting shocks. However, the integration of these two models remains underexplored, presenting a significant gap in the literature.\n\n1. GARCH Models and Their Extensions\nGARCH models are traditionally applied to capture conditional heteroskedasticity in financial returns. Previous studies have explored various extensions of the GARCH model, including:\n\nGARCH combined with Neural Networks: The GARCH-GRNN model has been proposed to enhance forecasting by combining the statistical modeling of GARCH with the flexibility of neural networks (Li et al., 2005).\nBilinear-GARCH (BL-GARCH) Models: This approach augments the traditional GARCH model by incorporating bilinear modeling to capture more complex nonlinearities, demonstrating improved performance in financial time series forecasting (Oyewale et al., 2013).\nHybrid GARCH-Deep Learning Models: Models integrating deep neural networks with GARCH, such as GRU-GARCH, have been tested for volatility and risk forecasting (Michańków et al., 2023).\n\nWhile these approaches improve volatility forecasting, they still fail to fully capture the dynamics of extreme events and their interactions over time.\n\n\n2. Hawkes Processes in Extreme Event Modeling\nHawkes processes are widely used for modeling financial shocks due to their self-exciting nature. However, existing literature primarily focuses on applying these models to predict isolated extreme events, without integrating them with traditional volatility models. A recent study introduced the 2T-POT Hawkes model, which enhances conditional quantile forecasting for extreme log returns and outperforms GARCH-EVT models for financial risk prediction (Tomlinson et al., 2022).\nDespite this advancement, the study does not explicitly combine Hawkes processes with GARCH models, which could further improve predictive performance by integrating both volatility modeling and extreme event structure.\n\n\n3. Gaps in the Literature and Justification for the Proposed Approach\nThe main gap in the literature lies in the lack of integration between GARCH models and Hawkes processes for financial time series forecasting. While several studies have explored variations of GARCH and machine learning techniques, no work has comprehensively addressed the combination of GARCH forecasts with Hawkes-based scenario simulation.\nThis paper aims to bridge this gap by:\n\nIntegrating GARCH(1,1) forecasts (with a skewed Student-t distribution) with a Hawkes process, allowing us to capture both the dynamics of conditional volatility and the temporal structure of extreme events.\nSimulating future scenarios that account not only for forecasted volatility but also for the occurrence of self-exciting shocks, resulting in more robust predictions than machine learning or deep learning models alone.\nOvercoming the limitations of existing models, which often fail to properly capture structural breaks and the self-excitation of extreme financial shocks.\n\nThe literature suggests that hybrid models hold promise for financial forecasting, yet the specific combination of GARCH and Hawkes processes remains largely unexplored. This study aims to contribute to this field by developing a methodological framework that enhances predictive accuracy and robustness in financial asset forecasting.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#data-preparation",
    "href": "garch_hawkes_simmodel.html#data-preparation",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.1. Data Preparation",
    "text": "2.1. Data Preparation\nFirst, we download historical corn price data and compute daily log-returns. Let \\(P_t\\) denote the closing price at time \\(t\\). The log-return is computed as:\n\\[\nr_t = \\ln(P_t) - \\ln(P_{t-1}).\n\\]\nThese returns form the basis for our subsequent volatility modeling.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#garch11-model-with-skewed-studentt-distribution",
    "href": "garch_hawkes_simmodel.html#garch11-model-with-skewed-studentt-distribution",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.2. GARCH(1,1) Model with Skewed Student‑t Distribution",
    "text": "2.2. GARCH(1,1) Model with Skewed Student‑t Distribution\nWe model the conditional variance of returns using a GARCH(1,1) process. The conditional variance is defined as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha_1\\,\\varepsilon_{t-1}^2 + \\beta_1\\,\\sigma_{t-1}^2,\n\\]\nwhere: -\\(\\varepsilon_t\\) are the shocks, -\\(\\sigma_t\\) is the conditional volatility, -\\(\\omega\\) is the constant term, -\\(\\alpha_1\\) represents the ARCH effect, -\\(\\beta_1\\) represents the GARCH effect.\nIn our model, we assume that the return innovations follow a skewed Student‑t distribution (denoted as sstd in the rugarch package), which is well-suited to capture heavy tails and asymmetry often observed in financial data.\nAfter fitting the model, we extract the conditional volatility \\(\\sigma_t\\) for each time \\(t\\).",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#identification-of-extreme-events",
    "href": "garch_hawkes_simmodel.html#identification-of-extreme-events",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.3. Identification of Extreme Events",
    "text": "2.3. Identification of Extreme Events\nTo identify extreme events, we compute the standardized residuals:\n\\[\n\\tilde{\\varepsilon}_t = \\frac{\\varepsilon_t}{\\sigma_t}.\n\\]\nWe define an event as “extreme” if:\n\\[\n\\left|\\tilde{\\varepsilon}_t\\right| &gt; \\tau,\n\\]\nwith a chosen threshold (for example,\\(\\tau=2\\)). These extreme events (or jumps) are used to calibrate the Hawkes process.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#calibration-of-the-hawkes-process",
    "href": "garch_hawkes_simmodel.html#calibration-of-the-hawkes-process",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.4. Calibration of the Hawkes Process",
    "text": "2.4. Calibration of the Hawkes Process\nThe Hawkes process is a self-exciting point process used to model the occurrence of extreme events. Its intensity function is given by:\n\\[\n\\lambda(t) = \\mu_h + \\alpha_h \\sum_{t_j &lt; t} e^{-\\beta_h (t-t_j)},\n\\]\nwhere:\n-\\(\\mu_h\\) is the baseline intensity, -\\(\\alpha_h\\) quantifies the impact of past events, -\\(\\beta_h\\) is the decay rate.\nWe calibrate the parameters \\((\\mu_h, \\alpha_h, \\beta_h)\\) by maximizing the log-likelihood function based on the historical occurrence times of extreme events. The log-likelihood function is expressed as:\n\\[\n\\mathcal{L}(\\mu_h, \\alpha_h, \\beta_h) = \\sum_{i=1}^{N} \\ln\\left(\\mu_h + \\alpha_h \\sum_{t_j &lt; t_i} e^{-\\beta_h (t_i-t_j)}\\right) - \\mu_h T - \\frac{\\alpha_h}{\\beta_h}\\sum_{j=1}^{N}\\left(1-e^{-\\beta_h (T-t_j)}\\right),\n\\]\nwhere:\n-\\(T\\) is the total observation period, -\\(N\\) is the number of extreme events.\nWe solve for the parameters using numerical optimization techniques.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#simulation-of-future-price-trajectories",
    "href": "garch_hawkes_simmodel.html#simulation-of-future-price-trajectories",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.5. Simulation of Future Price Trajectories",
    "text": "2.5. Simulation of Future Price Trajectories\nThe final step is to simulate future price trajectories over a forecast horizon (e.g., 252 trading days). The simulation involves two components:\n\nBaseline Returns:\nThese are generated using the GARCH forecast for the next 252 days, assuming that returns follow a normal distribution with forecasted mean and volatility.\nJumps:\nFor each day, if the Hawkes process simulates extreme events, jump magnitudes are sampled from the historical extreme returns and added to the baseline return.\n\nThe total return for day \\(t\\) is given by:\n\\[\nr_t^{\\text{total}} = r_t^{\\text{baseline}} + r_t^{\\text{jump}},\n\\]\nand the simulated price is computed as:\n\\[\nP_t = P_0 \\times \\exp\\left(\\sum_{i=1}^{t} r_i^{\\text{total}}\\right),\n\\]\nwhere \\(P_0\\) is the last observed price.\nWe repeat this simulation multiple times (e.g., 3 scenarios) to capture the uncertainty in the future price evolution.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#data-preparation-1",
    "href": "garch_hawkes_simmodel.html#data-preparation-1",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we download historical corn price data and compute the logarithmic returns.\n\n\nCode\n# Load required packages\nlibrary(quantmod)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(timetk)\nlibrary(patchwork)\nlibrary(plotly)\nlibrary(rugarch)\n\n# Download historical corn prices (symbol ZC=F) from Yahoo Finance\ngetSymbols(\"ZC=F\", src = \"yahoo\", from = \"2020-01-01\", to = Sys.Date())\n\n\n[1] \"ZC=F\"\n\n\nCode\n# Create a data frame with the date and closing price\nmilho_data &lt;- data.frame(\n  Date  = index(`ZC=F`),\n  Close = as.numeric(Cl(`ZC=F`))\n)\n\n# Calculate daily log returns and remove rows with NA\nmilho_data &lt;- milho_data |&gt;\n  mutate(Return = c(NA, diff(log(Close)))) |&gt;\n  na.omit()\n\n# Display the first few rows\nhead(milho_data)\n\n\n\n  \n\n\n\nThe historical corn price data have been successfully loaded with columns for Date, Close, and the daily log returns (Return). These returns will serve as the basis for modeling volatility.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#estimating-the-garch11-model-with-skewed-studentt-distribution",
    "href": "garch_hawkes_simmodel.html#estimating-the-garch11-model-with-skewed-studentt-distribution",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Estimating the GARCH(1,1) Model with Skewed Student‑t Distribution",
    "text": "Estimating the GARCH(1,1) Model with Skewed Student‑t Distribution\nIn this section, we model the conditional variance of returns using a GARCH(1,1) process. The model is defined as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha_1 \\varepsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2,\n\\]\nwhere: -$ _t$ are the shocks (unexpected returns), -$ _t$ is the conditional volatility, -$ $ is the constant term, -$ _1$ represents the ARCH effect (the impact of the previous period’s shock), -$ _1$ represents the GARCH effect (the persistence of past volatility).\nTo capture the heavy tails and asymmetry often observed in financial return data, we assume that the innovations follow a skewed Student‑t distribution (denoted as sstd in the rugarch package).\nThe following R code specifies and fits the GARCH(1,1) model with the skewed Student‑t distribution:\n\n\nCode\n# Create a vector of returns from the corn price data\nret &lt;- milho_data$Return\n\n# Specify the GARCH(1,1) model with the skewed Student‑t distribution\nspec &lt;- ugarchspec(\n  variance.model = list(\n    model = \"sGARCH\",\n    garchOrder = c(1, 1)\n  ),\n  mean.model = list(\n    armaOrder = c(0, 0),\n    include.mean = TRUE\n  ),\n  distribution.model = \"sstd\"  # skewed Student‑t\n)\n\n# Fit the model\nfit &lt;- ugarchfit(\n  spec   = spec,\n  data   = ret,\n  solver = \"hybrid\"\n)\n\n# Display the model summary\nshow(fit)\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(0,0,0)\nDistribution    : sstd \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.000423    0.000408   1.0369 0.299795\nomega   0.000022    0.000008   2.9239 0.003456\nalpha1  0.108857    0.028539   3.8142 0.000137\nbeta1   0.811656    0.046136  17.5926 0.000000\nskew    0.977761    0.041524  23.5466 0.000000\nshape   5.777605    0.851525   6.7850 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.000423    0.000409   1.0342 0.301044\nomega   0.000022    0.000011   2.0700 0.038454\nalpha1  0.108857    0.032841   3.3147 0.000917\nbeta1   0.811656    0.063952  12.6916 0.000000\nskew    0.977761    0.049087  19.9188 0.000000\nshape   5.777605    0.994373   5.8103 0.000000\n\nLogLikelihood : 3577.816 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.4951\nBayes        -5.4712\nShibata      -5.4951\nHannan-Quinn -5.4861\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                   0.008921  0.9248\nLag[2*(p+q)+(p+q)-1][2]  0.125596  0.9021\nLag[4*(p+q)+(p+q)-1][5]  3.025137  0.4025\nd.o.f=0\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                     0.4278  0.5131\nLag[2*(p+q)+(p+q)-1][5]    4.3204  0.2167\nLag[4*(p+q)+(p+q)-1][9]    7.0881  0.1919\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]     1.942 0.500 2.000  0.1634\nARCH Lag[5]     2.811 1.440 1.667  0.3184\nARCH Lag[7]     5.177 2.315 1.543  0.2072\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.2679\nIndividual Statistics:              \nmu     0.45670\nomega  0.13446\nalpha1 0.19537\nbeta1  0.15873\nskew   0.60654\nshape  0.07964\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.49 1.68 2.12\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value   prob sig\nSign Bias           1.2219 0.2220    \nNegative Sign Bias  0.8674 0.3859    \nPositive Sign Bias  1.6370 0.1019    \nJoint Effect        3.4392 0.3287    \n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     30.52      0.04551\n2    30     32.78      0.28651\n3    40     47.38      0.16775\n4    50     50.62      0.40957\n\n\nElapsed time : 0.3281171 \n\n\nThe model summary shows that the mean return$ $ is close to zero, which is expected for daily returns. The parameters\\(\\omega\\), \\(\\alpha_1\\), and$ _1$ indicate a low base level of volatility with high persistence, as evidenced by the sum \\(\\alpha_1 + \\beta_1\\) being close to 1. The skew and shape parameters reveal that the distribution has heavy tails and slight asymmetry, which justifies the use of a skewed Student‑t distribution for modeling the innovations.\nAfter fitting the model, we extract the conditional volatility, \\(\\sigma_t\\), for further analysis:\n\n\nCode\n# Extract the estimated conditional volatility and convert it to a numeric vector\ncond_vol &lt;- sigma(fit)\nmilho_data$Volatility &lt;- as.numeric(cond_vol)\n\n# Display the first few rows of the volatility data\nhead(milho_data[, c(\"Date\", \"Volatility\")])\n\n\n\n  \n\n\n\nThis conditional volatility will be used later in the process to identify extreme events and to simulate future price trajectories.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#visualizing-historical-data",
    "href": "garch_hawkes_simmodel.html#visualizing-historical-data",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Visualizing Historical Data",
    "text": "Visualizing Historical Data\nWe reformat the data to plot the series for Price, Log-Returns, and Volatility in faceted plots using timetk.\n\n\nCode\n# Convert data to long format\nmilho_data_long &lt;- milho_data |&gt;\n  select(Date, Close, Return, Volatility) |&gt;\n  pivot_longer(\n    cols = c(Close, Return, Volatility),\n    names_to = \"Serie\",\n    values_to = \"Valor\"\n  )\n\n# Set the order and labels for facets\nmilho_data_long$Serie &lt;- factor(milho_data_long$Serie,\n                                levels = c(\"Close\", \"Return\", \"Volatility\"),\n                                labels = c(\"Price\", \"Log-Returns\", \"Volatility\"))\n\n# Plot the faceted time series\nplot_faceted &lt;- milho_data_long |&gt;\n  group_by(Serie) |&gt;\n  plot_time_series(\n    .date_var    = Date,\n    .value       = Valor,\n    .interactive = FALSE,\n    .facet_ncol  = 1,\n    .smooth      = FALSE,\n    .title       = \"Corn Price, Log-Returns, and Conditional Volatility\"\n  ) +\n  theme(strip.background = element_rect(fill = \"white\", colour = \"white\"))\n\nggplotly(plot_faceted)",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#identifying-extreme-events-and-calibrating-the-hawkes-process",
    "href": "garch_hawkes_simmodel.html#identifying-extreme-events-and-calibrating-the-hawkes-process",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Identifying Extreme Events and Calibrating the Hawkes Process",
    "text": "Identifying Extreme Events and Calibrating the Hawkes Process\nWe compute the standardized residuals:\n\\[\n\\tilde{\\epsilon}_t = \\frac{\\mbox{log-return}_t}{\\sigma_t}\n\\]\nWe define extreme events as those with \\(|\\tilde{\\epsilon}_t|&gt;2\\)\n\n\nCode\n# Compute standardized residuals\nmilho_data &lt;- milho_data |&gt;\n  mutate(Standardized = Return / Volatility)\n\n# Define threshold for extreme events\nthreshold_std &lt;- 2\nextreme_idx &lt;- which(abs(milho_data$Standardized) &gt; threshold_std)\nextreme_returns &lt;- milho_data$Return[extreme_idx]\n\n# Convert dates of extreme events to time (in days relative to the first date)\nfirst_date &lt;- min(milho_data$Date)\nevent_times_hist &lt;- as.numeric(milho_data$Date[extreme_idx] - first_date)\n\n# Total observation time (in days)\nT_obs &lt;- as.numeric(max(milho_data$Date) - first_date)\n\ncat(\"Number of extreme events identified:\", length(extreme_idx), \"\\n\")\n\n\nNumber of extreme events identified: 64 \n\n\nThe number of extreme events indicates whether our threshold successfully captures significant shocks. These events are then used to calibrate the Hawkes process.\nNext, we calibrate the Hawkes process using a negative log-likelihood function. The intensity function for the Hawkes process is given by\n\\[\n\\lambda(t) = \\mu_h + \\alpha_h \\displaystyle \\sum_{t_j &lt; t} e^{-\\beta_h (t-t_j)}\n\\]\n\n\nCode\n# Negative log-likelihood function for the Hawkes process with exponential kernel\nneg_log_lik &lt;- function(params, event_times, T_total) {\n  mu_h    &lt;- params[1]\n  alpha_h &lt;- params[2]\n  beta_h  &lt;- params[3]\n  \n  if(mu_h &lt;= 0 || alpha_h &lt;= 0 || beta_h &lt;= 0) return(1e10)\n  \n  n &lt;- length(event_times)\n  log_sum &lt;- 0\n  for(i in seq_along(event_times)) {\n    ti &lt;- event_times[i]\n    if(i == 1) {\n      sum_exp &lt;- 0\n    } else {\n      sum_exp &lt;- sum(exp(-beta_h * (ti - event_times[1:(i-1)])))\n    }\n    lambda_ti &lt;- mu_h + alpha_h * sum_exp\n    log_sum &lt;- log_sum + log(lambda_ti)\n  }\n  integral_term &lt;- mu_h * T_total + (alpha_h / beta_h) * sum(1 - exp(-beta_h * (T_total - event_times)))\n  ll &lt;- log_sum - integral_term\n  return(-ll)  # negative for minimization\n}\n\n# Initial guess and calibration via optimization\ninit_par &lt;- c(mu_h = 0.1, alpha_h = 0.5, beta_h = 1.0)\n\nres_hawkes &lt;- optim(\n  par         = init_par,\n  fn          = neg_log_lik,\n  event_times = event_times_hist,\n  T_total     = T_obs,\n  method      = \"L-BFGS-B\",\n  lower       = c(1e-6, 1e-6, 1e-6),\n  upper       = c(Inf, Inf, Inf)\n)\n\nmu_h_hat    &lt;- res_hawkes$par[1]\nalpha_h_hat &lt;- res_hawkes$par[2]\nbeta_h_hat  &lt;- res_hawkes$par[3]\n\ncat(\"Calibrated Hawkes Parameters:\\n\")\n\n\nCalibrated Hawkes Parameters:\n\n\nCode\ncat(\"mu =\", mu_h_hat, \"\\n\")\n\n\nmu = 0.03387228 \n\n\nCode\ncat(\"alpha =\", alpha_h_hat, \"\\n\")\n\n\nalpha = 1e-06 \n\n\nCode\ncat(\"beta =\", beta_h_hat, \"\\n\")\n\n\nbeta = 5.569386 \n\n\nThe calibrated parameters (\\(u_h , \\alpha_h, \\beta_h\\)) describe the baseline intensity, the impact of past events, and the decay rate, respectively. These values will be used to simulate future extreme events.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#simulation-of-future-price-trajectories-1",
    "href": "garch_hawkes_simmodel.html#simulation-of-future-price-trajectories-1",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Simulation of Future Price Trajectories",
    "text": "Simulation of Future Price Trajectories\nUsing the GARCH forecast for the next 252 days and the Hawkes process for extreme events, we simulate three future price trajectories. The forecasted baseline returns are combined with jumps to obtain the total returns. The price path is computed as\n\\[\np_t = p_0 \\times \\exp{(\\displaystyle\\sum_{i=1}^{t}(\\mbox{baseline return}_i) + \\mbox{jump}_i) )}\n\\]\n\n\nCode\n# Forecast horizon: 252 days\nhorizon &lt;- 252\ngarch_forecast &lt;- ugarchforecast(fit, n.ahead = horizon)\nmu_forecast    &lt;- as.numeric(fitted(garch_forecast))\nsigma_forecast &lt;- as.numeric(sigma(garch_forecast))\n\n# Initial price: last historical price\nP0 &lt;- tail(milho_data$Close, 1)\n\n# Function to simulate future extreme events using Hawkes (thinning algorithm)\nsimulateHawkes &lt;- function(mu_h, alpha_h, beta_h, T_start, T_end, history = NULL) {\n  if(is.null(history)) history &lt;- numeric(0)\n  current_events &lt;- history\n  t &lt;- T_start\n  new_events &lt;- c()\n  \n  while(t &lt; T_end) {\n    n_recent &lt;- sum(current_events &gt; (t - 10))  # events in the last 10 days\n    lambda_bar &lt;- mu_h + alpha_h * n_recent\n    w &lt;- rexp(1, rate = lambda_bar)\n    t_candidate &lt;- t + w\n    if(t_candidate &gt; T_end) break\n    sum_exp &lt;- if(length(current_events[current_events &lt; t_candidate]) &gt; 0)\n      sum(exp(-beta_h * (t_candidate - current_events[current_events &lt; t_candidate]))) else 0\n    lambda_tc &lt;- mu_h + alpha_h * sum_exp\n    if(runif(1) &lt;= lambda_tc / lambda_bar) {\n      new_events &lt;- c(new_events, t_candidate)\n      current_events &lt;- c(current_events, t_candidate)\n    }\n    t &lt;- t_candidate\n  }\n  return(new_events)\n}\n\n# Simulate 3 trajectories\nn_sim &lt;- 3\nsimulated_paths &lt;- list()\n\nset.seed(123)\n\nfor(sim in 1:n_sim) {\n  new_event_times &lt;- simulateHawkes(mu_h_hat, alpha_h_hat, beta_h_hat,\n                                    T_start = T_obs,\n                                    T_end   = T_obs + horizon,\n                                    history = event_times_hist)\n  event_days &lt;- floor(new_event_times - T_obs) + 1\n  \n  # For each day, if there are events, sum the jumps. Jump values are sampled from the historical extreme returns.\n  jumps &lt;- rep(0, horizon)\n  if(length(event_days) &gt; 0) {\n    for(day in unique(event_days)) {\n      n_events &lt;- sum(event_days == day)\n      jump_vals &lt;- sample(extreme_returns, n_events, replace = TRUE)\n      jumps[day] &lt;- sum(jump_vals)\n    }\n  }\n  \n  # Simulate baseline returns using the GARCH forecast\n  baseline_returns &lt;- rnorm(horizon, mean = mu_forecast, sd = sigma_forecast)\n  \n  # Total return is the sum of baseline return and jumps\n  total_returns &lt;- baseline_returns + jumps\n  \n  # Price path: P_t = P0 * exp(cumsum(total_returns))\n  price_path &lt;- P0 * exp(cumsum(total_returns))\n  \n  simulated_paths[[sim]] &lt;- data.frame(\n    Day = 1:horizon,\n    Price = price_path,\n    Return = total_returns,\n    Trajectory = paste0(\"Sim\", sim)\n  )\n}\n\n# Combine simulated trajectories and add dates\nsim_df &lt;- do.call(rbind, simulated_paths)\nlast_date &lt;- tail(milho_data$Date, 1)\nsim_df &lt;- sim_df |&gt;\n  mutate(Date = as.Date(last_date) + Day)\n\n\nEach simulated trajectory combines the baseline returns (forecasted by the GARCH model) with the jumps generated by the Hawkes process. This yields realistic simulations of future prices that account for both dynamic volatility and extreme shocks.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#combining-historical-data-and-forecast-in-a-single-plot",
    "href": "garch_hawkes_simmodel.html#combining-historical-data-and-forecast-in-a-single-plot",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Combining Historical Data and Forecast in a Single Plot",
    "text": "Combining Historical Data and Forecast in a Single Plot\nFinally, we overlay the historical corn prices with the simulated forecast trajectories. A vertical red line marks the start of the forecast period.\n\n\nCode\n# Prepare historical data for plotting\nhistorical_df &lt;- milho_data |&gt;\n  select(Date, Close) |&gt;\n  rename(Price = Close)\n\n# Create the final plot: solid line for historical data, dashed lines for simulations,\n# and a vertical dotted line indicating the forecast start.\np_sim &lt;- ggplot() +\n  geom_line(data = historical_df, aes(x = Date, y = Price),\n            color = \"black\", size = 1) +\n  geom_line(data = sim_df, aes(x = Date, y = Price, color = Trajectory),\n            linetype = \"dashed\") +\n  geom_vline(xintercept = as.numeric(last_date),\n             linetype = \"dotted\", color = \"red\") +\n  labs(title = \"Corn Prices: Historical Data and 3 Simulated Trajectories (Next 252 Days)\",\n       x = \"Date\", y = \"Price\") +\n  theme_minimal()\n\nggplotly(p_sim)\n\n\n\n\n\n\nThe final plot shows the historical corn prices (solid black line) and the three simulated future trajectories (dashed colored lines) for the next 252 days. The vertical red dotted line indicates the start of the forecast period.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  }
]