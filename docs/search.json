[
  {
    "objectID": "time_series_portfolio.html",
    "href": "time_series_portfolio.html",
    "title": "Data Extraction for preselected commodities portfolio",
    "section": "",
    "text": "Abstract\n\n\n\nThis small document have the goal to share the time series extraction and the two basic features building, like price returns and their conditional variance…\n\n\n\n  \n\n\nIntro\n[… to be written …]\n\n\n\nPython codes\n\nPython libsLoading time seriesPrices log-returnsLog-returns conditional variances\n\n\n\n\nCode\n\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom arch import arch_model\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal\n\n\n\n\nThe portfolio contains the following commodities price returns:\n\nCorn Futures\nWheat Futures\nKC HRW Wheat Futures\nRough Rice Futures\nFeeder Cattle Futures\nSoyMeal Futures\nSoy Meal Futures\nSoyBeans Futures\n\n\n\nCode\n# Tickers for portfolio\nTICKERS = [\n    \"ZC=F\",  # Corn Futures\n    \"ZO=F\",  # Wheat Futures\n    \"KE=F\",  # KC HRW Wheat Futures\n    \"ZR=F\",  # Rough Rice Futures\n    \"GF=F\",  # Feeder Cattle Futures\n    \"ZS=F\",  # SoyMeal Futures\n    \"ZM=F\",  # Soybean Meal Futures\n    \"ZL=F\"   # SoyBeans Futures\n]\n\n\n# Downloading data from Yahoo Finance\nportfolio_prices = yf.download(TICKERS, start=\"2019-01-01\")['Adj Close']\n\n\n\n[                       0%                       ]\n[************          25%                       ]  2 of 8 completed\n[******************    38%                       ]  3 of 8 completed\n[**********************50%                       ]  4 of 8 completed\n[**********************62%*****                  ]  5 of 8 completed\n[**********************75%***********            ]  6 of 8 completed\n[**********************88%*****************      ]  7 of 8 completed\n[*********************100%***********************]  8 of 8 completed\n\n\nCode\nportfolio_prices.dropna(inplace=True)\n\n# Renaming columns for better readability\nportfolio_prices.columns = [\n    \"corn_fut\",\n    \"wheat_fut\",\n    \"KCWheat_fut\",\n    \"rice_fut\",\n    \"Feeder_Cattle\",\n    \"soymeal_fut\",\n    \"soyF_fut\",\n    \"soybeans_fut\"\n]\n\n\nShowing the prices time series side by side: (data in level)\n\n\nCode\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal, theme_void\n\n# Preparar os dados no formato long (necessário para plotnine/ggplot2)\nportfolio_prices_long = portfolio_prices.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Price')\n\ndef plot_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Price', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\np_prices = plot_with_ggplot(portfolio_prices_long, 'Commodity Prices Over Time', 'Price', background='white', fig_height=14, fig_width=8)\n\np_prices\n\n\n&lt;string&gt;:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1400)&gt;\n\n\n\n\n\n\n\n\n\n\n\nObtain the returns time series (first feature):\n\\[\n\\mbox{Price log returns}_t = ln(p_t) - ln(p_{t-1})\n\\]\n\n\nCode\n\n# Calculate log returns\nportfolio_log_returns = np.log(portfolio_prices / portfolio_prices.shift(1)).dropna()\nportfolio_log_returns.columns = [\n    \"ret_corn_fut\",\n    \"ret_wheat_fut\",\n    \"ret_KCWheat_fut\",\n    \"ret_rice_fut\",\n    \"ret_Feeder_Cattle\",\n    \"ret_soymeal_fut\",\n    \"ret_soyF_fut\",\n    \"ret_soybeans_fut\"\n]\n\n\nAnd plot it:\n\n\nCode\n# Preparar os dados no formato long para os log-retornos\nportfolio_log_returns_long = portfolio_log_returns.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Log Return')\n\ndef plot_log_returns_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Log Return', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\np_log_returns = plot_log_returns_with_ggplot(portfolio_log_returns_long, 'Log Returns of Commodities Over Time', 'Log Return', background='white', fig_height=12, fig_width=8)\n\n# Exibir o gráfico\np_log_returns\n\n\n&lt;string&gt;:3: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1200)&gt;\n\n\n\n\n\n\n\n\n\n\n\nAs risk measure, we use the conditional variances (volatilities), to deal better with day by day of the prices log-returns.\nThe GARCH(1,1) model with an asymmetric Student-t distribution is not directly available in most Python libraries. However, we can still use a GARCH(1,1) model with a standard Student-t distribution to estimate the conditional variance. The GARCH(1,1) model is represented as follows:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(r_t\\) is the log-return at time \\(t\\).\n\\(\\mu\\) is the mean of the returns.\n\\(\\epsilon_t\\) is the error term, modeled as conditional on past information.\n\\(\\sigma_t^2\\) is the conditional variance at time \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) are the parameters to be estimated, with \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) follows a Student-t distribution with \\(\\nu\\) degrees of freedom to capture the heavy tails observed in financial returns.\n\n\n\nCode\n# Initialize an empty DataFrame to store conditional variances\ncond_variances = pd.DataFrame(index=portfolio_log_returns.index, columns=portfolio_log_returns.columns)\n\n# Loop through each commodity's log-returns and fit a GARCH(1,1) model\nfor col in portfolio_log_returns.columns:\n    # Fit a GARCH(1,1) model with a Student-t distribution for each series of log returns\n    model = arch_model(portfolio_log_returns[col], vol='Garch', p=1, q=1, dist='t')\n    res = model.fit(disp='off')\n    \n    # Extract conditional variances and store them in the DataFrame\n    cond_variances[col] = res.conditional_volatility\n\n# Show the first few rows of the conditional variances DataFrame\ncond_variances.head()\n\n\n                           ret_corn_fut  ...  ret_soybeans_fut\nDate                                     ...                  \n2019-01-03 00:00:00+00:00      0.038394  ...          0.027239\n2019-01-04 00:00:00+00:00      0.048643  ...          0.032317\n2019-01-07 00:00:00+00:00      0.054103  ...          0.035044\n2019-01-08 00:00:00+00:00      0.056858  ...          0.034374\n2019-01-09 00:00:00+00:00      0.058676  ...          0.033381\n\n[5 rows x 8 columns]\n\n\nand visualizing them:\n\n\nCode\n# Preparar os dados no formato long para as variâncias condicionais\ncond_variances_long = cond_variances.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Conditional Variance')\n\n# Função para criar o gráfico com fundo branco ou transparente e ajustar o tamanho da figura\ndef plot_cond_variances_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Conditional Variance', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\n# Exemplo de uso para as variâncias condicionais das commodities\np_cond_variances = plot_cond_variances_with_ggplot(cond_variances_long, 'Conditional Variances Over Time (GARCH(1,1))', 'Conditional Variance', background='white', fig_height=12, fig_width=8)\n\np_cond_variances\n\n\n&lt;string&gt;:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~2\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1200)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR codes\n\nR packagesPortfolio set\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n#library(plotly)\nlibrary(rugarch)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(caTools)\nlibrary(PerformanceAnalytics)\nlibrary(MASS)\nlibrary(PortfolioAnalytics)\nlibrary(ROI)\nrequire(ROI.plugin.glpk)\nrequire(ROI.plugin.quadprog)\nlibrary(quadprog)\nlibrary(corpcor)\nlibrary(DEoptim)\nlibrary(cowplot) # devtools::install_github(\"wilkelab/cowplot/\")\nlibrary(lattice)\nlibrary(timetk)\n\n\n\n\nLoading time series data, for portfolio setting…\n\n\nCode\ntickers &lt;- c(\n         \"ZC=F\", # Corn Futures\n         \"ZO=F\", # Wheat Futures\n         \"KE=F\", # Futuros KC HRW Wheat Futures\n         \"ZR=F\", # Rough Rice Futures\n         \"GF=F\", # Feeder Cattle Futures\n         \"ZS=F\", # SoyMeal Futures \n         \"ZM=F\", # Futuros farelo soja\n         \"ZL=F\"  # SoyBeans Futures\n)\n\n\nObtain daily prices and their returns:\n\n\nCode\nportfolioPrices &lt;- NULL\n  for ( Ticker in tickers )\n    portfolioPrices &lt;- cbind(\n      portfolioPrices, \n      getSymbols.yahoo(\n        Ticker,\n        from = \"2019-01-01\",\n        auto.assign = FALSE\n      )[,4]\n    )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"corn_fut\",\n  \"wheat_fut\",\n  \"KCWheat_fut\",\n  \"rice_fut\",\n  \"Feeder_Cattle\",\n  \"soymeal_fut\",\n  \"soyF_fut\",\n  \"soybeans_fut\"\n)\n\ntail(portfolioPrices)\n\n\n           corn_fut wheat_fut KCWheat_fut rice_fut Feeder_Cattle soymeal_fut\n2024-11-15   424.00    356.75      540.00   1504.5       251.100      998.50\n2024-11-18   429.25    363.75      555.25   1516.5       251.825     1009.75\n2024-11-19   427.25    344.75      558.25   1509.5       254.250      998.50\n2024-11-20   430.25    344.75      561.75   1512.0       254.800      990.50\n2024-11-21   426.75    346.25      555.50   1517.0       255.125      977.75\n2024-11-22   425.50    349.25      554.25   1514.5       254.300      983.50\n           soyF_fut soybeans_fut\n2024-11-15    289.6        45.35\n2024-11-18    290.3        45.52\n2024-11-19    288.6        44.84\n2024-11-20    289.4        43.28\n2024-11-21    287.7        42.18\n2024-11-22    289.2        41.77\n\n\nPlotting the time series prices (in level):\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along( corn_fut )\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\nObtain the returns time series (first feature):\n\\[\n\\mbox{Price log returns}_t = ln(p_t) - ln(p_{t-1})\n\\]\n\n\nCode\n# Calculate log returns for the portfolio prices\nportfolioReturs &lt;- na.omit(diff(log(portfolioPrices))) |&gt; as.data.frame()\n\ncolnames(portfolioReturs) &lt;- c(\n  \"ret_corn_fut\",\n  \"ret_wheat_fut\",\n  \"ret_KCWheat_fut\",\n  \"ret_rice_fut\",\n  \"ret_Feeder_Cattle\",\n  \"ret_soymeal_fut\",\n  \"ret_soyF_fut\",\n  \"ret_soybeans_fut\"\n)\n\nglimpse(portfolioReturs)\n\n\nRows: 1,484\nColumns: 8\n$ ret_corn_fut      &lt;dbl&gt; 0.0105891128, 0.0085218477, -0.0019601444, -0.005903…\n$ ret_wheat_fut     &lt;dbl&gt; 0.0008980692, 0.0053715438, -0.0017873106, 0.0115608…\n$ ret_KCWheat_fut   &lt;dbl&gt; 0.0220892515, 0.0049529571, -0.0059464992, 0.0039682…\n$ ret_rice_fut      &lt;dbl&gt; 0.0083930387, 0.0053934919, 0.0174507579, 0.00813596…\n$ ret_Feeder_Cattle &lt;dbl&gt; -0.0096783375, -0.0111522135, 0.0075628145, 0.011068…\n$ ret_soymeal_fut   &lt;dbl&gt; 0.0061281529, 0.0102224954, 0.0030190774, -0.0065988…\n$ ret_soyF_fut      &lt;dbl&gt; 0.0054513913, 0.0076457646, 0.0097900861, -0.0018874…\n$ ret_soybeans_fut  &lt;dbl&gt; 0.0099858421, 0.0081286732, -0.0052938051, -0.002834…\n\n\nCode\n#portfolioReturs &lt;- as.timeSeries(portfolioReturs)\n\n\nPlot all time series and their returns:\n\n\nCode\nportfolioReturs |&gt; \n  mutate(\n    time = seq_along( ret_corn_fut )\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\nPlotting the histograms:\n\n\nCode\nportfolioPrices_df &lt;- as_tibble(portfolioPrices, rownames = \"date\")\nportfolioPrices_df$date &lt;- ymd(portfolioPrices_df$date)\n\nportfolioReturs_df &lt;- na.omit( ROC( portfolioPrices ), type = \"discrete\" ) |&gt;\n  as_tibble(rownames = \"date\")\nportfolioReturs_df$date &lt;- ymd(portfolioReturs_df$date)\ncolnames(portfolioReturs_df) &lt;- c(\n  \"date\",\n  \"ret_corn_fut\",\n  \"ret_wheat_fut\",\n  \"ret_KCWheat_fut\",\n  \"ret_rice_fut\",\n  \"ret_Feeder_Cattle\",\n  \"ret_soymeal_fut\",\n  \"ret_soyF_fut\",\n  \"ret_soybeans_fut\"\n)\n\n# Remover a coluna com nome NA\nportfolioReturs_df &lt;- portfolioReturs_df[, !is.na(colnames(portfolioReturs_df))]\n\n# Verificar novamente os nomes das colunas para garantir que estão corretos\ncolnames(portfolioReturs_df)\n\n\n[1] \"date\"              \"ret_corn_fut\"      \"ret_wheat_fut\"    \n[4] \"ret_KCWheat_fut\"   \"ret_rice_fut\"      \"ret_Feeder_Cattle\"\n[7] \"ret_soymeal_fut\"   \"ret_soyF_fut\"      \"ret_soybeans_fut\" \n\n\nCode\nportfolioReturs_long &lt;- portfolioReturs_df |&gt; \n  pivot_longer(\n    cols = -date, # Exclui a coluna de data\n    names_to = \"fut_type\", \n    values_to = \"returns\"\n  )\n\nggplot(portfolioReturs_long, aes(x = returns)) + \n  geom_histogram(aes(y = ..density..), binwidth = .01, color = \"black\", fill = \"white\") +\n  geom_density(alpha = .2, fill=\"lightgray\") +\n  theme_minimal() +\n  theme(\n    axis.line  = element_line(colour = \"black\"),\n    axis.text  = element_text(colour = \"black\"),  \n    axis.ticks = element_line(colour = \"black\"), \n    legend.position = c(.1,.9), \n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank()\n  ) +\n  theme(plot.title   = element_text(size = 10),  \n        axis.title.x = element_text(size = 7), \n        axis.title.y = element_text(size = 7)) + \n  labs(x = \"Returns\", y = \"Density\") +\n  facet_wrap(~fut_type, scales = \"free\", ncol = 2) \n\n\n\n\n\n\n\n\n\nAnd finnaly, the last feature, is called, the conditional variance (risk measure), obtained by GARCH(1,1) model, formalized as:\nThe GARCH(1,1) model with asymmetric Student-t distribution can be represented mathematically as:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(r_t\\) is the return at time \\(t\\).\n\\(\\mu\\) is the mean of the returns.\n\\(\\epsilon_t\\) is the error term, modeled as conditional on past information.\n\\(\\sigma_t^2\\) is the conditional variance at time \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) are the parameters to be estimated, with \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) follows an asymmetric Student-t distribution with \\(\\nu\\) degrees of freedom to better capture the heavy tails and skewness observed in financial returns.\n\n\n\nCode\n# Load necessary packages\nlibrary(rugarch)\n\n# Define the GARCH(1,1) model specification with Student-t distribution\nspec &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n  mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),\n  distribution.model = \"std\" # Using Student-t distribution\n)\n\n# Estimate the model for each asset in the portfolio and extract conditional variances\ngarch_models &lt;- list()\nconditional_variances &lt;- list()\n\nfor (i in colnames(portfolioReturs)) {\n  garch_models[[i]] &lt;- ugarchfit(spec, data = portfolioReturs[[i]])\n  conditional_variances[[i]] &lt;- sigma(garch_models[[i]])^2\n}\n\n# Convert conditional variances list to a data frame\nconditional_variances_df &lt;- do.call(cbind, conditional_variances) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(time = seq_along(conditional_variances[[1]]))\n\ncolnames(conditional_variances_df) &lt;- c(\n  \"cond_var_corn_fut\",\n  \"cond_var_wheat_fut\",\n  \"cond_var_KCWheat_fut\",\n  \"cond_var_rice_fut\",\n  \"cond_var_Feeder_Cattle\",\n  \"cond_var_soymeal_fut\",\n  \"cond_var_soyF_fut\",\n  \"cond_var_soybeans_fut\",\n  \"time\"\n)\n\n# Reshape data for plotting\nconditional_variances_long &lt;- conditional_variances_df %&gt;%\n  pivot_longer(!time, names_to = \"Variables\", values_to = \"Value\")\n\n\nAnd the plot of the conditional variance (risk):\n\n\nCode\nconditional_variances_long |&gt; \n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\nReferences\nGujarati, D., N. (2004) Basic Econometrics, fourth edition, The McGraw−Hill Companies\nHair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2019). Multivariate Data Analysis. Pearson.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on oct 2023.\n \n \n\n\n\nCode\n# Total timing to compile this Quarto document\n\nend_time = datetime.now()\ntime_diff = end_time - start_time\n\nprint(f\"Total Quarto document compiling time: {time_diff}\")\n\n\nTotal Quarto document compiling time: 0:00:29.937205",
    "crumbs": [
      "About",
      "Predictive Models",
      "Data Extraction for preselected commodities portfolio"
    ]
  },
  {
    "objectID": "projects.html#project-1",
    "href": "projects.html#project-1",
    "title": "Projects",
    "section": "Project 1",
    "text": "Project 1\n\nOur first Colab Notebook  here\n\n This project is related with the step News and Impact on price and volatilities dynamics"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "predictive_model.html#data-collection-and-preprocessing",
    "href": "predictive_model.html#data-collection-and-preprocessing",
    "title": "Final Project: Time Series Forecasting with LSTMs, Neural Networks Eng. Class",
    "section": "Data Collection and Preprocessing",
    "text": "Data Collection and Preprocessing\nPython libs\n\n\nCode\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import ParameterGrid\nimport psutil\nimport time\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Additional libraries for residual analysis\nfrom statsmodels.stats.diagnostic import acorr_ljungbox, het_breuschpagan\nimport statsmodels.api as sm\n\n\nFirst of all, we need to check the hardware availability:\n\n\nCode\n# Collecting hardware information\ndef get_system_info():\n    system_info = {\n        'CPU_cores': psutil.cpu_count(logical=True),\n        'CPU_freq_MHz': psutil.cpu_freq().current,\n        'Total_RAM_GB': round(psutil.virtual_memory().total / (1024 ** 3), 2),\n        'Available_RAM_GB': round(psutil.virtual_memory().available / (1024 ** 3), 2),\n        'GPU_info': 'Not available'  # Placeholder, can be expanded with libraries like GPUtil\n    }\n    return system_info\n\nsystem_info = get_system_info()\nprint(\"System Information:\", system_info)\n\n\nSystem Information: {'CPU_cores': 12, 'CPU_freq_MHz': 1800.0, 'Total_RAM_GB': 31.69, 'Available_RAM_GB': 15.43, 'GPU_info': 'Not available'}\n\n\nLoading data:\n\n\nCode\n# Defining the tickers\ntickers = [\n    \"ZC=F\",  # Corn Futures\n    \"ZW=F\",  # Wheat Futures\n    \"KE=F\",  # KC HRW Wheat Futures\n    \"ZR=F\",  # Rough Rice Futures\n    \"GF=F\",  # Feeder Cattle Futures\n    \"ZM=F\",  # Soybean Meal Futures\n    \"ZL=F\",  # Soybean Oil Futures\n    \"ZS=F\"   # Soybean Futures\n]\n\n# Downloading price data\nprint(\"\\nDownloading price data...\")\ndata = yf.download(tickers, start=\"2015-01-01\")['Close']\n\n# Handling missing data\nprint(\"\\nHandling missing data...\")\ndata.fillna(method='ffill', inplace=True)  # Forward fill\ndata.dropna(axis=1, how='all', inplace=True)  # Drop columns with all NaNs\ndata.dropna(axis=0, how='any', inplace=True)  # Drop rows with any NaNs\n\n# Verify data\nprint(\"\\nData columns and their non-null counts:\")\nprint(data.count())\n\nif data.empty:\n    print(\"Data is empty after cleaning. Exiting.\")\n    exit()\n\n# Calculating logarithmic returns\nreturns = np.log(data / data.shift(1)).dropna()\n\n# Verify returns\nprint(\"\\nReturns DataFrame info:\")\nprint(returns.info())\nprint(returns.head())\n\nif returns.empty:\n    print(\"Returns DataFrame is empty. Exiting.\")\n    exit()\n    \n\nreturns.head() # Showing time series used (without features)\n\n\n\nDownloading price data...\n\n\n[                       0%                       ][************          25%                       ]  2 of 8 completed[******************    38%                       ]  3 of 8 completed[**********************50%                       ]  4 of 8 completed[**********************62%*****                  ]  5 of 8 completed[**********************75%***********            ]  6 of 8 completed[**********************88%*****************      ]  7 of 8 completed[*********************100%***********************]  8 of 8 completed\n\n\n\nHandling missing data...\n\nData columns and their non-null counts:\nTicker\nGF=F    2491\nKE=F    2491\nZC=F    2491\nZL=F    2491\nZM=F    2491\nZR=F    2491\nZS=F    2491\nZW=F    2491\ndtype: int64\n\nReturns DataFrame info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2490 entries, 2015-01-05 00:00:00+00:00 to 2024-11-25 00:00:00+00:00\nData columns (total 8 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   GF=F    2490 non-null   float64\n 1   KE=F    2490 non-null   float64\n 2   ZC=F    2490 non-null   float64\n 3   ZL=F    2490 non-null   float64\n 4   ZM=F    2490 non-null   float64\n 5   ZR=F    2490 non-null   float64\n 6   ZS=F    2490 non-null   float64\n 7   ZW=F    2490 non-null   float64\ndtypes: float64(8)\nmemory usage: 175.1 KB\nNone\nTicker                         GF=F      KE=F      ZC=F      ZL=F      ZM=F  \\\nDate                                                                          \n2015-01-05 00:00:00+00:00  0.007673  0.012483  0.025570  0.023203  0.034462   \n2015-01-06 00:00:00+00:00 -0.004330  0.010350 -0.002466 -0.000306  0.004866   \n2015-01-07 00:00:00+00:00  0.004219 -0.017983 -0.021842  0.008832 -0.006222   \n2015-01-08 00:00:00+00:00 -0.000111 -0.019956 -0.005060  0.018029 -0.019732   \n2015-01-09 00:00:00+00:00 -0.014284 -0.012001  0.015104 -0.001192  0.006896   \n\nTicker                         ZR=F      ZS=F      ZW=F  \nDate                                                     \n2015-01-05 00:00:00+00:00  0.003537  0.036483  0.013245  \n2015-01-06 00:00:00+00:00  0.002644  0.010762  0.004658  \n2015-01-07 00:00:00+00:00  0.004392  0.001664 -0.020919  \n2015-01-08 00:00:00+00:00 -0.010130 -0.007389 -0.021806  \n2015-01-09 00:00:00+00:00  0.002653  0.006201 -0.005748  \n\n\n\n\n\n\n\n\n\n\n\nTicker\nGF=F\nKE=F\nZC=F\nZL=F\nZM=F\nZR=F\nZS=F\nZW=F\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-05 00:00:00+00:00\n0.007673\n0.012483\n0.025570\n0.023203\n0.034462\n0.003537\n0.036483\n0.013245\n\n\n2015-01-06 00:00:00+00:00\n-0.004330\n0.010350\n-0.002466\n-0.000306\n0.004866\n0.002644\n0.010762\n0.004658\n\n\n2015-01-07 00:00:00+00:00\n0.004219\n-0.017983\n-0.021842\n0.008832\n-0.006222\n0.004392\n0.001664\n-0.020919\n\n\n2015-01-08 00:00:00+00:00\n-0.000111\n-0.019956\n-0.005060\n0.018029\n-0.019732\n-0.010130\n-0.007389\n-0.021806\n\n\n2015-01-09 00:00:00+00:00\n-0.014284\n-0.012001\n0.015104\n-0.001192\n0.006896\n0.002653\n0.006201\n-0.005748\n\n\n\n\n\n\n\nPlotting the time series of prices and returns side by side (2 per row)\n\n\nCode\n# Create a directory for plots if it doesn't exist\nplots_dir = 'plots'\nif not os.path.exists(plots_dir):\n    os.makedirs(plots_dir)\n\n# Plot prices\nprint(\"\\nPlotting time series of prices...\")\nnum_cols = 2  # Number of plots per row\nnum_plots = len(data.columns)\nnum_rows = (num_plots + num_cols - 1) // num_cols  # Ensure enough rows\n\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\naxs = axs.flatten()\n\nfor i, col in enumerate(data.columns):\n    axs[i].plot(data.index, data[col])\n    axs[i].set_title(f'Price Series - {col}')\n    axs[i].set_xlabel('Date')\n    axs[i].set_ylabel('Price')\n\n# Hide unused subplots\nfor j in range(i + 1, len(axs)):\n    fig.delaxes(axs[j])\n\nplt.tight_layout()\nplt.savefig(os.path.join(plots_dir, 'price_series.png'))\nplt.show()\nplt.close()\n\n# Plot returns\nprint(\"Plotting time series of returns...\")\nnum_plots_ret = len(returns.columns)\nnum_rows_ret = (num_plots_ret + num_cols - 1) // num_cols\n\nfig, axs = plt.subplots(num_rows_ret, num_cols, figsize=(15, 5 * num_rows_ret))\naxs = axs.flatten()\n\nfor i, col in enumerate(returns.columns):\n    axs[i].plot(returns.index, returns[col])\n    axs[i].set_title(f'Return Series - {col}')\n    axs[i].set_xlabel('Date')\n    axs[i].set_ylabel('Log Return')\n\n# Hide unused subplots\nfor j in range(i + 1, len(axs)):\n    fig.delaxes(axs[j])\n\nplt.tight_layout()\nplt.savefig(os.path.join(plots_dir, 'return_series.png'))\nplt.show()\nplt.close()\n\n\n\nPlotting time series of prices...\n\n\n\n\n\n\n\n\n\nPlotting time series of returns...\n\n\n\n\n\n\n\n\n\nPreprocessing data for LSTM time series modelling:\n\n\nCode\n# Function to prepare data for LSTM\ndef prepare_data(series, time_steps):\n    X, y = [], []\n    for i in range(len(series) - time_steps):\n        X.append(series[i:(i + time_steps)])\n        y.append(series[i + time_steps])\n    return np.array(X), np.array(y)\n\n\nSetting the parameters:\n\n\nCode\n# Defining parameters\ntime_steps = 5  # Number of time steps\nepochs = 10  # Reduced epochs for faster execution during testing\n\n# Dictionaries to store results\nmodels = {}\nhistories = {}\nmse_results = {}\nscalers = {}\npredictions = {}\nbest_params_dict = {}\nresiduals_analysis = {}\n\n# Directory to save reports and graphs\nreport_dir = 'report'\nif not os.path.exists(report_dir):\n    os.makedirs(report_dir)",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "predictive_model.html#lstm-time-series-model-fitting",
    "href": "predictive_model.html#lstm-time-series-model-fitting",
    "title": "Final Project: Time Series Forecasting with LSTMs, Neural Networks Eng. Class",
    "section": "LSTM time series model fitting",
    "text": "LSTM time series model fitting\n\n\nCode\n# Loop through each time series\nfor col in returns.columns:\n    print(f\"\\nProcessing column: {col}\")\n    series = returns[col].values.reshape(-1, 1)\n    \n    # Check if series is empty\n    if len(series) == 0:\n        print(f\"Series {col} is empty after preprocessing. Skipping.\")\n        continue\n    \n    print(f\"Series {col} has {len(series)} data points.\")\n    \n    # Normalizing data\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    series_scaled = scaler.fit_transform(series)\n    scalers[col] = scaler  # Storing the scaler for later inversion\n    \n    # Preparing data\n    X, y = prepare_data(series_scaled, time_steps)\n    \n    # Check if X and y are non-empty\n    if X.shape[0] == 0:\n        print(f\"Not enough data points in {col} after preparation. Skipping.\")\n        continue\n    \n    # Splitting into training and test sets\n    split_index = int(0.8 * len(X))\n    X_train_full, X_test = X[:split_index], X[split_index:]\n    y_train_full, y_test = y[:split_index], y[split_index:]\n    X_train_full = X_train_full.reshape((X_train_full.shape[0], X_train_full.shape[1], 1))\n    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n    \n    # Hyperparameter grid for Grid Search\n    param_grid = {\n        'neurons': [30, 50],\n        'learning_rate': [0.001, 0.01],\n        'activation': ['tanh', 'relu'],\n        'batch_size': [32, 64]\n    }\n    grid = ParameterGrid(param_grid)\n    \n    # Initializing variables to store best results\n    best_mse = float('inf')\n    best_params = None\n    best_model = None\n    \n    # Performing Grid Search\n    print(f\"Performing Grid Search for {col}...\")\n    for params in grid:\n        model = Sequential()\n        model.add(LSTM(params['neurons'], activation=params['activation'], input_shape=(time_steps, 1)))\n        model.add(Dense(1))\n        optimizer = Adam(learning_rate=params['learning_rate'])\n        model.compile(optimizer=optimizer, loss='mean_squared_error')\n        \n        history = model.fit(\n            X_train_full, y_train_full,\n            validation_data=(X_test, y_test),\n            epochs=epochs,\n            batch_size=params['batch_size'],\n            verbose=0\n        )\n        \n        y_pred = model.predict(X_test)\n        y_pred_inv = scaler.inverse_transform(y_pred)\n        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n        mse = mean_squared_error(y_test_inv, y_pred_inv)\n        \n        if mse &lt; best_mse:\n            best_mse = mse\n            best_params = params\n            best_model = model\n            best_y_pred = y_pred\n    \n    best_params_dict[col] = best_params\n    print(f\"Best parameters for {col}: {best_params} with MSE: {best_mse}\")\n    \n    models[col] = best_model\n    predictions[col] = {'Best Model': best_y_pred}\n    \n    # Inverting the normalization\n    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n    y_pred_inv = scaler.inverse_transform(best_y_pred)\n    \n    # Calculating MSE\n    mse_results[col] = {'Best Model': best_mse}\n    \n    # Visualization of results\n    plt.figure(figsize=(10, 4))\n    plt.plot(y_test_inv, label='Actual Value')\n    plt.plot(y_pred_inv, label='Prediction')\n    plt.title(f'Prediction vs Actual - {col} - Best Model')\n    plt.legend()\n    plt.savefig(os.path.join(report_dir, f'pred_vs_actual_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Residual Analysis\n    residuals = y_test_inv - y_pred_inv\n    \n    # Plotting residuals\n    plt.figure(figsize=(10, 4))\n    plt.plot(residuals, label='Residuals')\n    plt.title(f'Residuals - {col} - Best Model')\n    plt.legend()\n    plt.savefig(os.path.join(report_dir, f'residuals_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Ljung-Box test for autocorrelation in residuals\n    lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n    lb_pvalue = lb_test['lb_pvalue'].values[0]\n    \n    # Plotting residuals ACF\n    fig, ax = plt.subplots(figsize=(10, 4))\n    sm.graphics.tsa.plot_acf(residuals.squeeze(), lags=40, ax=ax)\n    plt.title(f'Residuals Autocorrelation Function - {col}')\n    plt.savefig(os.path.join(report_dir, f'acf_residuals_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Heteroscedasticity test (Breusch-Pagan Test)\n    exog = sm.add_constant(best_model.predict(X_test))\n    test_bp = het_breuschpagan(residuals, exog)\n    bp_pvalue = test_bp[3]\n    \n    # Convert p-values to Python float\n    lb_pvalue = float(lb_pvalue)\n    bp_pvalue = float(bp_pvalue)\n    \n    # Saving statistical test results\n    residuals_analysis[col] = {\n        'residuals': residuals.flatten().tolist(),\n        'ljung_box_pvalue': lb_pvalue,\n        'breusch_pagan_pvalue': bp_pvalue\n    }\n    \n    print(f\"Residual Analysis for {col}:\")\n    print(f\"Ljung-Box Test p-value: {lb_pvalue}\")\n    print(f\"Breusch-Pagan Test p-value: {bp_pvalue}\")\n\n# Displaying final results in a table\nprint(\"\\nFinal Results:\")\nresults_table = pd.DataFrame(mse_results)\nprint(results_table)\n\n\n\nProcessing column: GF=F\nSeries GF=F has 2490 data points.\nPerforming Grid Search for GF=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 91ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 92ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\nBest parameters for GF=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.00010868478962423662\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\nResidual Analysis for GF=F:\nLjung-Box Test p-value: 0.902659956751392\nBreusch-Pagan Test p-value: 0.5397976927527175\n\nProcessing column: KE=F\nSeries KE=F has 2490 data points.\nPerforming Grid Search for KE=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 99ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\nBest parameters for KE=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.00038466229067952355\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 946us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \nResidual Analysis for KE=F:\nLjung-Box Test p-value: 0.042672065593137444\nBreusch-Pagan Test p-value: 0.5976967615779537\n\nProcessing column: ZC=F\nSeries ZC=F has 2490 data points.\nPerforming Grid Search for ZC=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\nBest parameters for ZC=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.0003084699606203519\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZC=F:\nLjung-Box Test p-value: 2.4321688763796456e-08\nBreusch-Pagan Test p-value: 0.5752530015247288\n\nProcessing column: ZL=F\nSeries ZL=F has 2490 data points.\nPerforming Grid Search for ZL=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 113ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 93ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 40ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 120ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\nBest parameters for ZL=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.0003659478498196799\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 842us/step\nResidual Analysis for ZL=F:\nLjung-Box Test p-value: 0.028354871362706188\nBreusch-Pagan Test p-value: 0.009850669008256404\n\nProcessing column: ZM=F\nSeries ZM=F has 2490 data points.\nPerforming Grid Search for ZM=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 99ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 93ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\nBest parameters for ZM=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.0003361053211469741\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZM=F:\nLjung-Box Test p-value: 0.12168771362420232\nBreusch-Pagan Test p-value: 0.16570924960299585\n\nProcessing column: ZR=F\nSeries ZR=F has 2490 data points.\nPerforming Grid Search for ZR=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 101ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 93ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 91ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\nBest parameters for ZR=F: {'activation': 'tanh', 'batch_size': 64, 'learning_rate': 0.001, 'neurons': 50} with MSE: 0.13865800350340546\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step  \nResidual Analysis for ZR=F:\nLjung-Box Test p-value: 8.9153034251227e-10\nBreusch-Pagan Test p-value: 1.9535432135951564e-09\n\nProcessing column: ZS=F\nSeries ZS=F has 2490 data points.\nPerforming Grid Search for ZS=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step 2/16 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 93ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 99ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\nBest parameters for ZS=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.00014765890147711775\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZS=F:\nLjung-Box Test p-value: 0.15423438464524167\nBreusch-Pagan Test p-value: 0.17142546188479935\n\nProcessing column: ZW=F\nSeries ZW=F has 2490 data points.\nPerforming Grid Search for ZW=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 102ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\nBest parameters for ZW=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.00040886616402550284\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\nResidual Analysis for ZW=F:\nLjung-Box Test p-value: 0.01822308241612975\nBreusch-Pagan Test p-value: 0.9482390552823244\n\nFinal Results:\n                GF=F      KE=F      ZC=F      ZL=F      ZM=F      ZR=F  \\\nBest Model  0.000109  0.000385  0.000308  0.000366  0.000336  0.138658   \n\n                ZS=F      ZW=F  \nBest Model  0.000148  0.000409  \n\n\nSaving the results in a table:\n\n\nCode\n# Saving results to a CSV file\nresults_table.to_csv(os.path.join(report_dir, 'mse_results_updated.csv'), index=True)\n\n# Saving the best parameters found\nwith open(os.path.join(report_dir, 'best_params.json'), 'w') as f:\n    json.dump(best_params_dict, f, indent=4)\n\n# Saving the residual analysis\nwith open(os.path.join(report_dir, 'residuals_analysis.json'), 'w') as f:\n    json.dump(residuals_analysis, f, indent=4)\n\n\nPloting the MSEs for each time series:\n\n\nCode\n# Report: Documenting the results\n# Plotting the MSEs for each time series\nfor col in mse_results.keys():\n    mse_series = mse_results[col]\n    plt.figure(figsize=(10, 5))\n    plt.bar(mse_series.keys(), mse_series.values(), color='blue')\n    plt.title(f'MSE Comparison - {col}')\n    plt.ylabel('MSE')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(os.path.join(report_dir, f'mse_comparison_{col}.png'))\n    plt.close()\n\n\nSaving system info:\n\n\nCode\n# End timer\nend_time = datetime.now()\nelapsed_time = end_time - start_time  # This is a timedelta object\nprint(f\"Total execution time: {elapsed_time}\")\n\n# Save execution time to the report\nsystem_info['Execution_Time_seconds'] = elapsed_time.total_seconds()  # Convert to float for JSON\nwith open(os.path.join(report_dir, 'system_info.json'), 'w') as f:\n    json.dump(system_info, f, indent=4)\n\n\nTotal execution time: 0:04:46.581668\n\n\nGenerating an automatic final report:\n\n\nCode\n# Final Report: Generating a text document with the results\nreport_path = os.path.join(report_dir, 'final_report.txt')\nwith open(report_path, 'w') as report_file:\n    report_file.write(\"Final Project Report - Forecasting Commodity Returns with LSTM\\n\")\n    report_file.write(\"=\"*80 + \"\\n\\n\")\n    \n    report_file.write(\"1. Project Objectives:\\n\")\n    report_file.write(\"Forecast future returns of a commodity portfolio using LSTM Neural Networks.\\n\\n\")\n    \n    report_file.write(\"2. Methodology:\\n\")\n    report_file.write(\"- Collecting commodity price data.\\n\")\n    report_file.write(\"- Calculating logarithmic returns.\\n\")\n    report_file.write(\"- Normalizing the data.\\n\")\n    report_file.write(\"- Training LSTM models with different configurations.\\n\")\n    report_file.write(\"- Performing grid search to optimize hyperparameters.\\n\")\n    report_file.write(\"- Conducting residual analysis to identify uncaptured patterns and issues like autocorrelation or heteroscedasticity.\\n\\n\")\n    \n    report_file.write(\"3. Results:\\n\")\n    report_file.write(results_table.to_string())\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"4. Best Parameters Found (Grid Search):\\n\")\n    report_file.write(json.dumps(best_params_dict, indent=4))\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"5. Residual Analysis:\\n\")\n    for col, res in residuals_analysis.items():\n        report_file.write(f\"Residual Analysis for {col}:\\n\")\n        report_file.write(f\"Ljung-Box Test p-value: {res['ljung_box_pvalue']}\\n\")\n        report_file.write(f\"Breusch-Pagan Test p-value: {res['breusch_pagan_pvalue']}\\n\\n\")\n    report_file.write(\"\\n\")\n    \n    report_file.write(\"6. Conclusions:\\n\")\n    report_file.write(\"The study demonstrated the importance of proper hyperparameter selection and model architecture for forecasting financial returns. Regularization techniques and the choice of activation function significantly influenced model performance. The residual analysis highlighted the need to consider autocorrelation and heteroscedasticity in modeling financial time series.\\n\\n\")\n    \n    report_file.write(\"7. Recommendations for Future Work:\\n\")\n    report_file.write(\"- Implement additional regularization techniques, such as DropConnect or Batch Normalization.\\n\")\n    report_file.write(\"- Explore more advanced architectures, like GRU or bidirectional models.\\n\")\n    report_file.write(\"- Increase the dataset to improve the models' generalization capacity.\\n\")\n    report_file.write(\"- Use more robust cross-validation methods to assess model stability.\\n\")\n    report_file.write(\"- Integrate other features, such as technical indicators or macroeconomic variables, to enrich model inputs.\\n\")\n    report_file.write(\"- Consider hybrid models that combine Machine Learning techniques with traditional statistical models.\\n\")\n    \n    report_file.write(\"\\nSystem Information and Execution Time:\\n\")\n    report_file.write(json.dumps(system_info, indent=4))\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"End of Report.\\n\")",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "news.html#analyzing-the-causal-impact-of-news-on-commodity-prices-returns-and-volatility",
    "href": "news.html#analyzing-the-causal-impact-of-news-on-commodity-prices-returns-and-volatility",
    "title": "News and Impact on price and volatilities dynamics",
    "section": "Analyzing the Causal Impact of News on Commodity Prices, Returns, and Volatility",
    "text": "Analyzing the Causal Impact of News on Commodity Prices, Returns, and Volatility\nThe hypothesis that abrupt changes in prices, returns, and volatility in agricultural commodity markets are driven by emerging market news is supported by a robust body of literature. One of the foundational works in this area is Robert Engle’s introduction of the News Impact Curve (Engle, 1993), which demonstrates how news can asymmetrically affect volatility in financial markets. Engle’s work shows that negative news tends to increase volatility more than positive news, creating a nonlinear relationship between news and market behavior. This framework suggests that sudden shifts in market conditions may be attributed to external news shocks that alter expectations and investor sentiment, thereby influencing price dynamics and return volatility.\nIn the context of agricultural commodities, this phenomenon is even more pronounced due to the sensitivity of these markets to exogenous shocks such as geopolitical developments, climatic events, and policy changes. A similar argument was made in the work of Ozon (2008), who applied volatility forecasting models to the Brazilian agricultural market. His research, which utilized data-driven models for volatility prediction, highlights the importance of incorporating external information, such as market news, into predictive models to capture the impact of unforeseen market shifts. Ozon’s findings suggest that neglecting the influence of market news can lead to significant forecasting errors, particularly in markets characterized by high volatility and abrupt price changes. His work is documented in the project repository at this link.\n\nObjective of Causal Analysis Using Market News and Search Trends\nBuilding on the insights provided by Engle and Ozon, this research aims to investigate the causal relationship between news events, search trends, and market volatility in agricultural commodity markets. Our hypothesis is that news events, particularly those that are not easily quantifiable through traditional numerical data, have a measurable and statistically significant effect on commodity prices and returns. Additionally, we will explore how search trends (e.g., Google Trends data) for specific keywords related to commodities reflect real-time market sentiment and drive price changes.\nThis study will utilize causal inference techniques to test whether certain news events are not only correlated with but are causally linked to market volatility. The innovative contribution of this research lies in combining news data with large language models (LLMs), such as GPT, to automatically identify and rank the most impactful news events. These LLMs will help in filtering vast amounts of information, pinpointing the events most likely to influence market movements.\n\n\nMethodological Approach\n\nData Collection: We will collect time-series data on commodity prices, returns, and volatility from various financial databases. Additionally, news articles and reports related to agricultural commodities will be gathered from reputable financial news sources.\nGoogle Trends: We will use Google Trends data to track search frequencies for specific keywords related to the commodities under study. The hypothesis here is that increased search activity correlates with heightened market volatility, as more people become aware of emerging news events.\nNews Filtering Using LLMs: LLMs, such as GPT, will be employed to filter news articles and highlight those with the highest potential to influence market dynamics. This automated news filtering process allows us to focus on key events that are likely to cause market disruptions.\nCausal Impact Testing: Once we have identified key news events, we will conduct causal impact tests using methodologies such as Granger causality tests, Bayesian structural time series (BSTS) or maybe the most recent CausalImpact algorithm (Brodersen et. al., 2015), models. These tests will allow us to determine whether a specific news event had a statistically significant impact on commodity prices or volatility.\nEvaluation: Finally, we will evaluate the overall effectiveness of our news-based predictive models by comparing them against baseline models that do not account for news events or search trends.\n\n\n\nExpected Contributions\nThe innovative contribution of this research is twofold. First, it introduces a novel framework that links market sentiment, captured through news data and search trends, to real-time volatility and price forecasting. Second, by incorporating LLMs for news filtering, the research provides a more scalable and efficient method of analyzing vast amounts of textual data, which is crucial in fast-moving markets.\nThis research aims to provide both theoretical insights and practical tools for market participants, enabling them to anticipate market shifts based on news events and search trends, and adjust their portfolio strategies accordingly.\n \n \n\n\n\nReferences\n\nBrodersen, K. H., Gallusser, F., Koehler, J., Remy, N., & Scott, S. L. (2015). Inferring causal impact using Bayesian structural time-series models. Annals of Applied Statistics, 9, 247–274.\nEngle, R. F. (1993). Statistical Models for Financial Volatility. Financial Analysts Journal.\nOzon, R. H. (2008). Volatility Forecasting in Agricultural Markets. Available at this link.",
    "crumbs": [
      "About",
      "Commodities (grains) news"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "AgriPredict",
    "section": "",
    "text": "This research project is being conducted at PUCPR with the support of undergraduate students participating in the Scientific Initiation Program (PIBIC Jr.). The primary aim is to explore advanced predictive models and multi-objective optimization approaches applied to agricultural commodities, particularly focusing on grains.\nThe research combines the efforts of faculty members and students, promoting an enriching learning experience while addressing real-world agricultural challenges. By leveraging time series models for price forecasting and optimization techniques, this project aims to provide insights that support decision-making processes in the agricultural sector.\nThrough the collaboration with our undergraduate students, we focus on developing practical skills and research capabilities, empowering the next generation of researchers and professionals in agricultural analytics and data-driven decision-making.",
    "crumbs": [
      "About",
      "Intro"
    ]
  },
  {
    "objectID": "introduction.html#learn-more",
    "href": "introduction.html#learn-more",
    "title": "AgriPredict",
    "section": "Learn More",
    "text": "Learn More\nFor more information about the Scientific Initiation Program, visit the PUCPR Scientific Initiation Program.",
    "crumbs": [
      "About",
      "Intro"
    ]
  },
  {
    "objectID": "acknowlegment.html",
    "href": "acknowlegment.html",
    "title": "Acknowlegment",
    "section": "",
    "text": "To the professor Gilberto Reynoso-Meza for all the help needed to evaluate at this research subject.",
    "crumbs": [
      "About",
      "Acknowlegment"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About the PIBIC and PIBITI Jr.",
    "section": "",
    "text": "About the PIBIC and PIBITI Jr.\nThis project is carried out as part of a research initiation (PIBIC and PIBITI Jr.) at PUCPR, focusing on advanced forecasting and optimization techniques applied to agriculture.\n\nRodrigoRicardoGabrielly\n\n\n  \n\nAdvisor/Researcher\n\nEconomist and Msc. in Economic Development (UFPR, 2008 & 2010), PhD Candidate at PPGEPS/PUCPR (2022-2026)",
    "crumbs": [
      "About",
      "Intro",
      "About the PIBIC and PIBITI Jr."
    ]
  },
  {
    "objectID": "mindmap.html",
    "href": "mindmap.html",
    "title": "The Research Mindmap",
    "section": "",
    "text": "Overview\nThe following flowchart illustrates the research workflow, detailing the key methodologies and how they are interconnected. It starts from the problem identification and moves through volatility modeling, predictive modeling, and optimization processes.\n\n\n\n\n\n\n\n\n\n\n\nResearch Proposal: Advanced Techniques for Multiperiod Multiobjective Portfolio Optimization in Commodity Markets\nThis research addresses the significant challenge of modeling and forecasting agricultural commodity prices, which are subject to high volatility and complex dynamics. Agricultural markets are highly sensitive to external factors such as climatic changes, geopolitical events, and supply-demand imbalances, making accurate forecasting and risk management difficult for investors and policymakers.\n\nKey Components of the Research:\n\nVolatility Modeling Using GAMLSS and MSGARCH:\n\nGAMLSS: This technique provides a nuanced understanding of the distributional characteristics of commodity returns, capturing the probabilistic behaviors that traditional models often overlook.\nMSGARCH: By implementing the Markov-Switching GARCH model, the research captures regime shifts in volatility, which are common in commodities due to external shocks and systemic changes.\n\nMulti-Objective Portfolio Optimization:\n\nThis step involves developing a multi-objective optimization framework using evolutionary algorithms such as NSGA-II and Differential Evolution (DEOptim). These algorithms help optimize portfolio allocation by balancing risk, return, and diversification, particularly for portfolios with high-volatility assets like agricultural commodities.\n\nReinforcement Learning for Portfolio Management:\n\nThe research also introduces Reinforcement Learning (RL) methods, such as Q-Learning and K-Bandit algorithms, to adaptively manage portfolio strategies. These techniques are particularly suited for dynamic portfolio management, allowing strategies to evolve as market conditions change.\n\n\n\n\nContribution to Knowledge:\nThe research’s innovative contribution lies in combining these advanced econometric and machine learning techniques to tackle the unique challenges of commodity markets. It offers a comprehensive methodological framework that improves the modeling and forecasting of volatility and returns in agricultural commodities. This work enhances portfolio optimization strategies, offering practical applications for financial markets by providing tools that help portfolio managers make informed, data-driven decisions in the face of volatile market conditions.\nFinal Objective: The primary goal of this research is to develop robust methods for volatility modeling and portfolio optimization that dynamically adapt to market conditions. This approach offers a significant advancement in both academic and professional fields by providing actionable insights for managing portfolios in volatile commodity markets.",
    "crumbs": [
      "About",
      "Intro",
      "The Research Mindmap"
    ]
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Multiobjective Optimization",
    "section": "",
    "text": "… to be included…",
    "crumbs": [
      "About",
      "MultiObjective Multiperiod Optimization"
    ]
  },
  {
    "objectID": "presFAE.html#introdução",
    "href": "presFAE.html#introdução",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Introdução",
    "text": "Introdução\n \n\n\nMercado financeiro:\n\nÉ um ambiente dinâmico onde decisões baseadas em dados podem otimizar resultados.\n\nData driven decision making:\n\nComo tomar uma decisão econômica de maneira rápida, ótima e eficiente ?\n\n\n\n\n\n\n\n\nPor que Big Data é relevante ?\n\n\n\nVolume de Dados: A análise do mercado de commodities envolve um enorme volume de dados financeiros e econômicos que cresce rapidamente, especialmente considerando históricos de preços diários, transações e eventos externos que influenciam os mercados.\nVariabilidade e Velocidade: As séries de preços de commodities possuem mudanças rápidas e variações, exigindo a capacidade de processar e analisar dados em tempo hábil para garantir insights relevantes.\nAnálises em Profundidade: Necessidade de descobrir padrões ocultos que requerem recursos computacionais consideráveis para manipular e analisar um volume grande e complexo de dados.",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#problemática-de-pesquisa",
    "href": "presFAE.html#problemática-de-pesquisa",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Problemática de Pesquisa",
    "text": "Problemática de Pesquisa\n\n\n\n\n\n\n\nSource: Data extraction from Yahoo!Finances API ploted by using Plotly",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#problemática-de-pesquisa-1",
    "href": "presFAE.html#problemática-de-pesquisa-1",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Problemática de Pesquisa",
    "text": "Problemática de Pesquisa\n \n\n\n\n\nPerguntas\n\n\n\nQuais as causas ou desencadeadores desses movimentos repentinos de inversão de tendência nas séries de preços ?\nÉ possível estimar/medir o quanto essas mudanças bruscas de tendência geram de impacto na economia e no mercado de commodities agrícolas ?\nComo podemos antecipar/prever o acontecimento dessas “quebras” nas séries temporais de preços no futuro ?\nPode-se otimizar o processo decisório de compra e venda de grãos recomendando as melhores alocações de portfólio de commodities em condições de risco e incerteza ?\nComo decidir ou facilitar o processo decisório humano dentro de um contexto de massivo volume e velocidade de informações ?\nQual o volume de dados ou tamanho de série histórica necessária pra construir um modelo eficiente ?",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#teorias-de-base",
    "href": "presFAE.html#teorias-de-base",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Teorias de Base",
    "text": "Teorias de Base\n\n\n\n\n\n\n\n\n\n\n\n\nÁrea de Ciência\nTeoria\nPensadores\n\n\n\n\nMicroeconomia\nTeoria da Demanda e do Consumidor\nWalrás, Pareto, Arrow, Debreu, Samuelson, Hicks\n\n\nMicroeconomia\nEstruturas de Mercado\nPorter, Chamberlin, Joan Robinson, Bain\n\n\nMicroeconomia\nFinanças Comportamentais\nDaniel Kahneman, Amos Tversky, Robert Shiller\n\n\nMicroeconomia\nEficiência de Mercado\nEugene Fama, Fischer Black e Myron Scholes, Jensen\n\n\nMicroeconomia\nTeoria do Portfólio\nHarry Markowitz, Milton Friedman, Keynes\n\n\nFinanças\nTeoria dos Ciclos Financeiros\nHyman Minsky, Irving Fischer, Joseph Schumpeter e Kondratiev\n\n\nFinanças\nTeoria do Mais Tolo (ou Teoria do Toque de Midas Reverso)\nJohn Kenneth Galbraith, Nassim Taleb\n\n\nEconometria Financeira\nBayesian GARCH with Markov Regime Switching\nDavid Ardia, Robert Engle, Tim Bollerslev, Gary Koop\n\n\nMacroeconomia\nTeoria da Formação das Expectativas\nRobert Lucas, Milton Friedman, Edmund Phelps, Franco Modigliani\n\n\nNeuroeconomia\nTeoria da Hipótese da Antecipação de Recompensa\nWolfram Schultz, Antonio Rangel, Paul Glimcher\n\n\nMicroeconomia\nTeoria da Seleção Adversa\nGeorge Akerlof, Michael Spence, Stiglitz\n\n\nComplexidade (Física de redes)\nSistemas Dinâmicos Adpatativos não-lineares\nArthur Ávila, Brian Arthur, Robert May",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#hipóteses-científicas",
    "href": "presFAE.html#hipóteses-científicas",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Hipóteses Científicas",
    "text": "Hipóteses Científicas\n\n\n\n\n\n\n\nInsights\n\n\n\nVolatility Clustering e mudanças estruturais\n\n \n\nAnálise de Intervenção Causal em Séries Temporais nas quebras e “efeito disseminação”\n\n \n\nDesenvolvimentos do modelo de otimização de portfolio de Markowitz (CAPM, B&S, Merton, Black-Litterman …)\n\n \n\nMúltiplos Objetivos variando conforme o contexto de mercado e as expectativas percebidas (risco e incerteza)",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#justificativa-e-relevância",
    "href": "presFAE.html#justificativa-e-relevância",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Justificativa e relevância",
    "text": "Justificativa e relevância\n \n\n\n\n\n\n\n\nContribuições teóricas\n\n\n\nIdentificação dos drivers dos preços auxilia na investigação da causa dos movimentos repentinos nas séries de preços (Teoria da Demanda do Consumidor, Estruturas de Mercado e Teoria do Portfólio [motivo transação, especulação ou precaução]) pode ser utilizada em conjunto com a técnica Econométrica de Análise de Intervenção em Séries Temporais [Angrist e Imbens, Brodersen et. alli (2015)] para avaliar seu impacto causal na série de preço estudada;\nA Teoria dos Ciclos Financeiros ajuda a compreender em qual contexto econômico a disseminação de efeito econômico-financeiro nocivo ou positivo está inserida frente a quebra repentina da tendência da trajetória de preços de alimentos (commodities)\nO uso das técnicas pertinentes dentro da teoria da Econometria Financeira com o uso do modelo Bayesiano GARCH com mudanças de regime markovianos se mostra aderente à realidade dos dados e condizente com os últimos desenvolvimentos teóricos a respeito do fenômeno da dinâmica complexa dos preços dessas commodities;\nA teoria de alocação de portfólio desde Markowitz pode ser melhor elaborada combinando as ferramentas de otimização multiobjetivo multiperíodo de maneira dinâmica em consonância com modelos econométricos que consigam incorporar com maior clareza a “incerteza” percebida pelos players de mercado na sua aferição de risco x retorno. Assim, os processos decisórios de compra e venda em momentos oportunos se tornariam mais claros.",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#big-data-na-análise-de-commodities",
    "href": "presFAE.html#big-data-na-análise-de-commodities",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Big Data na Análise de Commodities",
    "text": "Big Data na Análise de Commodities\n \n\n\n\n\nAmostragem de Dados Significativa\n\n\n\nEstratégia de Amostragem: Utilizar amostragem estratificada para garantir que a variabilidade ao longo do tempo seja capturada de forma adequada (como choques econômicos ou eventos climáticos que impactam os preços).\nRedução de Dimensionalidade: Utilizar técnicas de PCA (Principal Component Analysis) para reduzir o número de variáveis sem perder informações importantes, permitindo uma análise mais eficiente dos dados.\nUso de Dados Representativos: A seleção de um subconjunto representativo de dados pode ser feita para capturar as tendências de mercado de diferentes períodos, garantindo que os insights gerados sejam válidos e aplicáveis.",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#big-data-na-análise-de-commodities-1",
    "href": "presFAE.html#big-data-na-análise-de-commodities-1",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Big Data na Análise de Commodities",
    "text": "Big Data na Análise de Commodities\n \n\n\n\n\nFerramentas e Bibliotecas Utilizadas (alguns exemplos)\n\n\nPython:\n\nPySpark: Uma ferramenta poderosa para processamento distribuído e análise de grandes volumes de dados, ideal para trabalhar com dados de commodities de históricos extensivos.\nDask: Alternativa à biblioteca Pandas, que facilita o processamento de grandes datasets que não cabem na memória. Dask permite a execução de operações paralelas, otimizando análises.\n\nR:\n\nsparklyr e SparkR: Integração do Spark no ambiente R, possibilitando o processamento distribuído e a manipulação eficiente de datasets gigantescos, com foco em análises financeiras.\nvroom e data.table: Utilizadas para leitura rápida e manipulação de grandes volumes de dados armazenados em arquivos CSV, permitindo o carregamento de arquivos grandes em poucos segundos.\n\n*Existem algumas outras pra R e Python que poderiam ser mencionadas aqui, mas por questões de parcimônia limitaremos a pequenos exemplos.",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#big-data-com-r",
    "href": "presFAE.html#big-data-com-r",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Big Data com R",
    "text": "Big Data com R\n \n\nAnálise de Commodities com sparklyr e vroom\n\nlibrary(sparklyr)\nlibrary(dplyr)\nlibrary(vroom)\n\n# Conectar ao Spark\nsc &lt;- spark_connect(master = \"local[*]\") # Você pode rodar também no Databricks, Azure, AWS, Google Colab...\n\n# Ler grandes volumes de dados usando vroom\nfile_list &lt;- list.files(\"data\", pattern = \"*_data.csv\", full.names = TRUE)\ncombined_data &lt;- vroom(file_list, col_types = list(\n  Date = col_date(),\n  Open = col_double(),\n  High = col_double(),\n  Low = col_double(),\n  Close = col_double(),\n  Volume = col_double(),\n  Adj.Close = col_double(),\n  Ticker = col_character()\n))\n\n# Copiar os dados para o Spark\nspark_data &lt;- copy_to(sc, combined_data, \"commodities_data\", overwrite = TRUE)\n\n# Executar análise no Spark: calcular a média de fechamento ajustado por commodity\naverage_close &lt;- spark_data |&gt;\n  group_by(Ticker) |&gt;\n  summarise(Average_Close = mean(Adj.Close, na.rm = TRUE)) |&gt;\n  collect()\n\nprint(average_close)\n\n# Desconectar do Spark\nspark_disconnect(sc)",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#big-data-com-python",
    "href": "presFAE.html#big-data-com-python",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Big Data com Python",
    "text": "Big Data com Python\n \n\nAnálise de Commodities com PySpark e Dask\n\nfrom pyspark.sql import SparkSession\nimport dask.dataframe as dd\n\n# Inicializar a sessão Spark\nspark = SparkSession.builder.master(\"local\").appName(\"Commodities Analysis\").getOrCreate()\n\n# Carregar dados grandes de commodities usando Dask\nfile_list = [\"data/ZC=F_data.csv\", \"data/ZO=F_data.csv\", \"data/KE=F_data.csv\"]\ndf_dask = dd.read_csv(file_list)\n\n# Converter o DataFrame Dask para Spark\ndf_spark = spark.createDataFrame(df_dask.compute())\n\n# Analisar os dados no Spark\ndf_spark.createOrReplaceTempView(\"commodities_data\")\nresult = spark.sql(\"SELECT Ticker, AVG(Adj_Close) as Average_Close FROM commodities_data GROUP BY Ticker\")\nresult.show()\n\n# Finalizar a sessão Spark\nspark.stop()",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "presFAE.html#obrigado",
    "href": "presFAE.html#obrigado",
    "title": "Análise de Dados do Mercado Financeiro:",
    "section": "Obrigado!",
    "text": "Obrigado!\n \n\n\n\n\n\n\n\nRodrigo Hermont Ozon\n\n\n\\(\\Rightarrow\\) Agradecimentos à todos os membros da banca examinadora e demais ouvintes:\n\nMeu perfil no Google Scholar\nMeu CV Lattes\nMeu site com posts, tutoriais e artigos\nMeu perfil no LinkeDin\n\n\n\n\n \n \n\n\n\n\"Situations emerge in the process of creative destruction in which many firms may have to perish that nevertheless would be able to live on vigorously and usefully if they could weather a particular storm.\n\n[... Capitalism requires] the perennial gale of Creative Destruction.\" Schumpeter, Joseph A. (1994) [1942]. Capitalism, Socialism and Democracy. London: Routledge. pp. 82–83. ISBN 978-0-415-10762-4. Retrieved 23 November 2011.",
    "crumbs": [
      "About",
      "Big Data in financial markets",
      "Análise de Dados do Mercado Financeiro:"
    ]
  },
  {
    "objectID": "revsyslit.html",
    "href": "revsyslit.html",
    "title": "Literature Review and Scientific Challenge",
    "section": "",
    "text": "Code\nstart_time &lt;- Sys.time()",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-sentiment-analysis-and-commodity-prices",
    "href": "revsyslit.html#keyword-sentiment-analysis-and-commodity-prices",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Sentiment Analysis AND Commodity Prices",
    "text": "Keyword: “Sentiment Analysis AND Commodity Prices\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Sentiment Analysis AND Commodity Prices\"\nresults &lt;- cr_works(query = \"Sentiment Analysis AND Commodity Prices\", limit = 1000)$data # Max is 1000 searches\n\n\ncat(\"Show all features avaiable in selected keywords... \\n\")\n\n\nShow all features avaiable in selected keywords... \n\n\nCode\n# Exibir os resultados da busca\nglimpse(results)\n\n\nRows: 1,000\nColumns: 40\n$ created                &lt;chr&gt; \"2009-07-03\", \"2019-12-20\", \"2010-06-25\", \"2009…\n$ deposited              &lt;chr&gt; \"2021-09-03\", \"2023-08-17\", \"2021-09-03\", \"2021…\n$ doi                    &lt;chr&gt; \"10.1787/656681831805\", \"10.1002/9781119603849.…\n$ indexed                &lt;chr&gt; \"2022-04-01\", \"2024-05-11\", \"2022-03-29\", \"2022…\n$ member                 &lt;chr&gt; \"1963\", \"311\", \"1963\", \"1963\", \"2026\", \"1963\", …\n$ prefix                 &lt;chr&gt; \"10.1787\", \"10.1002\", \"10.1787\", \"10.1787\", \"10…\n$ publisher              &lt;chr&gt; \"Organisation for Economic Co-Operation and Dev…\n$ score                  &lt;chr&gt; \"29.093645\", \"29.018356\", \"28.319153\", \"28.2387…\n$ source                 &lt;chr&gt; \"Crossref\", \"Crossref\", \"Crossref\", \"Crossref\",…\n$ reference.count        &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ references.count       &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"2\", \"0…\n$ title                  &lt;chr&gt; \"Box 1.1. Commodity prices have rebounded. Chin…\n$ type                   &lt;chr&gt; \"component\", \"other\", \"component\", \"component\",…\n$ url                    &lt;chr&gt; \"http://dx.doi.org/10.1787/656681831805\", \"http…\n$ alternative.id         &lt;chr&gt; NA, \"10.1002/9781119603849.ch1,10.1002/97811196…\n$ archive                &lt;chr&gt; NA, \"Portico\", NA, NA, NA, NA, \"Portico\", NA, N…\n$ container.title        &lt;chr&gt; NA, \"Advanced Positioning, Flow, and Sentiment …\n$ published.print        &lt;chr&gt; NA, \"2019-11-18\", NA, NA, \"2013-08-26\", NA, \"20…\n$ published.online       &lt;chr&gt; NA, \"2019-12-19\", NA, NA, NA, NA, \"2019-12-19\",…\n$ isbn                   &lt;chr&gt; NA, \"9781119603825,9781119603849\", NA, NA, NA, …\n$ issued                 &lt;chr&gt; NA, \"2019-11-18\", NA, NA, \"2013-08-26\", NA, \"20…\n$ page                   &lt;chr&gt; NA, \"7-18\", NA, NA, \"2038-2040\", NA, \"163-176\",…\n$ language               &lt;chr&gt; NA, \"en\", NA, NA, \"en\", NA, \"en\", NA, NA, \"en\",…\n$ link                   &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[3 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;t…\n$ license                &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, &lt;NU…\n$ issn                   &lt;chr&gt; NA, NA, NA, NA, \"1001-9081\", NA, NA, NA, NA, NA…\n$ issue                  &lt;chr&gt; NA, NA, NA, NA, \"7\", NA, NA, NA, NA, NA, NA, NA…\n$ subtitle               &lt;chr&gt; NA, NA, NA, NA, \"Text sentiment analysis-orient…\n$ volume                 &lt;chr&gt; NA, NA, NA, NA, \"32\", NA, NA, NA, NA, NA, NA, N…\n$ short.container.title  &lt;chr&gt; NA, NA, NA, NA, \"Journal of Computer Applicatio…\n$ author                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[1 x 3…\n$ reference              &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ abstract               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ update.policy          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ assertion              &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ funder                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ archive_               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ na.                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ na..1                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nFirst_Keyword &lt;- results |&gt; \n  arrange( desc(score), desc(is.referenced.by.count) ) |&gt; \n  distinct() |&gt; \n  filter(title != \"Front Matter\", \n         title != \"Cover\",\n         title != \"Books Received\")\n\nFirst_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-volatility-models-and-agricultural-markets",
    "href": "revsyslit.html#keyword-volatility-models-and-agricultural-markets",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Volatility Models AND Agricultural Markets”",
    "text": "Keyword: “Volatility Models AND Agricultural Markets”\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Volatility Models AND Agricultural Markets\"\nresults &lt;- cr_works(query = \"Volatility Models AND Agricultural Markets\", limit = 1000)$data # Max is 1000 searches\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nSecond_Keyword &lt;- results |&gt; arrange( desc(score), desc(is.referenced.by.count) ) |&gt; distinct()\n\nSecond_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-structural-breaks-and-agricultural-prices",
    "href": "revsyslit.html#keyword-structural-breaks-and-agricultural-prices",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Structural Breaks AND Agricultural Prices”",
    "text": "Keyword: “Structural Breaks AND Agricultural Prices”\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Structural Breaks AND Agricultural Prices\"\nresults &lt;- cr_works(query = \"Structural Breaks AND Agricultural Prices\", limit = 1000)$data # Max is 1000 searches\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nThird_Keyword &lt;- results |&gt; \n  arrange( desc(score), desc(is.referenced.by.count) ) |&gt; distinct()\n\nThird_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#final-dataframe",
    "href": "revsyslit.html#final-dataframe",
    "title": "Literature Review and Scientific Challenge",
    "section": "Final dataframe",
    "text": "Final dataframe\n\n\nCode\nFinal_df &lt;- bind_rows(\n  First_Keyword,\n  Second_Keyword,\n  Third_Keyword\n) |&gt; \n  arrange( desc(title), desc(score) ) |&gt; \n  distinct()\n\n\nglimpse(Final_df)\n\n\nRows: 393\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 2]&gt;], [&lt;tbl_df[3 x 3]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"dataset\", \"dataset\", \"journal-article…\n$ url                    &lt;chr&gt; \"http://dx.doi.org/10.1108/oxan-ga261626\", \"htt…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"CRAN: Contributed …\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Commodity Markets\", NA,…\n$ publisher              &lt;chr&gt; \"Emerald\", \"The R Foundation\", \"The R Foundatio…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.32614/cran.package…\n$ published.print        &lt;mth&gt; 2021 mai, 2020 set, 2023 jan, 2023 dez, 2023 ju…\n$ score                  &lt;dbl&gt; 19.98478, 18.73033, 17.08698, 19.98206, 16.1120…\n$ reference.count        &lt;chr&gt; \"0\", \"1\", \"1\", \"47\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[2 …\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[1 x 3]&gt;], …\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", NA, NA, \"2405-8513\", \"1999-1142\", …\n\n\n\n\nCode\nFinal_df_unique &lt;- Final_df |&gt;\n  distinct(title, doi, .keep_all = TRUE)\n\nglimpse(Final_df_unique)\n\n\nRows: 389\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 2]&gt;], [&lt;tbl_df[3 x 3]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"dataset\", \"dataset\", \"journal-article…\n$ url                    &lt;chr&gt; \"http://dx.doi.org/10.1108/oxan-ga261626\", \"htt…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"CRAN: Contributed …\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Commodity Markets\", NA,…\n$ publisher              &lt;chr&gt; \"Emerald\", \"The R Foundation\", \"The R Foundatio…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.32614/cran.package…\n$ published.print        &lt;mth&gt; 2021 mai, 2020 set, 2023 jan, 2023 dez, 2023 ju…\n$ score                  &lt;dbl&gt; 19.98478, 18.73033, 17.08698, 19.98206, 16.1120…\n$ reference.count        &lt;chr&gt; \"0\", \"1\", \"1\", \"47\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[2 …\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[1 x 3]&gt;], …\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", NA, NA, \"2405-8513\", \"1999-1142\", …\n\n\nCode\nFinal_df_unique\n\n\n\n  \n\n\n\nBy expanding the author names columns:\n\n\nCode\n# Unnest the author column\nFinal_df_unique_expanded &lt;- Final_df_unique |&gt;\n  unnest(author) |&gt;\n  select(title, given, family)\n\n# Rename the author name columns\nFinal_df_unique_expanded &lt;- Final_df_unique_expanded |&gt;\n  rename(Author_First_Name = given, Author_Last_Name = family)\n\n# Print the expanded data frame\nprint(Final_df_unique_expanded)\n\n\n# A tibble: 629 × 3\n   title                                      Author_First_Name Author_Last_Name\n   &lt;chr&gt;                                      &lt;chr&gt;             &lt;chr&gt;           \n 1 stochvolTMB: Likelihood Estimation of Sto… &lt;NA&gt;              &lt;NA&gt;            \n 2 mbreaks: Estimation and Inference for Str… Linh              Nguyen          \n 3 mbreaks: Estimation and Inference for Str… Yohei             Yamamoto        \n 4 mbreaks: Estimation and Inference for Str… Pierre            Perron          \n 5 World regional natural gas prices: Conver… Jose Roberto      Loureiro        \n 6 World regional natural gas prices: Conver… Julian            Inchauspe       \n 7 World regional natural gas prices: Conver… Roberto F.        Aguilera        \n 8 World commodity prices and partial defaul… Manoj             Atolia          \n 9 World commodity prices and partial defaul… Shuang            Feng            \n10 Who should buy stocks when volatility spi… Andrés            Schneider       \n# ℹ 619 more rows\n\n\nNow inserting the authors name in a new df\n\n\nCode\nauthors_df &lt;- Final_df_unique_expanded |&gt;\n  group_by(title) |&gt;\n  summarise(Author_Names = paste(Author_First_Name, Author_Last_Name, collapse = \"; \")) |&gt;\n  ungroup()\n\nprint(authors_df)\n\n\n# A tibble: 274 × 2\n   title                                                            Author_Names\n   &lt;chr&gt;                                                            &lt;chr&gt;       \n 1 A Behavioral Approach to Pricing in Commodity Markets: Dual Pro… Florian Dos…\n 2 A CEEMD-ARIMA-SVM model with structural breaks to forecast the … Yuxiang Che…\n 3 A Commodity Review Sentiment Analysis Based on BERT-CNN Model    Junchao Don…\n 4 A Study of Asymmetric Volatilities in Korean Stock Markets Usin… Eunhee Lee  \n 5 A Theoretical Framework for Reconceiving Agricultural Markets    Anthony Pah…\n 6 A double mixture autoregressive model of commodity prices        Gilbert Mba…\n 7 A note on institutional hierarchy and volatility in financial m… S. Alfarano…\n 8 A quantile autoregression analysis of price volatility in agric… Jean‐Paul C…\n 9 ARE AGRICULTURAL COMMODITY PRICES AFFECTED BY COVID-19? A STRUC… Katarzyna C…\n10 ASV: Stochastic Volatility Models with or without Leverage       Yasuhiro Om…\n# ℹ 264 more rows\n\n\nAuthor(s)\n\n\nCode\ndf_author &lt;- Final_df_unique |&gt;\n  select(title,\n         author) |&gt;\n  unnest(cols = author)\n\nglimpse(df_author)\n\n\nRows: 629\nColumns: 11\n$ title               &lt;chr&gt; \"stochvolTMB: Likelihood Estimation of Stochastic …\n$ name                &lt;chr&gt; \"Jens Christian Wahl &lt;jens.c.wahl@gmail.com&gt;\", NA,…\n$ sequence            &lt;chr&gt; \"first\", \"first\", \"additional\", \"additional\", \"fir…\n$ given               &lt;chr&gt; NA, \"Linh\", \"Yohei\", \"Pierre\", \"Jose Roberto\", \"Ju…\n$ family              &lt;chr&gt; NA, \"Nguyen\", \"Yamamoto\", \"Perron\", \"Loureiro\", \"I…\n$ ORCID               &lt;chr&gt; NA, NA, NA, NA, \"http://orcid.org/0000-0003-4643-1…\n$ authenticated.orcid &lt;lgl&gt; NA, NA, NA, NA, FALSE, NA, NA, FALSE, NA, NA, NA, …\n$ affiliation.name    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bank for …\n$ affiliation1.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ affiliation2.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ affiliation3.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nCode\nprint(df_author)\n\n\n# A tibble: 629 × 11\n   title  name  sequence given family ORCID authenticated.orcid affiliation.name\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;lgl&gt;               &lt;chr&gt;           \n 1 stoch… Jens… first    &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;  NA                  &lt;NA&gt;            \n 2 mbrea… &lt;NA&gt;  first    Linh  Nguyen &lt;NA&gt;  NA                  &lt;NA&gt;            \n 3 mbrea… &lt;NA&gt;  additio… Yohei Yamam… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 4 mbrea… &lt;NA&gt;  additio… Pier… Perron &lt;NA&gt;  NA                  &lt;NA&gt;            \n 5 World… &lt;NA&gt;  first    Jose… Loure… http… FALSE               &lt;NA&gt;            \n 6 World… &lt;NA&gt;  additio… Juli… Incha… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 7 World… &lt;NA&gt;  additio… Robe… Aguil… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 8 World… &lt;NA&gt;  first    Manoj Atolia http… FALSE               &lt;NA&gt;            \n 9 World… &lt;NA&gt;  additio… Shua… Feng   &lt;NA&gt;  NA                  &lt;NA&gt;            \n10 Who s… &lt;NA&gt;  first    Andr… Schne… &lt;NA&gt;  NA                  &lt;NA&gt;            \n# ℹ 619 more rows\n# ℹ 3 more variables: affiliation1.name &lt;chr&gt;, affiliation2.name &lt;chr&gt;,\n#   affiliation3.name &lt;chr&gt;\n\n\nLinks\n\n\nCode\ndf_link &lt;- Final_df |&gt;\n  select(\n    title,\n    link\n  ) |&gt;\n  unnest(\n    cols = link\n  )\n\nglimpse(df_link)\n\n\nReferences\n\n\nCode\ndf_references &lt;- Final_df |&gt;\n  select(\n    title,\n    reference\n  ) |&gt;\n  unnest(\n    cols = reference\n  )\n\nglimpse(df_references)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#n-grams",
    "href": "revsyslit.html#n-grams",
    "title": "Literature Review and Scientific Challenge",
    "section": "N-grams",
    "text": "N-grams\nInstead of analyzing individual words, N-gram analysis examines sequences of two or more words that frequently appear together (bi-grams, tri-grams).\nObjective: To identify more relevant phrases or compound terms in the literature, which may not be captured in the analysis of individual words.\nApplication: To discover compound terms like “volatility spillover,” “agricultural prices,” or “portfolio optimization” that frequently appear.\n\n\nCode\n# Filtrar apenas artigos com abstracts disponíveis\nFinal_df_filtered &lt;- Final_df_unique |&gt;\n  filter(!is.na(abstract))  # Remove os artigos com abstracts 'NA'\n\n# Verificar o resultado\nglimpse(Final_df_filtered)\n\n\nRows: 81\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 4]&gt;], [&lt;tbl_df[2 x 4]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"journal-article\", \"journal-article\", …\n$ url                    &lt;chr&gt; \"http://dx.doi.org/10.1108/oxan-ga261626\", \"htt…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"Review of Finance\"…\n$ short.container.title  &lt;chr&gt; NA, NA, \"Agricultural Economics\", \"Journal of F…\n$ publisher              &lt;chr&gt; \"Emerald\", \"Oxford University Press (OUP)\", \"Wi…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.1093/rof/rfad038\",…\n$ published.print        &lt;mth&gt; 2021 mai, 2024 mai, 2021 mar, 2021 fev, 2020 ab…\n$ score                  &lt;dbl&gt; 19.98478, 18.47519, 16.14462, 17.98851, 19.2822…\n$ reference.count        &lt;chr&gt; \"0\", \"62\", \"41\", \"49\", \"66\", \"64\", \"36\", \"45\", …\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"6\", \"9\", \"20\", \"5\", \"3\", \"16\", \"0\", …\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], [&lt;tbl_df[3 x 4]&gt;], [&lt;tbl_df…\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[62 x 9]&gt;], [&lt;tbl_df[41 x 12]&gt;…\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", \"1572-3097,1573-692X\", \"0169-5150,…\n\n\nThen we build corpus with only avaiable abstracts\n\n\nCode\n# Criar o corpus de texto a partir da coluna de resumos (abstracts) filtrados\ncorpus &lt;- Corpus(VectorSource(Final_df_filtered$abstract))\n\n# Visualizar parte do corpus\ninspect(corpus[1:3])\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] &lt;jats:p&gt;Demand will surge for the minerals for low-carbon technologies, but supply is concentrated and will lag, raising prices&lt;/jats:p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n[2] &lt;jats:title&gt;Abstract&lt;/jats:title&gt;\\n               &lt;jats:p&gt;This article studies exchange-traded funds’ (ETFs) price impact in the most ETF-dominated asset classes: volatility (VIX) and commodities. I propose a new way to measure ETF-related price distortions based on the specifics of futures contracts. This allows me to isolate a component in VIX futures prices that is strongly related to the rebalancing of ETFs. I derive a novel decomposition of ETF trading demand into leverage rebalancing, calendar rebalancing, and flow rebalancing, and show that trading against ETFs is risky. Leverage rebalancing has the largest effects on the ETF-related price component. This rebalancing amplifies price changes and exposes ETF counterparties to variance.&lt;/jats:p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n[3] &lt;jats:title&gt;Abstract&lt;/jats:title&gt;&lt;jats:p&gt;Teff is an ancient grain primarily produced in Ethiopia and providing more than 10% of the total calories consumed in the country. The grain is considered as “super grain” due to its nutritional qualities, and it has seen an increase in its demand and price in recent years. These trends raise public concerns about the affordability of the grain and the prevalence of food insecurity in Ethiopia. Therefore, we investigate the impacts of increasing teff prices on consumers’ welfare by regions. Using data from two waves of the Ethiopia Socioeconomic Survey 2013–2014 and 2015–2016, we examine the consumption patterns of cereals in Ethiopia and estimate a two‐stage structural demand system. We find that teff is the most own‐price inelastic grain in Ethiopia and an increase of 10% in teff prices will reduce consumer welfare by 0.81, 1.29, and 1.73 Birrs per week for the average rural, town, and urban consumers, respectively. These estimates correspond to 1.64, 2.31, and 2.46% of their weekly food budgets. We also find the negative effects of an increase in teff prices are smaller for the lower‐income groups as they have relatively lower expenditures on teff. Additionally, we analyze the effects of simultaneous changes in wheat and teff prices to measure the extent to which Ethiopia's food policy of distributing subsidized wheat could offset the consumer welfare loss due to an increase in teff prices.&lt;/jats:p&gt;\n\n\nclean corpus and apply the N-grams processing\n\n\nCode\n# Função para limpar o texto e remover termos indesejados\nclean_text &lt;- function(corpus){\n  # Remover tags HTML\n  corpus &lt;- tm_map(corpus, content_transformer(function(x) gsub(\"&lt;.*?&gt;\", \"\", x))) # Remove tags HTML\n  corpus &lt;- tm_map(corpus, content_transformer(tolower)) # Converter para minúsculas\n  corpus &lt;- tm_map(corpus, removePunctuation) # Remover pontuação\n  corpus &lt;- tm_map(corpus, removeNumbers) # Remover números\n  corpus &lt;- tm_map(corpus, removeWords, stopwords(\"english\")) # Remover stopwords em inglês\n  corpus &lt;- tm_map(corpus, stripWhitespace) # Remover espaços extras\n  \n  # Lista de termos indesejados a remover\n  termos_indesejados &lt;- c(\"jats\", \"title\", \"sec\", \"abstract\", \"type\", \"content\", \"of the\", \"in the\", \"and the\")\n  \n  # Remover termos indesejados\n  corpus &lt;- tm_map(corpus, removeWords, termos_indesejados)\n  \n  return(corpus)\n}\n# Limpar o corpus\ncorpus_clean &lt;- clean_text(corpus)\n\n# Criar um dataframe de texto limpo\nclean_text_df &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE)\n\n# Criar um tokenizador de bigramas com o corpus limpo\nbigram_data_clean &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE) |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# Contar a frequência dos bigramas\nbigram_filtered_clean &lt;- bigram_data_clean |&gt;\n  count(bigram, sort = TRUE)\n\n# Filtrar os bigramas que não estão vazios ou não são códigos\nbigram_filtered_clean &lt;- bigram_filtered_clean |&gt;\n  filter(!str_detect(bigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os bigramas mais frequentes\nhead(bigram_filtered_clean, 10)\n\n\n\n  \n\n\n\nNow we can plot bars with the most frequent bigrams:\n\n\nCode\n# Filtrar os 10 bigramas mais frequentes\ntop_bigrams_clean &lt;- bigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_bigrams_clean, aes(x = reorder(bigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Bigramas Mais Frequentes\", x = \"Bigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nWe can expand for three grams:\n\n\nCode\n# Tokenização em trigramas\ntrigram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(trigram, text, token = \"ngrams\", n = 3)\n\n# Contar a frequência dos trigramas\ntrigram_filtered_clean &lt;- trigram_data_clean |&gt;\n  count(trigram, sort = TRUE)\n\n# Filtrar os trigramas indesejados\ntrigram_filtered_clean &lt;- trigram_filtered_clean |&gt;\n  filter(!str_detect(trigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os trigramas mais frequentes\nhead(trigram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 trigramas mais frequentes\ntop_trigrams_clean &lt;- trigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_trigrams_clean, aes(x = reorder(trigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Trigramas Mais Frequentes\", x = \"Trigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nAnd quadrigrams:\n\n\nCode\n# Tokenização em quadrigramas\nquadrigram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(quadrigram, text, token = \"ngrams\", n = 4)\n\n# Contar a frequência dos quadrigramas\nquadrigram_filtered_clean &lt;- quadrigram_data_clean |&gt;\n  count(quadrigram, sort = TRUE)\n\n# Filtrar os quadrigram indesejados\nquadrigram_filtered_clean &lt;- quadrigram_filtered_clean |&gt;\n  filter(!str_detect(quadrigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os quadrigram mais frequentes\nhead(quadrigram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 quadrigramas mais frequentes\ntop_quadrigrams_clean &lt;- quadrigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_quadrigrams_clean, aes(x = reorder(quadrigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Quadrigramas Mais Frequentes\", x = \"Quadrigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nAnd finnaly the darkest pentagram:\n\n\nCode\n# Tokenização em pentagram\npentagram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(pentagram, text, token = \"ngrams\", n = 5)\n\n# Contar a frequência dos pentagram\npentagram_filtered_clean &lt;- pentagram_data_clean |&gt;\n  count(pentagram, sort = TRUE)\n\n# Filtrar os pentagram indesejados\npentagram_filtered_clean &lt;- pentagram_filtered_clean |&gt;\n  filter(!str_detect(pentagram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os quadrigram mais frequentes\nhead(pentagram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 pentagrams mais frequentes\ntop_pentagrams_clean &lt;- pentagram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_pentagrams_clean, aes(x = reorder(pentagram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Pentagramas Mais Frequentes\", x = \"Pentagramas\", y = \"Frequência\")",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#word-co-occurrence-analysis",
    "href": "revsyslit.html#word-co-occurrence-analysis",
    "title": "Literature Review and Scientific Challenge",
    "section": "Word Co-Occurrence Analysis",
    "text": "Word Co-Occurrence Analysis\nWhat it is: Examine which words tend to appear together in a text.\nObjective: Explore the relationships between different terms and how they are connected within a broader context.\nApplication: Identify patterns in the associations between important terms such as “volatility” and “crisis” or “agricultural prices” and “portfolio optimization.”\n\n\nCode\n# Carregar pacotes necessários\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tm)\n\n# Passo 1: Limpeza do Corpus (assumindo que o corpus já foi criado e limpo anteriormente)\n# Criar o dataframe com os textos limpos (usando o corpus_clean criado anteriormente)\ntext_df &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE)\n\n# Passo 2: Tokenização para Bigramas (palavras em pares)\nbigrams &lt;- text_df %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# Passo 3: Separar os bigramas em duas colunas para fazer a análise de co-ocorrência\nbigram_separated &lt;- bigrams %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\n# Passo 4: Contar as co-ocorrências de palavras\nbigram_count &lt;- bigram_separated %&gt;%\n  count(word1, word2, sort = TRUE)\n\n# Passo 5: Filtrar co-ocorrências que aparecem mais de uma vez (ou outro limite que faça sentido)\nbigram_filtered &lt;- bigram_count %&gt;%\n  filter(n &gt; 1)\n\n# Passo 6: Criar o grafo de co-ocorrência usando o pacote igraph\nword_network &lt;- bigram_filtered %&gt;%\n  graph_from_data_frame()\n\n# Passo 7: Plotar a rede de co-ocorrência usando o pacote ggraph\nset.seed(1234)  # Para reprodutibilidade\nplotly::ggplotly(ggraph(word_network, layout = \"fr\") +  # Layout da rede (fr = force-directed)\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +  # Conexões entre palavras\n  geom_node_point(color = \"lightblue\", size = 5) +  # Nós das palavras\n  geom_node_text(aes(label = name), vjust = 1.5, hjust = 1.5) +  # Textos das palavras\n  theme_void() +  # Remover fundo e eixos\n  labs(title = \"Word Co-occurrence Network\", subtitle = \"Bigram Co-occurrences\")\n)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#tdm-and-lda",
    "href": "revsyslit.html#tdm-and-lda",
    "title": "Literature Review and Scientific Challenge",
    "section": "TDM and LDA",
    "text": "TDM and LDA\nWhat it is: Extract underlying topics from texts by grouping related keywords.\nObjective: Automatically discover the main themes discussed in a large collection of articles. Application: Help identify different research areas within a broader field, for example, whether articles on “sentiment analysis” focus more on price volatility, market predictions, or risk analysis.\nTo continue with our Exploratory Lit Rev, we first need to build the corpus:\n\n\nCode\n# Criar um corpus de texto a partir da coluna de resumos (abstracts)\ncorpus &lt;- Corpus(VectorSource(Final_df_unique$abstract))\n\n# Visualizar parte do corpus\ninspect(corpus[1:3])\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] &lt;jats:p&gt;Demand will surge for the minerals for low-carbon technologies, but supply is concentrated and will lag, raising prices&lt;/jats:p&gt;\n[2] &lt;NA&gt;                                                                                                                                    \n[3] &lt;NA&gt;                                                                                                                                    \n\n\nThen we proceed for the next step, cleaning the text:\nNow we can build the terms matrix (Term-Document Matrix - TDM), were each row represents an term and each column represents an document.\n\n\nCode\n# Criar a matriz de termos\ntdm &lt;- TermDocumentMatrix(corpus_clean)\n\n# Converter para um dataframe\ntdm_matrix &lt;- as.matrix(tdm)\n\n# Ver os termos mais frequentes\nterm_freq &lt;- rowSums(tdm_matrix)\nterm_freq_sorted &lt;- sort(term_freq, decreasing = TRUE)\n\n# Visualizar as palavras mais frequentes\nhead(term_freq_sorted, 10)\n\n\n  volatility       prices        price        model      markets       models \n         201           96           92           83           79           68 \n       study        stock       market agricultural \n          63           62           57           51 \n\n\nConvert the matrix for an LDA format:\n\n\nCode\n# Criar a matriz de termos a partir do corpus limpo\ntdm &lt;- TermDocumentMatrix(corpus_clean)\n\n# Converter a matriz de termos para um formato compatível com o LDA\ntdm_sparse &lt;- as.matrix(tdm)\n\n# Verifique se a ordem dos termos é correta\nterms &lt;- Terms(tdm)  # Extraia os termos da TDM\n\n\nOne wordcloud is an good view to see the most frequent terms:\n\n\nCode\n# Criar uma nuvem de palavras\nwordcloud(words = names(term_freq_sorted), freq = term_freq_sorted, min.freq = 5,\n          max.words=100, random.order=FALSE, colors=brewer.pal(8, \"Dark2\"))\n\n\n\n\n\n\n\n\n\nIf we needs an more quant analisys, we can generate an bar graph with the most frequent words:\n\n\nCode\n# Converter para dataframe\ndf_term_freq &lt;- data.frame(term = names(term_freq_sorted), freq = term_freq_sorted)\n\n# Filtrar as 10 palavras mais frequentes\ntop_terms &lt;- df_term_freq |&gt; top_n(10, freq)\n\n# Criar gráfico de barras\nggplot(top_terms, aes(x = reorder(term, freq), y = freq)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Frequent Terms\", x = \"Terms\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nTo indentify the principal topics inside the papers, we can use Latent Dirichlet Allocation (LDA).\n\n\nCode\n# Definir o número de tópicos\nnum_topics &lt;- 3\n\n# Rodar o modelo LDA\nlda_model &lt;- LDA(tdm_sparse, k = num_topics, control = list(seed = 1234))\n\n# Extrair os tópicos e garantir que os termos sejam corretamente mapeados\ntopics &lt;- tidy(lda_model, matrix = \"beta\")\n\n# Conecte os IDs de termos reais\ntopics &lt;- topics |&gt;\n  mutate(term = terms[as.numeric(term)])  # Substituir os índices pelos termos reais\n\ntopics\n\n\n\n  \n\n\n\nSeeing the most representatives terms by topic:\n\n\nCode\n# Mostrar as palavras mais importantes para cada tópico\ntop_terms_per_topic &lt;- topics |&gt;\n  group_by(topic) |&gt;\n  top_n(10, beta) |&gt;\n  ungroup() |&gt;\n  arrange(topic, -beta)\n\n# Visualização corrigida com os termos reais\nlibrary(ggplot2)\nlibrary(forcats)\n\ntop_terms_per_topic |&gt;\n  mutate(term = fct_reorder(term, beta)) |&gt;\n  ggplot(aes(term, beta, fill = as.factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  labs(title = \"Top terms in each topic\", x = \"Terms\", y = \"Beta (Importance)\")\n\n\n\n\n\n\n\n\n\nThe graph generated is a topic modeling visualization based on an LDA (Latent Dirichlet Allocation) model. Each bar shows the most important words in each identified topic, derived from the text data provided. Let’s break down each part:\nInterpreting the Axes:\n\nX-axis (“Beta Importance”): The “beta” value indicates the probability that the word is relevant to the specific topic. The higher the beta value, the more important the word is in describing that topic.\nY-axis (Terms): Shows the terms (words) that are most representative within each topic.\n\nInterpretation by Topic:\nTopic 1 (red bar)\n\nThe most relevant terms include: “novel”, “exposes”, “way”, “contracts”, “volatility”, “surge”, “specifics”, “birrs”, “calendar”, “consumer.”\nThese terms suggest a set of documents focused on new contracts or methods related to volatility and consumption. The term “volatility” indicates that this topic likely discusses market fluctuations or innovations in managing contracts and consumption.\n\nTopic 2 (green bar)\n\nMost relevant terms: “contracts”, “propose”, “studies”, “etfs”, “correspond”, “lag”, “measure”, “new”, “consumption”, “based.”\nThis topic seems to be related to studies proposing the use of ETFs (Exchange-Traded Funds) and financial contracts, with a focus on consumption measures and market response lags. It could be discussing proposals for new studies on contracts and ETFs related to different consumption patterns.\n\nTopic 3 (blue bar)\n\nMost relevant terms: “consumed”, “propose”, “trading”, “impact”, “component”, “consumers”, “country”, “affordability”, “risky”, “amplifies.”\nThis topic is clearly related to consumption, trading, and impact across different regions or countries. The terms “affordability” and “risky” suggest that this topic might be discussing market accessibility and associated risks, perhaps in the context of trade policies.\n\nConclusion:\nThe graph highlights the most relevant words within each of the three topics, which seem to be:\n\nInnovations and contracts in the context of volatility and consumption.\nProposals and studies on ETFs and financial contracts, with a focus on consumption and performance measures.\nTrade and consumption, focusing on the impact on countries, market affordability, and potential associated risks.\n\nEach topic provides insight into different areas of study within the context of the analyzed documents. If you’re interested in a deeper analysis, you can explore how these topics relate to specific articles and contexts they cover.",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#exploratory-literature-review-with-text-mining-final-considerations",
    "href": "revsyslit.html#exploratory-literature-review-with-text-mining-final-considerations",
    "title": "Literature Review and Scientific Challenge",
    "section": "Exploratory Literature Review with Text Mining: Final Considerations",
    "text": "Exploratory Literature Review with Text Mining: Final Considerations\nIn this section, we applied text mining techniques to explore the main themes and trends in the literature related to sentiment analysis, volatility models, structural breaks, and multiobjective portfolio optimization in agricultural markets. The Latent Dirichlet Allocation (LDA) method was used to extract latent topics from the abstracts of papers, providing insights into the key terms that characterize the body of research.\n\nKey Insights from Topic Modeling\nThree topics were identified through LDA, each represented by a set of significant terms with their respective importance (Beta values):\n\nTopic 1 primarily emphasizes GARCH models, including terms like “bekk-garch” and “dcc-garch,” as well as terms related to market quality, volatility spillovers (“vov”), and settlement effects. This topic highlights a focus on econometric models used to assess volatility and risk in financial markets, with applications to agricultural commodities.\nTopic 2 features terms such as “return,” “option-impli,” and “long,” indicating that this topic revolves around the long-term returns and option pricing in agricultural markets. Other terms like “find” and “pcs” suggest explorations of statistical methods used to identify patterns and model market behavior.\nTopic 3 focuses on spillovers and global market trends, with terms like “spillov,” “precis,” “global,” and “detect.” This topic suggests an emphasis on crisis detection, market interdependence, and the global nature of agricultural commodities. The presence of “india” and “japan” also suggests the regional analysis of agricultural markets.\n\n\n\nImplications for Future Research\nThe results of this exploratory text mining analysis indicate that the literature in the field of agricultural market volatility, sentiment analysis, and portfolio optimization is deeply rooted in econometric modeling, risk assessment, and global market interconnections. Moving forward, researchers may want to delve further into the integration of sentiment metrics with econometric models to enhance predictive capabilities, especially in the context of volatility and structural breaks.\nBy leveraging these insights, future work could focus on developing multiobjective optimization strategies that incorporate both financial metrics and qualitative sentiment data, offering a more holistic approach to managing risk and improving investment decisions in agricultural markets.",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#frank-fabozzi",
    "href": "revsyslit.html#frank-fabozzi",
    "title": "Literature Review and Scientific Challenge",
    "section": "Frank Fabozzi",
    "text": "Frank Fabozzi\nScholar link https://scholar.google.com/citations?user=tqXS4IMAAAAJ&hl=en\n\n\nCode\n## Define the id for Frank Fabozzi\nid &lt;- 'tqXS4IMAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Frank J Fabozzi\"\n\n\nCode\nl$affiliation\n\n\n[1] \"EDHEC Business School\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 89\n\n\nCode\nl$i10_index\n\n\n[1] 466\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[6])\n\n\n[1] \"Project financing\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[6])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and David Ardia\nids &lt;- c('tqXS4IMAAAAJ', 'BPNrOUYAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'BPNrOUYAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nget_profile('BPNrOUYAAAAJ')$name\n\n\n[1] \"David Ardia\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('BPNrOUYAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#duan-li",
    "href": "revsyslit.html#duan-li",
    "title": "Literature Review and Scientific Challenge",
    "section": "Duan Li",
    "text": "Duan Li\nScholar link https://scholar.google.com/citations?user=e0IkYKcAAAAJ&hl=en\n\n\nCode\n## Define the id for Duan Li\nid &lt;- 'e0IkYKcAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Duan Li\"\n\n\nCode\nl$affiliation\n\n\n[1] \"City University of Hong Kong + The Chinese University of Hong Kong + University of Virginia\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 54\n\n\nCode\nl$i10_index\n\n\n[1] 188\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"Optimal dynamic portfolio selection: Multiperiod mean‐variance formulation\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Duan Li\nids &lt;- c('tqXS4IMAAAAJ', 'e0IkYKcAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'e0IkYKcAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"Duan Li\"\n\n\nCode\nget_profile('e0IkYKcAAAAJ')$name\n\n\n[1] \"Duan Li\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('e0IkYKcAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#woo-chang-kim",
    "href": "revsyslit.html#woo-chang-kim",
    "title": "Literature Review and Scientific Challenge",
    "section": "Woo Chang Kim",
    "text": "Woo Chang Kim\nScholar link https://scholar.google.com/citations?user=e0IkYKcAAAAJ&hl=en\n\n\nCode\n## Define the id for Woo Chang Kim\nid &lt;- '7NmBs1kAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\nl$affiliation\n\n\n[1] \"Professor, Industrial and Systems Engineering, KAIST\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 19\n\n\nCode\nl$i10_index\n\n\n[1] 35\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"Dynamic asset allocation for varied financial markets under regime switching framework\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Woo Chang Kim\nids &lt;- c('tqXS4IMAAAAJ', '7NmBs1kAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- '7NmBs1kAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\nget_profile('7NmBs1kAAAAJ')$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('7NmBs1kAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#david-ardia",
    "href": "revsyslit.html#david-ardia",
    "title": "Literature Review and Scientific Challenge",
    "section": "David Ardia",
    "text": "David Ardia\nScholar link https://scholar.google.com/citations?hl=en&user=BPNrOUYAAAAJ\n\n\nCode\n## Define the id for David Ardia\nid &lt;- 'BPNrOUYAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nl$affiliation\n\n\n[1] \"HEC Montréal & GERAD\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 25\n\n\nCode\nl$i10_index\n\n\n[1] 40\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"DEoptim: An R package for global optimization by differential evolution\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and David Ardia\nids &lt;- c('tqXS4IMAAAAJ', 'BPNrOUYAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'BPNrOUYAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nget_profile('BPNrOUYAAAAJ')$name\n\n\n[1] \"David Ardia\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('BPNrOUYAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#herman-koene-van-dijk",
    "href": "revsyslit.html#herman-koene-van-dijk",
    "title": "Literature Review and Scientific Challenge",
    "section": "Herman Koene Van Dijk",
    "text": "Herman Koene Van Dijk\nScholar link https://scholar.google.com/citations?user=8y5_FWQAAAAJ&hl=en\n\n\nCode\n## Define the id for Herman Koene Van Dijk\nid &lt;- 'FWQAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\nl$affiliation\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\nl$i10_index\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Herman Van Dijk\nids &lt;- c('tqXS4IMAAAAJ', 'FWQAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'FWQAAAAJ&hl'\nget_profile(coautorias)$name\n\nget_profile('FWQAAAAJ')$name\n\n\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('FWQAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#bernhard-pfaff",
    "href": "revsyslit.html#bernhard-pfaff",
    "title": "Literature Review and Scientific Challenge",
    "section": "Bernhard Pfaff",
    "text": "Bernhard Pfaff\n\nGitHub https://github.com/bpfaff/\nScholar https://scholar.google.com.br/scholar?q=bernhard+pfaff&hl=pt-BR&as_sdt=0&as_vis=1&oi=scholart",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  }
]