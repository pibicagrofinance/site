[
  {
    "objectID": "predictive_model.html#data-collection-and-preprocessing",
    "href": "predictive_model.html#data-collection-and-preprocessing",
    "title": "Final Project: Time Series Forecasting with LSTMs, Neural Networks Eng. Class",
    "section": "Data Collection and Preprocessing",
    "text": "Data Collection and Preprocessing\nPython libs\n\n\nCode\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\n#import yfinance as yf\nfrom yahooquery import Ticker\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import ParameterGrid\nimport psutil\nimport time\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Additional libraries for residual analysis\nfrom statsmodels.stats.diagnostic import acorr_ljungbox, het_breuschpagan\nimport statsmodels.api as sm\n\n\nFirst of all, we need to check the hardware availability:\n\n\nCode\n# Collecting hardware information\ndef get_system_info():\n    system_info = {\n        'CPU_cores': psutil.cpu_count(logical=True),\n        'CPU_freq_MHz': psutil.cpu_freq().current,\n        'Total_RAM_GB': round(psutil.virtual_memory().total / (1024 ** 3), 2),\n        'Available_RAM_GB': round(psutil.virtual_memory().available / (1024 ** 3), 2),\n        'GPU_info': 'Not available'  # Placeholder, can be expanded with libraries like GPUtil\n    }\n    return system_info\n\nsystem_info = get_system_info()\nprint(\"System Information:\", system_info)\n\n\nSystem Information: {'CPU_cores': 12, 'CPU_freq_MHz': 1800.0, 'Total_RAM_GB': 31.69, 'Available_RAM_GB': 10.41, 'GPU_info': 'Not available'}\n\n\nLoading data:\n\n\nCode\nfrom yahooquery import Ticker\nimport pandas as pd\nimport numpy as np\n\n# Definindo os tickers\ntickers = [\n    \"ZC=F\",  # Corn Futures\n    \"ZW=F\",  # Wheat Futures\n    \"KE=F\",  # KC HRW Wheat Futures\n    \"ZR=F\",  # Rough Rice Futures\n    \"GF=F\",  # Feeder Cattle Futures\n    \"ZM=F\",  # Soybean Meal Futures\n    \"ZL=F\",  # Soybean Oil Futures\n    \"ZS=F\"   # Soybean Futures\n]\n\nprint(\"\\nDownloading price data using yahooquery...\")\nticker_obj = Ticker(tickers)\ndata = ticker_obj.history(start=\"2015-01-01\")\n\n# Se o DataFrame possuir índice MultiIndex (com 'symbol' e 'date'), reinicializamos o índice e pivotamos\nif isinstance(data.index, pd.MultiIndex):\n    data = data.reset_index()\n    if 'close' in data.columns:\n        data = data.pivot(index='date', columns='symbol', values='close')\n    else:\n        print(\"Column 'close' not found in data.\")\nelse:\n    # Caso não possua MultiIndex, verifica se há a coluna 'close'\n    data = data['close'] if 'close' in data.columns else data\n\n# Converter o índice para datetime (tz-aware para UTC e, em seguida, remover a informação de fuso, ficando tz-naive)\ndata.index = pd.to_datetime(data.index, utc=True).tz_convert(None)\n\n# Se data for uma Series, converte para DataFrame\nif isinstance(data, pd.Series):\n    data = data.to_frame()\n\n# Tratamento de dados ausentes\nprint(\"\\nHandling missing data...\")\ndata.fillna(method='ffill', inplace=True)  # Preenchimento forward\ndata.dropna(axis=1, how='all', inplace=True)  # Remove colunas com todos os valores NaN\ndata.dropna(axis=0, how='any', inplace=True)  # Remove linhas com qualquer valor NaN\n\n# Verificando os dados\nprint(\"\\nData columns and their non-null counts:\")\nprint(data.count())\n\nif data.empty:\n    print(\"Data is empty after cleaning. Exiting.\")\n    exit()\n\n# Calculando os retornos logarítmicos\nreturns = np.log(data / data.shift(1)).dropna()\n\n# Verificando os retornos\nprint(\"\\nReturns DataFrame info:\")\nprint(returns.info())\nprint(returns.head())\n\nif returns.empty:\n    print(\"Returns DataFrame is empty. Exiting.\")\n    exit()\n\nreturns.head()  # Exibindo as séries temporais utilizadas (sem features adicionais)\n\n\n\nDownloading price data using yahooquery...\n\nHandling missing data...\n\nData columns and their non-null counts:\nsymbol\nGF=F    2563\nKE=F    2563\nZC=F    2563\nZL=F    2563\nZM=F    2563\nZR=F    2563\nZS=F    2563\nZW=F    2563\ndtype: int64\n\nReturns DataFrame info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2562 entries, 2015-01-05 00:00:00 to 2025-03-10 18:19:56\nData columns (total 8 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   GF=F    2562 non-null   float64\n 1   KE=F    2562 non-null   float64\n 2   ZC=F    2562 non-null   float64\n 3   ZL=F    2562 non-null   float64\n 4   ZM=F    2562 non-null   float64\n 5   ZR=F    2562 non-null   float64\n 6   ZS=F    2562 non-null   float64\n 7   ZW=F    2562 non-null   float64\ndtypes: float64(8)\nmemory usage: 180.1 KB\nNone\nsymbol          GF=F      KE=F      ZC=F      ZL=F      ZM=F      ZR=F  \\\ndate                                                                     \n2015-01-05  0.007673  0.012483  0.025570  0.023203  0.034462  0.003537   \n2015-01-06 -0.004330  0.010350 -0.002466 -0.000306  0.004866  0.002644   \n2015-01-07  0.004219 -0.017983 -0.021842  0.008832 -0.006222  0.004392   \n2015-01-08 -0.000111 -0.019956 -0.005060  0.018029 -0.019732 -0.010130   \n2015-01-09 -0.014284 -0.012001  0.015104 -0.001192  0.006896  0.002653   \n\nsymbol          ZS=F      ZW=F  \ndate                            \n2015-01-05  0.036483  0.013245  \n2015-01-06  0.010762  0.004658  \n2015-01-07  0.001664 -0.020919  \n2015-01-08 -0.007389 -0.021806  \n2015-01-09  0.006201 -0.005748  \n\n\n\n\n\n\n\n\nsymbol\nGF=F\nKE=F\nZC=F\nZL=F\nZM=F\nZR=F\nZS=F\nZW=F\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-05\n0.007673\n0.012483\n0.025570\n0.023203\n0.034462\n0.003537\n0.036483\n0.013245\n\n\n2015-01-06\n-0.004330\n0.010350\n-0.002466\n-0.000306\n0.004866\n0.002644\n0.010762\n0.004658\n\n\n2015-01-07\n0.004219\n-0.017983\n-0.021842\n0.008832\n-0.006222\n0.004392\n0.001664\n-0.020919\n\n\n2015-01-08\n-0.000111\n-0.019956\n-0.005060\n0.018029\n-0.019732\n-0.010130\n-0.007389\n-0.021806\n\n\n2015-01-09\n-0.014284\n-0.012001\n0.015104\n-0.001192\n0.006896\n0.002653\n0.006201\n-0.005748\n\n\n\n\n\n\n\nPlotting the time series of prices and returns side by side (2 per row)\n\n\nCode\n# Create a directory for plots if it doesn't exist\nplots_dir = 'plots'\nif not os.path.exists(plots_dir):\n    os.makedirs(plots_dir)\n\n# Plot prices\nprint(\"\\nPlotting time series of prices...\")\nnum_cols = 2  # Number of plots per row\nnum_plots = len(data.columns)\nnum_rows = (num_plots + num_cols - 1) // num_cols  # Ensure enough rows\n\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\naxs = axs.flatten()\n\nfor i, col in enumerate(data.columns):\n    axs[i].plot(data.index, data[col])\n    axs[i].set_title(f'Price Series - {col}')\n    axs[i].set_xlabel('Date')\n    axs[i].set_ylabel('Price')\n\n# Hide unused subplots\nfor j in range(i + 1, len(axs)):\n    fig.delaxes(axs[j])\n\nplt.tight_layout()\nplt.savefig(os.path.join(plots_dir, 'price_series.png'))\nplt.show()\nplt.close()\n\n# Plot returns\nprint(\"Plotting time series of returns...\")\nnum_plots_ret = len(returns.columns)\nnum_rows_ret = (num_plots_ret + num_cols - 1) // num_cols\n\nfig, axs = plt.subplots(num_rows_ret, num_cols, figsize=(15, 5 * num_rows_ret))\naxs = axs.flatten()\n\nfor i, col in enumerate(returns.columns):\n    axs[i].plot(returns.index, returns[col])\n    axs[i].set_title(f'Return Series - {col}')\n    axs[i].set_xlabel('Date')\n    axs[i].set_ylabel('Log Return')\n\n# Hide unused subplots\nfor j in range(i + 1, len(axs)):\n    fig.delaxes(axs[j])\n\nplt.tight_layout()\nplt.savefig(os.path.join(plots_dir, 'return_series.png'))\nplt.show()\nplt.close()\n\n\n\nPlotting time series of prices...\n\n\n\n\n\n\n\n\n\nPlotting time series of returns...\n\n\n\n\n\n\n\n\n\nPreprocessing data for LSTM time series modelling:\n\n\nCode\n# Function to prepare data for LSTM\ndef prepare_data(series, time_steps):\n    X, y = [], []\n    for i in range(len(series) - time_steps):\n        X.append(series[i:(i + time_steps)])\n        y.append(series[i + time_steps])\n    return np.array(X), np.array(y)\n\n\nSetting the parameters:\n\n\nCode\n# Defining parameters\ntime_steps = 5  # Number of time steps\nepochs = 10  # Reduced epochs for faster execution during testing\n\n# Dictionaries to store results\nmodels = {}\nhistories = {}\nmse_results = {}\nscalers = {}\npredictions = {}\nbest_params_dict = {}\nresiduals_analysis = {}\n\n# Directory to save reports and graphs\nreport_dir = 'report'\nif not os.path.exists(report_dir):\n    os.makedirs(report_dir)",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "predictive_model.html#lstm-time-series-model-fitting",
    "href": "predictive_model.html#lstm-time-series-model-fitting",
    "title": "Final Project: Time Series Forecasting with LSTMs, Neural Networks Eng. Class",
    "section": "LSTM time series model fitting",
    "text": "LSTM time series model fitting\n\n\nCode\n# Loop through each time series\nfor col in returns.columns:\n    print(f\"\\nProcessing column: {col}\")\n    series = returns[col].values.reshape(-1, 1)\n    \n    # Check if series is empty\n    if len(series) == 0:\n        print(f\"Series {col} is empty after preprocessing. Skipping.\")\n        continue\n    \n    print(f\"Series {col} has {len(series)} data points.\")\n    \n    # Normalizing data\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    series_scaled = scaler.fit_transform(series)\n    scalers[col] = scaler  # Storing the scaler for later inversion\n    \n    # Preparing data\n    X, y = prepare_data(series_scaled, time_steps)\n    \n    # Check if X and y are non-empty\n    if X.shape[0] == 0:\n        print(f\"Not enough data points in {col} after preparation. Skipping.\")\n        continue\n    \n    # Splitting into training and test sets\n    split_index = int(0.8 * len(X))\n    X_train_full, X_test = X[:split_index], X[split_index:]\n    y_train_full, y_test = y[:split_index], y[split_index:]\n    X_train_full = X_train_full.reshape((X_train_full.shape[0], X_train_full.shape[1], 1))\n    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n    \n    # Hyperparameter grid for Grid Search\n    param_grid = {\n        'neurons': [30, 50],\n        'learning_rate': [0.001, 0.01],\n        'activation': ['tanh', 'relu'],\n        'batch_size': [32, 64]\n    }\n    grid = ParameterGrid(param_grid)\n    \n    # Initializing variables to store best results\n    best_mse = float('inf')\n    best_params = None\n    best_model = None\n    \n    # Performing Grid Search\n    print(f\"Performing Grid Search for {col}...\")\n    for params in grid:\n        model = Sequential()\n        model.add(LSTM(params['neurons'], activation=params['activation'], input_shape=(time_steps, 1)))\n        model.add(Dense(1))\n        optimizer = Adam(learning_rate=params['learning_rate'])\n        model.compile(optimizer=optimizer, loss='mean_squared_error')\n        \n        history = model.fit(\n            X_train_full, y_train_full,\n            validation_data=(X_test, y_test),\n            epochs=epochs,\n            batch_size=params['batch_size'],\n            verbose=0\n        )\n        \n        y_pred = model.predict(X_test)\n        y_pred_inv = scaler.inverse_transform(y_pred)\n        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n        mse = mean_squared_error(y_test_inv, y_pred_inv)\n        \n        if mse &lt; best_mse:\n            best_mse = mse\n            best_params = params\n            best_model = model\n            best_y_pred = y_pred\n    \n    best_params_dict[col] = best_params\n    print(f\"Best parameters for {col}: {best_params} with MSE: {best_mse}\")\n    \n    models[col] = best_model\n    predictions[col] = {'Best Model': best_y_pred}\n    \n    # Inverting the normalization\n    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n    y_pred_inv = scaler.inverse_transform(best_y_pred)\n    \n    # Calculating MSE\n    mse_results[col] = {'Best Model': best_mse}\n    \n    # Visualization of results\n    plt.figure(figsize=(10, 4))\n    plt.plot(y_test_inv, label='Actual Value')\n    plt.plot(y_pred_inv, label='Prediction')\n    plt.title(f'Prediction vs Actual - {col} - Best Model')\n    plt.legend()\n    plt.savefig(os.path.join(report_dir, f'pred_vs_actual_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Residual Analysis\n    residuals = y_test_inv - y_pred_inv\n    \n    # Plotting residuals\n    plt.figure(figsize=(10, 4))\n    plt.plot(residuals, label='Residuals')\n    plt.title(f'Residuals - {col} - Best Model')\n    plt.legend()\n    plt.savefig(os.path.join(report_dir, f'residuals_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Ljung-Box test for autocorrelation in residuals\n    lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n    lb_pvalue = lb_test['lb_pvalue'].values[0]\n    \n    # Plotting residuals ACF\n    fig, ax = plt.subplots(figsize=(10, 4))\n    sm.graphics.tsa.plot_acf(residuals.squeeze(), lags=40, ax=ax)\n    plt.title(f'Residuals Autocorrelation Function - {col}')\n    plt.savefig(os.path.join(report_dir, f'acf_residuals_{col}_Best_Model.png'))\n    plt.close()\n    \n    # Heteroscedasticity test (Breusch-Pagan Test)\n    exog = sm.add_constant(best_model.predict(X_test))\n    test_bp = het_breuschpagan(residuals, exog)\n    bp_pvalue = test_bp[3]\n    \n    # Convert p-values to Python float\n    lb_pvalue = float(lb_pvalue)\n    bp_pvalue = float(bp_pvalue)\n    \n    # Saving statistical test results\n    residuals_analysis[col] = {\n        'residuals': residuals.flatten().tolist(),\n        'ljung_box_pvalue': lb_pvalue,\n        'breusch_pagan_pvalue': bp_pvalue\n    }\n    \n    print(f\"Residual Analysis for {col}:\")\n    print(f\"Ljung-Box Test p-value: {lb_pvalue}\")\n    print(f\"Breusch-Pagan Test p-value: {bp_pvalue}\")\n\n# Displaying final results in a table\nprint(\"\\nFinal Results:\")\nresults_table = pd.DataFrame(mse_results)\nprint(results_table)\n\n\n\nProcessing column: GF=F\nSeries GF=F has 2562 data points.\nPerforming Grid Search for GF=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 139ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 153ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 121ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 120ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 921us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 140ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 956us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 132ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 133ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 152ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 121ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 11s 743ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step   \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 125ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \nBest parameters for GF=F: {'activation': 'relu', 'batch_size': 64, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.001863558358285238\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for GF=F:\nLjung-Box Test p-value: 3.114439298300653e-19\nBreusch-Pagan Test p-value: 5.572021567735004e-12\n\nProcessing column: KE=F\nSeries KE=F has 2562 data points.\nPerforming Grid Search for KE=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 130ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 152ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 880us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 117ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 172ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 153ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 151ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 119ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 139ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 716us/step\nBest parameters for KE=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.00041234203904946966\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for KE=F:\nLjung-Box Test p-value: 0.006590268761731124\nBreusch-Pagan Test p-value: 0.415827361757533\n\nProcessing column: ZC=F\nSeries ZC=F has 2562 data points.\nPerforming Grid Search for ZC=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 146ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 136ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 153ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 139ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 124ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 126ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 16s 1s/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 142ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 129ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 124ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 141ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \nBest parameters for ZC=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.00031450223427219255\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nResidual Analysis for ZC=F:\nLjung-Box Test p-value: 1.2706819946105328e-08\nBreusch-Pagan Test p-value: 0.8329847721504908\n\nProcessing column: ZL=F\nSeries ZL=F has 2562 data points.\nPerforming Grid Search for ZL=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 132ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 662us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 134ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 140ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 158ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 129ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 147ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 149ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 165ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 160ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 191ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \nBest parameters for ZL=F: {'activation': 'relu', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.0010778309310390621\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step 2/16 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step\nResidual Analysis for ZL=F:\nLjung-Box Test p-value: 4.039168937925409e-11\nBreusch-Pagan Test p-value: 0.0004951263920116207\n\nProcessing column: ZM=F\nSeries ZM=F has 2562 data points.\nPerforming Grid Search for ZM=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 159ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 150ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 176ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 160ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 167ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 990us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 136ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 151ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 133ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \nBest parameters for ZM=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.0003333248517181665\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 19ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for ZM=F:\nLjung-Box Test p-value: 0.159790669133358\nBreusch-Pagan Test p-value: 0.4065472611473179\n\nProcessing column: ZR=F\nSeries ZR=F has 2562 data points.\nPerforming Grid Search for ZR=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 137ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 148ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 135ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 145ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 134ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 154ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 122ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 150ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 142ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 151ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 151ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 122ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 127ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 136ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \nBest parameters for ZR=F: {'activation': 'tanh', 'batch_size': 64, 'learning_rate': 0.001, 'neurons': 50} with MSE: 0.12913776207143213\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for ZR=F:\nLjung-Box Test p-value: 1.2371562402040067e-10\nBreusch-Pagan Test p-value: 4.876451554339863e-08\n\nProcessing column: ZS=F\nSeries ZS=F has 2562 data points.\nPerforming Grid Search for ZS=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 134ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 143ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 706us/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 142ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 148ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 139ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 193ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 131ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 112ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 128ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 5s 388ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 401ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 460ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 9s 622ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 415ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 5s 399ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 432ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step\nBest parameters for ZS=F: {'activation': 'tanh', 'batch_size': 32, 'learning_rate': 0.01, 'neurons': 50} with MSE: 0.00020035861546976582\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 46ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nResidual Analysis for ZS=F:\nLjung-Box Test p-value: 0.0030306191622350835\nBreusch-Pagan Test p-value: 0.0002490492748648036\n\nProcessing column: ZW=F\nSeries ZW=F has 2562 data points.\nPerforming Grid Search for ZW=F...\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 413ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 5s 370ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 432ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 406ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 12s 840ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 8/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/16 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 10s 702ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 9/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 11ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 459ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 7s 472ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/16 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 6s 404ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 310ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 8s 555ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/16 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 316ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 326ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 5s 361ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 311ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \n 1/16 ━━━━━━━━━━━━━━━━━━━━ 4s 279ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step  \nBest parameters for ZW=F: {'activation': 'tanh', 'batch_size': 64, 'learning_rate': 0.01, 'neurons': 30} with MSE: 0.00041271055702721423\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 0s 42ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step \nResidual Analysis for ZW=F:\nLjung-Box Test p-value: 0.002700318474036124\nBreusch-Pagan Test p-value: 0.5546230357603605\n\nFinal Results:\n                GF=F      KE=F      ZC=F      ZL=F      ZM=F      ZR=F  \\\nBest Model  0.001864  0.000412  0.000315  0.001078  0.000333  0.129138   \n\n              ZS=F      ZW=F  \nBest Model  0.0002  0.000413  \n\n\nSaving the results in a table:\n\n\nCode\n# Saving results to a CSV file\nresults_table.to_csv(os.path.join(report_dir, 'mse_results_updated.csv'), index=True)\n\n# Saving the best parameters found\nwith open(os.path.join(report_dir, 'best_params.json'), 'w') as f:\n    json.dump(best_params_dict, f, indent=4)\n\n# Saving the residual analysis\nwith open(os.path.join(report_dir, 'residuals_analysis.json'), 'w') as f:\n    json.dump(residuals_analysis, f, indent=4)\n\n\nPloting the MSEs for each time series:\n\n\nCode\n# Report: Documenting the results\n# Plotting the MSEs for each time series\nfor col in mse_results.keys():\n    mse_series = mse_results[col]\n    plt.figure(figsize=(10, 5))\n    plt.bar(mse_series.keys(), mse_series.values(), color='blue')\n    plt.title(f'MSE Comparison - {col}')\n    plt.ylabel('MSE')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(os.path.join(report_dir, f'mse_comparison_{col}.png'))\n    plt.close()\n\n\nSaving system info:\n\n\nCode\n# End timer\nend_time = datetime.now()\nelapsed_time = end_time - start_time  # This is a timedelta object\nprint(f\"Total execution time: {elapsed_time}\")\n\n# Save execution time to the report\nsystem_info['Execution_Time_seconds'] = elapsed_time.total_seconds()  # Convert to float for JSON\nwith open(os.path.join(report_dir, 'system_info.json'), 'w') as f:\n    json.dump(system_info, f, indent=4)\n\n\nTotal execution time: 0:10:16.011476\n\n\nGenerating an automatic final report:\n\n\nCode\n# Final Report: Generating a text document with the results\nreport_path = os.path.join(report_dir, 'final_report.txt')\nwith open(report_path, 'w') as report_file:\n    report_file.write(\"Final Project Report - Forecasting Commodity Returns with LSTM\\n\")\n    report_file.write(\"=\"*80 + \"\\n\\n\")\n    \n    report_file.write(\"1. Project Objectives:\\n\")\n    report_file.write(\"Forecast future returns of a commodity portfolio using LSTM Neural Networks.\\n\\n\")\n    \n    report_file.write(\"2. Methodology:\\n\")\n    report_file.write(\"- Collecting commodity price data.\\n\")\n    report_file.write(\"- Calculating logarithmic returns.\\n\")\n    report_file.write(\"- Normalizing the data.\\n\")\n    report_file.write(\"- Training LSTM models with different configurations.\\n\")\n    report_file.write(\"- Performing grid search to optimize hyperparameters.\\n\")\n    report_file.write(\"- Conducting residual analysis to identify uncaptured patterns and issues like autocorrelation or heteroscedasticity.\\n\\n\")\n    \n    report_file.write(\"3. Results:\\n\")\n    report_file.write(results_table.to_string())\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"4. Best Parameters Found (Grid Search):\\n\")\n    report_file.write(json.dumps(best_params_dict, indent=4))\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"5. Residual Analysis:\\n\")\n    for col, res in residuals_analysis.items():\n        report_file.write(f\"Residual Analysis for {col}:\\n\")\n        report_file.write(f\"Ljung-Box Test p-value: {res['ljung_box_pvalue']}\\n\")\n        report_file.write(f\"Breusch-Pagan Test p-value: {res['breusch_pagan_pvalue']}\\n\\n\")\n    report_file.write(\"\\n\")\n    \n    report_file.write(\"6. Conclusions:\\n\")\n    report_file.write(\"The study demonstrated the importance of proper hyperparameter selection and model architecture for forecasting financial returns. Regularization techniques and the choice of activation function significantly influenced model performance. The residual analysis highlighted the need to consider autocorrelation and heteroscedasticity in modeling financial time series.\\n\\n\")\n    \n    report_file.write(\"7. Recommendations for Future Work:\\n\")\n    report_file.write(\"- Implement additional regularization techniques, such as DropConnect or Batch Normalization.\\n\")\n    report_file.write(\"- Explore more advanced architectures, like GRU or bidirectional models.\\n\")\n    report_file.write(\"- Increase the dataset to improve the models' generalization capacity.\\n\")\n    report_file.write(\"- Use more robust cross-validation methods to assess model stability.\\n\")\n    report_file.write(\"- Integrate other features, such as technical indicators or macroeconomic variables, to enrich model inputs.\\n\")\n    report_file.write(\"- Consider hybrid models that combine Machine Learning techniques with traditional statistical models.\\n\")\n    \n    report_file.write(\"\\nSystem Information and Execution Time:\\n\")\n    report_file.write(json.dumps(system_info, indent=4))\n    report_file.write(\"\\n\\n\")\n    \n    report_file.write(\"End of Report.\\n\")",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html",
    "href": "garch_hawkes_simmodel.html",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "",
    "text": "Forecasting asset prices is a crucial topic in both academic research and risk management. Among the widely used models, the GARCH(1,1) model is popular for modeling the conditional volatility of returns, while the Hawkes process is effective in modeling the occurrence of extreme events (or “jumps”) that are not captured by traditional volatility measures.\nIn this paper, we integrate both models. The GARCH(1,1) model with a skewed Student‑t distribution is employed to capture the dynamics of conditional volatility. The model is specified as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha_1 \\varepsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2,\n\\]\nwhere \\(\\varepsilon_t\\) are the shocks and \\(\\sigma_t\\) is the conditional volatility. The use of a skewed Student‑t distribution (denoted as sstd in the rugarch package) allows us to capture heavy tails and asymmetry—characteristics often observed in financial return data.\nNext, standardized residuals are computed as:\n\\[\n\\tilde{\\varepsilon}_t = \\frac{\\varepsilon_t}{\\sigma_t},\n\\]\nand extreme events are defined as those for which \\(|\\tilde{\\varepsilon}_t| &gt; 2\\). These extreme events are then used to calibrate a Hawkes process with the intensity function:\n\\[\n\\lambda(t) = \\mu_h + \\alpha_h \\sum_{t_j &lt; t} e^{-\\beta_h (t-t_j)},\n\\]\nwhere \\(\\mu_h\\) is the baseline intensity, \\(\\alpha_h\\) quantifies the influence of past events, and \\(\\beta_h\\) is the decay rate.\nFinally, the forecasted baseline returns from the GARCH model are combined with the simulated jumps from the Hawkes process to generate three future price trajectories for the next 252 trading days.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#systematic-literature-review",
    "href": "garch_hawkes_simmodel.html#systematic-literature-review",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Systematic Literature Review",
    "text": "Systematic Literature Review\nForecasting financial time series has been extensively studied using econometric models and machine learning techniques. Among these models, Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models are widely used to model conditional volatility, while the Hawkes process is employed to capture the occurrence of extreme events and self-exciting shocks. However, the integration of these two models remains underexplored, presenting a significant gap in the literature.\n\n1. GARCH Models and Their Extensions\nGARCH models are traditionally applied to capture conditional heteroskedasticity in financial returns. Previous studies have explored various extensions of the GARCH model, including:\n\nGARCH combined with Neural Networks: The GARCH-GRNN model has been proposed to enhance forecasting by combining the statistical modeling of GARCH with the flexibility of neural networks (Li et al., 2005).\nBilinear-GARCH (BL-GARCH) Models: This approach augments the traditional GARCH model by incorporating bilinear modeling to capture more complex nonlinearities, demonstrating improved performance in financial time series forecasting (Oyewale et al., 2013).\nHybrid GARCH-Deep Learning Models: Models integrating deep neural networks with GARCH, such as GRU-GARCH, have been tested for volatility and risk forecasting (Michańków et al., 2023).\n\nWhile these approaches improve volatility forecasting, they still fail to fully capture the dynamics of extreme events and their interactions over time.\n\n\n2. Hawkes Processes in Extreme Event Modeling\nHawkes processes are widely used for modeling financial shocks due to their self-exciting nature. However, existing literature primarily focuses on applying these models to predict isolated extreme events, without integrating them with traditional volatility models. A recent study introduced the 2T-POT Hawkes model, which enhances conditional quantile forecasting for extreme log returns and outperforms GARCH-EVT models for financial risk prediction (Tomlinson et al., 2022).\nDespite this advancement, the study does not explicitly combine Hawkes processes with GARCH models, which could further improve predictive performance by integrating both volatility modeling and extreme event structure.\n\n\n3. Gaps in the Literature and Justification for the Proposed Approach\nThe main gap in the literature lies in the lack of integration between GARCH models and Hawkes processes for financial time series forecasting. While several studies have explored variations of GARCH and machine learning techniques, no work has comprehensively addressed the combination of GARCH forecasts with Hawkes-based scenario simulation.\nThis paper aims to bridge this gap by:\n\nIntegrating GARCH(1,1) forecasts (with a skewed Student-t distribution) with a Hawkes process, allowing us to capture both the dynamics of conditional volatility and the temporal structure of extreme events.\nSimulating future scenarios that account not only for forecasted volatility but also for the occurrence of self-exciting shocks, resulting in more robust predictions than machine learning or deep learning models alone.\nOvercoming the limitations of existing models, which often fail to properly capture structural breaks and the self-excitation of extreme financial shocks.\n\nThe literature suggests that hybrid models hold promise for financial forecasting, yet the specific combination of GARCH and Hawkes processes remains largely unexplored. This study aims to contribute to this field by developing a methodological framework that enhances predictive accuracy and robustness in financial asset forecasting.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#data-preparation",
    "href": "garch_hawkes_simmodel.html#data-preparation",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.1. Data Preparation",
    "text": "2.1. Data Preparation\nFirst, we download historical corn price data and compute daily log-returns. Let \\(P_t\\) denote the closing price at time \\(t\\). The log-return is computed as:\n\\[\nr_t = \\ln(P_t) - \\ln(P_{t-1}).\n\\]\nThese returns form the basis for our subsequent volatility modeling.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#garch11-model-with-skewed-studentt-distribution",
    "href": "garch_hawkes_simmodel.html#garch11-model-with-skewed-studentt-distribution",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.2. GARCH(1,1) Model with Skewed Student‑t Distribution",
    "text": "2.2. GARCH(1,1) Model with Skewed Student‑t Distribution\nWe model the conditional variance of returns using a GARCH(1,1) process. The conditional variance is defined as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha_1\\,\\varepsilon_{t-1}^2 + \\beta_1\\,\\sigma_{t-1}^2,\n\\]\nwhere: -\\(\\varepsilon_t\\) are the shocks, -\\(\\sigma_t\\) is the conditional volatility, -\\(\\omega\\) is the constant term, -\\(\\alpha_1\\) represents the ARCH effect, -\\(\\beta_1\\) represents the GARCH effect.\nIn our model, we assume that the return innovations follow a skewed Student‑t distribution (denoted as sstd in the rugarch package), which is well-suited to capture heavy tails and asymmetry often observed in financial data.\nAfter fitting the model, we extract the conditional volatility \\(\\sigma_t\\) for each time \\(t\\).",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#identification-of-extreme-events",
    "href": "garch_hawkes_simmodel.html#identification-of-extreme-events",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.3. Identification of Extreme Events",
    "text": "2.3. Identification of Extreme Events\nTo identify extreme events, we compute the standardized residuals:\n\\[\n\\tilde{\\varepsilon}_t = \\frac{\\varepsilon_t}{\\sigma_t}.\n\\]\nWe define an event as “extreme” if:\n\\[\n\\left|\\tilde{\\varepsilon}_t\\right| &gt; \\tau,\n\\]\nwith a chosen threshold (for example,\\(\\tau=2\\)). These extreme events (or jumps) are used to calibrate the Hawkes process.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#calibration-of-the-hawkes-process",
    "href": "garch_hawkes_simmodel.html#calibration-of-the-hawkes-process",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.4. Calibration of the Hawkes Process",
    "text": "2.4. Calibration of the Hawkes Process\nThe Hawkes process is a self-exciting point process used to model the occurrence of extreme events. Its intensity function is given by:\n\\[\n\\lambda(t) = \\mu_h + \\alpha_h \\sum_{t_j &lt; t} e^{-\\beta_h (t-t_j)},\n\\]\nwhere:\n-\\(\\mu_h\\) is the baseline intensity, -\\(\\alpha_h\\) quantifies the impact of past events, -\\(\\beta_h\\) is the decay rate.\nWe calibrate the parameters \\((\\mu_h, \\alpha_h, \\beta_h)\\) by maximizing the log-likelihood function based on the historical occurrence times of extreme events. The log-likelihood function is expressed as:\n\\[\n\\mathcal{L}(\\mu_h, \\alpha_h, \\beta_h) = \\sum_{i=1}^{N} \\ln\\left(\\mu_h + \\alpha_h \\sum_{t_j &lt; t_i} e^{-\\beta_h (t_i-t_j)}\\right) - \\mu_h T - \\frac{\\alpha_h}{\\beta_h}\\sum_{j=1}^{N}\\left(1-e^{-\\beta_h (T-t_j)}\\right),\n\\]\nwhere:\n-\\(T\\) is the total observation period, -\\(N\\) is the number of extreme events.\nWe solve for the parameters using numerical optimization techniques.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#simulation-of-future-price-trajectories",
    "href": "garch_hawkes_simmodel.html#simulation-of-future-price-trajectories",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "2.5. Simulation of Future Price Trajectories",
    "text": "2.5. Simulation of Future Price Trajectories\nThe final step is to simulate future price trajectories over a forecast horizon (e.g., 252 trading days). The simulation involves two components:\n\nBaseline Returns:\nThese are generated using the GARCH forecast for the next 252 days, assuming that returns follow a normal distribution with forecasted mean and volatility.\nJumps:\nFor each day, if the Hawkes process simulates extreme events, jump magnitudes are sampled from the historical extreme returns and added to the baseline return.\n\nThe total return for day \\(t\\) is given by:\n\\[\nr_t^{\\text{total}} = r_t^{\\text{baseline}} + r_t^{\\text{jump}},\n\\]\nand the simulated price is computed as:\n\\[\nP_t = P_0 \\times \\exp\\left(\\sum_{i=1}^{t} r_i^{\\text{total}}\\right),\n\\]\nwhere \\(P_0\\) is the last observed price.\nWe repeat this simulation multiple times (e.g., 3 scenarios) to capture the uncertainty in the future price evolution.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#data-preparation-1",
    "href": "garch_hawkes_simmodel.html#data-preparation-1",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we download historical corn price data and compute the logarithmic returns.\n\n\nCode\n# Load required packages\nlibrary(quantmod)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(timetk)\nlibrary(patchwork)\nlibrary(plotly)\nlibrary(rugarch)\n\n# Download historical corn prices (symbol ZC=F) from Yahoo Finance\ngetSymbols(\"ZC=F\", src = \"yahoo\", from = \"2020-01-01\", to = Sys.Date())\n\n\n[1] \"ZC=F\"\n\n\nCode\n# Create a data frame with the date and closing price\nmilho_data &lt;- data.frame(\n  Date  = index(`ZC=F`),\n  Close = as.numeric(Cl(`ZC=F`))\n)\n\n# Calculate daily log returns and remove rows with NA\nmilho_data &lt;- milho_data |&gt;\n  mutate(Return = c(NA, diff(log(Close)))) |&gt;\n  na.omit()\n\n# Display the first few rows\nhead(milho_data)\n\n\n\n  \n\n\n\nThe historical corn price data have been successfully loaded with columns for Date, Close, and the daily log returns (Return). These returns will serve as the basis for modeling volatility.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#estimating-the-garch11-model-with-skewed-studentt-distribution",
    "href": "garch_hawkes_simmodel.html#estimating-the-garch11-model-with-skewed-studentt-distribution",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Estimating the GARCH(1,1) Model with Skewed Student‑t Distribution",
    "text": "Estimating the GARCH(1,1) Model with Skewed Student‑t Distribution\nIn this section, we model the conditional variance of returns using a GARCH(1,1) process. The model is defined as:\n\\[\n\\sigma_t^2 = \\omega + \\alpha_1 \\varepsilon_{t-1}^2 + \\beta_1 \\sigma_{t-1}^2,\n\\]\nwhere: -$ _t$ are the shocks (unexpected returns), -$ _t$ is the conditional volatility, -$ $ is the constant term, -$ _1$ represents the ARCH effect (the impact of the previous period’s shock), -$ _1$ represents the GARCH effect (the persistence of past volatility).\nTo capture the heavy tails and asymmetry often observed in financial return data, we assume that the innovations follow a skewed Student‑t distribution (denoted as sstd in the rugarch package).\nThe following R code specifies and fits the GARCH(1,1) model with the skewed Student‑t distribution:\n\n\nCode\n# Create a vector of returns from the corn price data\nret &lt;- milho_data$Return\n\n# Specify the GARCH(1,1) model with the skewed Student‑t distribution\nspec &lt;- ugarchspec(\n  variance.model = list(\n    model = \"sGARCH\",\n    garchOrder = c(1, 1)\n  ),\n  mean.model = list(\n    armaOrder = c(0, 0),\n    include.mean = TRUE\n  ),\n  distribution.model = \"sstd\"  # skewed Student‑t\n)\n\n# Fit the model\nfit &lt;- ugarchfit(\n  spec   = spec,\n  data   = ret,\n  solver = \"hybrid\"\n)\n\n# Display the model summary\nshow(fit)\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(0,0,0)\nDistribution    : sstd \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.000423    0.000408   1.0369 0.299795\nomega   0.000022    0.000008   2.9239 0.003456\nalpha1  0.108857    0.028539   3.8142 0.000137\nbeta1   0.811656    0.046136  17.5926 0.000000\nskew    0.977761    0.041524  23.5466 0.000000\nshape   5.777605    0.851525   6.7850 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.000423    0.000409   1.0342 0.301044\nomega   0.000022    0.000011   2.0700 0.038454\nalpha1  0.108857    0.032841   3.3147 0.000917\nbeta1   0.811656    0.063952  12.6916 0.000000\nskew    0.977761    0.049087  19.9188 0.000000\nshape   5.777605    0.994373   5.8103 0.000000\n\nLogLikelihood : 3577.816 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.4951\nBayes        -5.4712\nShibata      -5.4951\nHannan-Quinn -5.4861\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                   0.008921  0.9248\nLag[2*(p+q)+(p+q)-1][2]  0.125596  0.9021\nLag[4*(p+q)+(p+q)-1][5]  3.025137  0.4025\nd.o.f=0\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                     0.4278  0.5131\nLag[2*(p+q)+(p+q)-1][5]    4.3204  0.2167\nLag[4*(p+q)+(p+q)-1][9]    7.0881  0.1919\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]     1.942 0.500 2.000  0.1634\nARCH Lag[5]     2.811 1.440 1.667  0.3184\nARCH Lag[7]     5.177 2.315 1.543  0.2072\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.2679\nIndividual Statistics:              \nmu     0.45670\nomega  0.13446\nalpha1 0.19537\nbeta1  0.15873\nskew   0.60654\nshape  0.07964\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.49 1.68 2.12\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value   prob sig\nSign Bias           1.2219 0.2220    \nNegative Sign Bias  0.8674 0.3859    \nPositive Sign Bias  1.6370 0.1019    \nJoint Effect        3.4392 0.3287    \n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     30.52      0.04551\n2    30     32.78      0.28651\n3    40     47.38      0.16775\n4    50     50.62      0.40957\n\n\nElapsed time : 0.3281171 \n\n\nThe model summary shows that the mean return$ $ is close to zero, which is expected for daily returns. The parameters\\(\\omega\\), \\(\\alpha_1\\), and$ _1$ indicate a low base level of volatility with high persistence, as evidenced by the sum \\(\\alpha_1 + \\beta_1\\) being close to 1. The skew and shape parameters reveal that the distribution has heavy tails and slight asymmetry, which justifies the use of a skewed Student‑t distribution for modeling the innovations.\nAfter fitting the model, we extract the conditional volatility, \\(\\sigma_t\\), for further analysis:\n\n\nCode\n# Extract the estimated conditional volatility and convert it to a numeric vector\ncond_vol &lt;- sigma(fit)\nmilho_data$Volatility &lt;- as.numeric(cond_vol)\n\n# Display the first few rows of the volatility data\nhead(milho_data[, c(\"Date\", \"Volatility\")])\n\n\n\n  \n\n\n\nThis conditional volatility will be used later in the process to identify extreme events and to simulate future price trajectories.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#visualizing-historical-data",
    "href": "garch_hawkes_simmodel.html#visualizing-historical-data",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Visualizing Historical Data",
    "text": "Visualizing Historical Data\nWe reformat the data to plot the series for Price, Log-Returns, and Volatility in faceted plots using timetk.\n\n\nCode\n# Convert data to long format\nmilho_data_long &lt;- milho_data |&gt;\n  select(Date, Close, Return, Volatility) |&gt;\n  pivot_longer(\n    cols = c(Close, Return, Volatility),\n    names_to = \"Serie\",\n    values_to = \"Valor\"\n  )\n\n# Set the order and labels for facets\nmilho_data_long$Serie &lt;- factor(milho_data_long$Serie,\n                                levels = c(\"Close\", \"Return\", \"Volatility\"),\n                                labels = c(\"Price\", \"Log-Returns\", \"Volatility\"))\n\n# Plot the faceted time series\nplot_faceted &lt;- milho_data_long |&gt;\n  group_by(Serie) |&gt;\n  plot_time_series(\n    .date_var    = Date,\n    .value       = Valor,\n    .interactive = FALSE,\n    .facet_ncol  = 1,\n    .smooth      = FALSE,\n    .title       = \"Corn Price, Log-Returns, and Conditional Volatility\"\n  ) +\n  theme(strip.background = element_rect(fill = \"white\", colour = \"white\"))\n\nggplotly(plot_faceted)",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#identifying-extreme-events-and-calibrating-the-hawkes-process",
    "href": "garch_hawkes_simmodel.html#identifying-extreme-events-and-calibrating-the-hawkes-process",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Identifying Extreme Events and Calibrating the Hawkes Process",
    "text": "Identifying Extreme Events and Calibrating the Hawkes Process\nWe compute the standardized residuals:\n\\[\n\\tilde{\\epsilon}_t = \\frac{\\mbox{log-return}_t}{\\sigma_t}\n\\]\nWe define extreme events as those with \\(|\\tilde{\\epsilon}_t|&gt;2\\)\n\n\nCode\n# Compute standardized residuals\nmilho_data &lt;- milho_data |&gt;\n  mutate(Standardized = Return / Volatility)\n\n# Define threshold for extreme events\nthreshold_std &lt;- 2\nextreme_idx &lt;- which(abs(milho_data$Standardized) &gt; threshold_std)\nextreme_returns &lt;- milho_data$Return[extreme_idx]\n\n# Convert dates of extreme events to time (in days relative to the first date)\nfirst_date &lt;- min(milho_data$Date)\nevent_times_hist &lt;- as.numeric(milho_data$Date[extreme_idx] - first_date)\n\n# Total observation time (in days)\nT_obs &lt;- as.numeric(max(milho_data$Date) - first_date)\n\ncat(\"Number of extreme events identified:\", length(extreme_idx), \"\\n\")\n\n\nNumber of extreme events identified: 64 \n\n\nThe number of extreme events indicates whether our threshold successfully captures significant shocks. These events are then used to calibrate the Hawkes process.\nNext, we calibrate the Hawkes process using a negative log-likelihood function. The intensity function for the Hawkes process is given by\n\\[\n\\lambda(t) = \\mu_h + \\alpha_h \\displaystyle \\sum_{t_j &lt; t} e^{-\\beta_h (t-t_j)}\n\\]\n\n\nCode\n# Negative log-likelihood function for the Hawkes process with exponential kernel\nneg_log_lik &lt;- function(params, event_times, T_total) {\n  mu_h    &lt;- params[1]\n  alpha_h &lt;- params[2]\n  beta_h  &lt;- params[3]\n  \n  if(mu_h &lt;= 0 || alpha_h &lt;= 0 || beta_h &lt;= 0) return(1e10)\n  \n  n &lt;- length(event_times)\n  log_sum &lt;- 0\n  for(i in seq_along(event_times)) {\n    ti &lt;- event_times[i]\n    if(i == 1) {\n      sum_exp &lt;- 0\n    } else {\n      sum_exp &lt;- sum(exp(-beta_h * (ti - event_times[1:(i-1)])))\n    }\n    lambda_ti &lt;- mu_h + alpha_h * sum_exp\n    log_sum &lt;- log_sum + log(lambda_ti)\n  }\n  integral_term &lt;- mu_h * T_total + (alpha_h / beta_h) * sum(1 - exp(-beta_h * (T_total - event_times)))\n  ll &lt;- log_sum - integral_term\n  return(-ll)  # negative for minimization\n}\n\n# Initial guess and calibration via optimization\ninit_par &lt;- c(mu_h = 0.1, alpha_h = 0.5, beta_h = 1.0)\n\nres_hawkes &lt;- optim(\n  par         = init_par,\n  fn          = neg_log_lik,\n  event_times = event_times_hist,\n  T_total     = T_obs,\n  method      = \"L-BFGS-B\",\n  lower       = c(1e-6, 1e-6, 1e-6),\n  upper       = c(Inf, Inf, Inf)\n)\n\nmu_h_hat    &lt;- res_hawkes$par[1]\nalpha_h_hat &lt;- res_hawkes$par[2]\nbeta_h_hat  &lt;- res_hawkes$par[3]\n\ncat(\"Calibrated Hawkes Parameters:\\n\")\n\n\nCalibrated Hawkes Parameters:\n\n\nCode\ncat(\"mu =\", mu_h_hat, \"\\n\")\n\n\nmu = 0.03387228 \n\n\nCode\ncat(\"alpha =\", alpha_h_hat, \"\\n\")\n\n\nalpha = 1e-06 \n\n\nCode\ncat(\"beta =\", beta_h_hat, \"\\n\")\n\n\nbeta = 5.569386 \n\n\nThe calibrated parameters (\\(u_h , \\alpha_h, \\beta_h\\)) describe the baseline intensity, the impact of past events, and the decay rate, respectively. These values will be used to simulate future extreme events.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#simulation-of-future-price-trajectories-1",
    "href": "garch_hawkes_simmodel.html#simulation-of-future-price-trajectories-1",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Simulation of Future Price Trajectories",
    "text": "Simulation of Future Price Trajectories\nUsing the GARCH forecast for the next 252 days and the Hawkes process for extreme events, we simulate three future price trajectories. The forecasted baseline returns are combined with jumps to obtain the total returns. The price path is computed as\n\\[\np_t = p_0 \\times \\exp{(\\displaystyle\\sum_{i=1}^{t}(\\mbox{baseline return}_i) + \\mbox{jump}_i) )}\n\\]\n\n\nCode\n# Forecast horizon: 252 days\nhorizon &lt;- 252\ngarch_forecast &lt;- ugarchforecast(fit, n.ahead = horizon)\nmu_forecast    &lt;- as.numeric(fitted(garch_forecast))\nsigma_forecast &lt;- as.numeric(sigma(garch_forecast))\n\n# Initial price: last historical price\nP0 &lt;- tail(milho_data$Close, 1)\n\n# Function to simulate future extreme events using Hawkes (thinning algorithm)\nsimulateHawkes &lt;- function(mu_h, alpha_h, beta_h, T_start, T_end, history = NULL) {\n  if(is.null(history)) history &lt;- numeric(0)\n  current_events &lt;- history\n  t &lt;- T_start\n  new_events &lt;- c()\n  \n  while(t &lt; T_end) {\n    n_recent &lt;- sum(current_events &gt; (t - 10))  # events in the last 10 days\n    lambda_bar &lt;- mu_h + alpha_h * n_recent\n    w &lt;- rexp(1, rate = lambda_bar)\n    t_candidate &lt;- t + w\n    if(t_candidate &gt; T_end) break\n    sum_exp &lt;- if(length(current_events[current_events &lt; t_candidate]) &gt; 0)\n      sum(exp(-beta_h * (t_candidate - current_events[current_events &lt; t_candidate]))) else 0\n    lambda_tc &lt;- mu_h + alpha_h * sum_exp\n    if(runif(1) &lt;= lambda_tc / lambda_bar) {\n      new_events &lt;- c(new_events, t_candidate)\n      current_events &lt;- c(current_events, t_candidate)\n    }\n    t &lt;- t_candidate\n  }\n  return(new_events)\n}\n\n# Simulate 3 trajectories\nn_sim &lt;- 3\nsimulated_paths &lt;- list()\n\nset.seed(123)\n\nfor(sim in 1:n_sim) {\n  new_event_times &lt;- simulateHawkes(mu_h_hat, alpha_h_hat, beta_h_hat,\n                                    T_start = T_obs,\n                                    T_end   = T_obs + horizon,\n                                    history = event_times_hist)\n  event_days &lt;- floor(new_event_times - T_obs) + 1\n  \n  # For each day, if there are events, sum the jumps. Jump values are sampled from the historical extreme returns.\n  jumps &lt;- rep(0, horizon)\n  if(length(event_days) &gt; 0) {\n    for(day in unique(event_days)) {\n      n_events &lt;- sum(event_days == day)\n      jump_vals &lt;- sample(extreme_returns, n_events, replace = TRUE)\n      jumps[day] &lt;- sum(jump_vals)\n    }\n  }\n  \n  # Simulate baseline returns using the GARCH forecast\n  baseline_returns &lt;- rnorm(horizon, mean = mu_forecast, sd = sigma_forecast)\n  \n  # Total return is the sum of baseline return and jumps\n  total_returns &lt;- baseline_returns + jumps\n  \n  # Price path: P_t = P0 * exp(cumsum(total_returns))\n  price_path &lt;- P0 * exp(cumsum(total_returns))\n  \n  simulated_paths[[sim]] &lt;- data.frame(\n    Day = 1:horizon,\n    Price = price_path,\n    Return = total_returns,\n    Trajectory = paste0(\"Sim\", sim)\n  )\n}\n\n# Combine simulated trajectories and add dates\nsim_df &lt;- do.call(rbind, simulated_paths)\nlast_date &lt;- tail(milho_data$Date, 1)\nsim_df &lt;- sim_df |&gt;\n  mutate(Date = as.Date(last_date) + Day)\n\n\nEach simulated trajectory combines the baseline returns (forecasted by the GARCH model) with the jumps generated by the Hawkes process. This yields realistic simulations of future prices that account for both dynamic volatility and extreme shocks.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "garch_hawkes_simmodel.html#combining-historical-data-and-forecast-in-a-single-plot",
    "href": "garch_hawkes_simmodel.html#combining-historical-data-and-forecast-in-a-single-plot",
    "title": "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process",
    "section": "Combining Historical Data and Forecast in a Single Plot",
    "text": "Combining Historical Data and Forecast in a Single Plot\nFinally, we overlay the historical corn prices with the simulated forecast trajectories. A vertical red line marks the start of the forecast period.\n\n\nCode\n# Prepare historical data for plotting\nhistorical_df &lt;- milho_data |&gt;\n  select(Date, Close) |&gt;\n  rename(Price = Close)\n\n# Create the final plot: solid line for historical data, dashed lines for simulations,\n# and a vertical dotted line indicating the forecast start.\np_sim &lt;- ggplot() +\n  geom_line(data = historical_df, aes(x = Date, y = Price),\n            color = \"black\", size = 1) +\n  geom_line(data = sim_df, aes(x = Date, y = Price, color = Trajectory),\n            linetype = \"dashed\") +\n  geom_vline(xintercept = as.numeric(last_date),\n             linetype = \"dotted\", color = \"red\") +\n  labs(title = \"Corn Prices: Historical Data and 3 Simulated Trajectories (Next 252 Days)\",\n       x = \"Date\", y = \"Price\") +\n  theme_minimal()\n\nggplotly(p_sim)\n\n\n\n\n\n\nThe final plot shows the historical corn prices (solid black line) and the three simulated future trajectories (dashed colored lines) for the next 252 days. The vertical red dotted line indicates the start of the forecast period.",
    "crumbs": [
      "About",
      "Predictive Models",
      "Corn Price Simulation: Integrating a Skewed Student‑t GARCH(1,1) Model and a Hawkes Process"
    ]
  },
  {
    "objectID": "projects.html#project-1",
    "href": "projects.html#project-1",
    "title": "Projects",
    "section": "Project 1",
    "text": "Project 1\n\nOur first Colab Notebook  here\n\n This project is related with the step News and Impact on price and volatilities dynamics"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2\n This project aims to find how and why the corn prices flows down for the zero in 2024-jun-16.\n\nMERCADO GLOBAL DE ARROZ ENFRENTA FORTE VOLATILIDADE EM JUNHO DE 2024\nPreço do Arroz Marca Zero no Gráfico em 16 de Junho: Erro Técnico ou Anomalia no Mercado?\nO Yahoo Finance, uma plataforma amplamente utilizada para acessar dados financeiros, cotações em tempo real e gráficos históricos, apresentou um dado intrigante sobre as variações do preço do arroz. No dia 16 de junho de 2024, o preço do alimento foi registrado como zero. Essa informação incomum levanta questionamentos sobre sua origem, com duas possíveis explicações: um erro técnico na plataforma ou uma anomalia no mercado de commodities.\nErro Técnico\nEspecialistas sugerem que a hipótese de um erro técnico não deve ser descartada. Falhas em plataformas financeiras podem ocorrer por problemas no endpoint, parâmetros incorretos ou dificuldades relacionadas ao processamento de dados por meio de APIs. Vale destacar que a data em questão cai em um domingo, dia em que muitos mercados permanecem fechados, o que pode ter gerado inconsistências nos registros.\nAnomalia no Mercado de Commodities\nPor outro lado, o mercado de commodities agrícolas, como o arroz, vive constante volatilidade devido a fatores como mudanças climáticas, safras impactadas, variações na demanda global e problemas logísticos. Em junho de 2024, os preços globais do arroz apresentaram um leve aumento de 0,5%, mas começaram a cair na metade do mês. Esse movimento foi impulsionado pela redução na demanda internacional e pelo aumento na oferta de exportação.\nAlém disso, previsões otimistas de produção nos principais países asiáticos indicaram um aumento global de 2% em relação ao ano anterior, contribuindo para a queda dos preços. No Brasil, o avanço no plantio da nova safra e a realização de leilões governamentais com valores abaixo dos registrados anteriormente pressionaram as cotações.\nA tendência de baixa continuou nos meses seguintes, com os preços internacionais atingindo, em novembro, o menor patamar em seis meses. A combinação de uma oferta crescente e um cenário positivo de produção nos principais produtores asiáticos intensificou a queda no mercado global.\nConclusão\nEmbora o registro de preço zero no dia 16 de junho tenha causado estranheza, a hipótese mais plausível é a ocorrência de um erro técnico na plataforma, devido à falta de movimentação no mercado nesse dia. Ainda assim, a volatilidade observada no mercado global de arroz durante o período reflete tendências de oferta e demanda, com a expectativa de safras robustas influenciando significativamente a queda nos preços. É essencial que plataformas financeiras revisem suas metodologias de coleta de dados para evitar informações equivocadas que possam confundir investidores e analistas.\nAppendix\nGráfico feito pela plataforma Yahoo finance\n\nREFERÊNCIAS\nastera.com\ninfoarroz.org\ngloborural.globo.com\nYahoo!Finance\nPor: Gabrielly dos Santos Mateus da Rosa, Rodrigo Hermont Ozon e Ricardo Vianna."
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "projects.html#atualidade-e-fatores-que-moldam-o-cenário",
    "href": "projects.html#atualidade-e-fatores-que-moldam-o-cenário",
    "title": "Projects",
    "section": "ATUALIDADE E FATORES QUE MOLDAM O CENÁRIO",
    "text": "ATUALIDADE E FATORES QUE MOLDAM O CENÁRIO\n\nMilho (CBOT):\nNa virada de junho, as cotações caíram para mínimas de seis meses, em torno de US$ 4,20/bu, e voltaram a rondar US$ 4,50/bu com previsões de calor intenso no Corn Belt.\nO Crop Watch da Reuters projeta rendimento de 184 bu/acre, acima da tendência do USDA, mas aponta vulnerabilidade do milho na polinização se julho terminar muito quente e danos localizados por tempestades.\nApesar de um corte de 50 milhões de bushels nos estoques finais no último WASDE, o USDA manteve o preço médio projetado da safra 25/26 em US$ 4,20/bu, sinalizando conforto na oferta global.\nMesmo com a retração chinesa (‑79% YoY), as exportações americanas seguem históricas, sustentando firmeza em contratos mais curtos.\n\n\nSoja (CBOT):\nTocou piso de três meses em US$ 10,32/bu no início de julho e ensaia reação técnica.\nPequim suspendeu licenças de três tradings dos EUA e reduziu compras, pressionando prêmios.\nO deslocamento da demanda para biodiesel (esmagamento projetado recorde de 2,54 bi bushels) garante suporte parcial.\nA soja depende ainda mais das chuvas de agosto, análises mostram forte conexão entre precipitação nesse período e rendimento final."
  },
  {
    "objectID": "projects.html#outros-fatores-que-aumentam-a-volatilidade",
    "href": "projects.html#outros-fatores-que-aumentam-a-volatilidade",
    "title": "Projects",
    "section": "OUTROS FATORES QUE AUMENTAM A VOLATILIDADE",
    "text": "OUTROS FATORES QUE AUMENTAM A VOLATILIDADE\n\nOrganização e frete\nO nível baixo do rio Mississipi aumenta o frete fluvial e reduz o “basis” em regiões de produção, tendo a possibilidade de de tirar milhões em receita dos agricultores\nNo Brasil, as chuvas e geadas atrasam a colheita da 2° safra de milho, conservando oferta doméstica alta, mas ampliando exportações, pressionando spreads CBOT-FOB.\n\n\nFundos e fluxos Globais\nOs fundos seguem com moderada posição vendida em grãos e oleaginosas, porém reduziram rapidamente o “Net short” antes do pico climático, deixando espaço para movimentos rápidos de cobertura caso o clima piore."
  },
  {
    "objectID": "projects.html#possíveis-cenários-para-os-preços",
    "href": "projects.html#possíveis-cenários-para-os-preços",
    "title": "Projects",
    "section": "POSSÍVEIS CENÁRIOS PARA OS PREÇOS",
    "text": "POSSÍVEIS CENÁRIOS PARA OS PREÇOS\nClima favorável e Oferta abundante: Milho deve oscilar em US$ 4,10-4,60/bu e soja em US$ 10-11/bu.\nImpacto Climático (calor ou seca): Cada perda de ponto percentual do milho pode subir o milho em 25-30 cent/bu; seca em agosto pode fazer com que a soja eleve acima de US$ 11, 50/bu.\nDificuldades logísticas persistentes: O basis negativo e fretes mais caros apertam o preço interno recebido, no entanto podem ampliar spreads exportadores e até beneficiar prêmios em portos brasileiros.\nDemanda interna por biodiesel: Expansões de mandatos nos Estados Unidos garantem amparo parcial para o óleo de soja, fortificando spreads “oilshare” e beneficiando a soja mesmo se as exportações caírem.\nAfastamento da China: Preserva um teto para rallies, uma eventual desaceleração comercial estimularia a demanda rapidamente, gerando alta expressiva."
  },
  {
    "objectID": "projects.html#conclusão",
    "href": "projects.html#conclusão",
    "title": "Projects",
    "section": "CONCLUSÃO",
    "text": "CONCLUSÃO\nOs preços do milho e da soja estão pressionados agora, muito por causa da expectativa de safra grande nos EUA e pela queda das compras da China. Mesmo assim, o mercado continua super sensível: se o clima apertar as lavouras ou se continuar difícil de escoar pelo Mississippi, os preços podem reagir rápido.\nAlém disso, a demanda interna nos EUA para biodiesel deve ajudar a segurar as quedas, principalmente na soja, que depende bastante disso.\nOu seja: nas próximas semanas, tudo deve girar em torno do clima no Corn Belt, da logística e de qualquer sinal de melhora na relação comercial entre EUA e China. E pra quem faz hedge ou acompanha de perto, vale mais ficar de olho no basis (diferença entre preços locais e futuros) e nos dados diários de exportação do que só olhar a cotação na tela."
  },
  {
    "objectID": "projects.html#referências",
    "href": "projects.html#referências",
    "title": "Projects",
    "section": "REFERÊNCIAS",
    "text": "REFERÊNCIAS\n\nhttps://www.reuters.com/markets/commodities/chicago-corn-soybeans-sink-effective-19-year-lows-2025-07-16/\nhttps://www.reuters.com/markets/commodities/crop-watch-yield-threats-still-possible-despite-near-ideal-season-2025-07-14/\nhttps://soygrowers.com/news-releases/july-2025-wasde-a-mixed-bag-for-soy-markets/\nhttps://www.reuters.com/markets/commodities/usda-rundown-record-us-corn-exports-us-biofuel-boom-2025-07-13/\nhttps://www.ers.usda.gov/topics/crops/soybeans-and-oil-crops/market-outlook\nhttps://www.agricultureofamerica.com/2025/04/01/estimating-the-impact-of-low-mississippi-river-levels-on-soybean-basis-in-the-midsouth"
  },
  {
    "objectID": "mindmap.html",
    "href": "mindmap.html",
    "title": "The Research Mindmap",
    "section": "",
    "text": "Overview\nThe following flowchart illustrates the research workflow, detailing the key methodologies and how they are interconnected. It starts from the problem identification and moves through volatility modeling, predictive modeling, and optimization processes.\n\n\n\n\n\n\n\n\n\n\n\nResearch Proposal: Advanced Techniques for Multiperiod Multiobjective Portfolio Optimization in Commodity Markets\nThis research addresses the significant challenge of modeling and forecasting agricultural commodity prices, which are subject to high volatility and complex dynamics. Agricultural markets are highly sensitive to external factors such as climatic changes, geopolitical events, and supply-demand imbalances, making accurate forecasting and risk management difficult for investors and policymakers.\n\nKey Components of the Research:\n\nVolatility Modeling Using GAMLSS and MSGARCH:\n\nGAMLSS: This technique provides a nuanced understanding of the distributional characteristics of commodity returns, capturing the probabilistic behaviors that traditional models often overlook.\nMSGARCH: By implementing the Markov-Switching GARCH model, the research captures regime shifts in volatility, which are common in commodities due to external shocks and systemic changes.\n\nMulti-Objective Portfolio Optimization:\n\nThis step involves developing a multi-objective optimization framework using evolutionary algorithms such as NSGA-II and Differential Evolution (DEOptim). These algorithms help optimize portfolio allocation by balancing risk, return, and diversification, particularly for portfolios with high-volatility assets like agricultural commodities.\n\nReinforcement Learning for Portfolio Management:\n\nThe research also introduces Reinforcement Learning (RL) methods, such as Q-Learning and K-Bandit algorithms, to adaptively manage portfolio strategies. These techniques are particularly suited for dynamic portfolio management, allowing strategies to evolve as market conditions change.\n\n\n\n\nContribution to Knowledge:\nThe research’s innovative contribution lies in combining these advanced econometric and machine learning techniques to tackle the unique challenges of commodity markets. It offers a comprehensive methodological framework that improves the modeling and forecasting of volatility and returns in agricultural commodities. This work enhances portfolio optimization strategies, offering practical applications for financial markets by providing tools that help portfolio managers make informed, data-driven decisions in the face of volatile market conditions.\nFinal Objective: The primary goal of this research is to develop robust methods for volatility modeling and portfolio optimization that dynamically adapt to market conditions. This approach offers a significant advancement in both academic and professional fields by providing actionable insights for managing portfolios in volatile commodity markets.\nHere we can see an much more complete and interactive mindmap: (if the HTML doesn´t work, see the pic below)\n\nAnother mindmap tryed to be made with networkD3\n\n\nCode\n# Libraries\nlibrary(networkD3)\nlibrary(dplyr)\n\n# Create the dataset (edges) for the mind map\ndata &lt;- data.frame(\n  from = c(\n    # Edges from the root node\n    \"PhD Thesis Proposal\", \"PhD Thesis Proposal\", \"PhD Thesis Proposal\", \n    \"PhD Thesis Proposal\", \"PhD Thesis Proposal\", \"PhD Thesis Proposal\", \"PhD Thesis Proposal\",\n    \n    # Edges under \"Research Motivation\"\n    \"Research Motivation\", \"Research Motivation\",\n    \n    # Edges under \"Methodological Framework\"\n    \"Methodological Framework\", \"Methodological Framework\",\n    \n    # Edges under \"Econometric Modeling Techniques & Volatility Modeling Approaches\"\n    \"Econometric Modeling Techniques & Volatility Modeling Approaches\", \"Econometric Modeling Techniques & Volatility Modeling Approaches\",\n    \n    # Edges under \"Portfolio Optimization & Allocation\"\n    \"Portfolio Optimization & Allocation\", \"Portfolio Optimization & Allocation\",\n    \n    # Edges under \"Multi-Objective Portfolio Optimization Framework\"\n    \"Multi-Objective Portfolio Optimization Framework\", \"Multi-Objective Portfolio Optimization Framework\",\n    \n    # Edges under \"Optimization Algorithms\"\n    \"Optimization Algorithms\", \"Optimization Algorithms\",\n    \n    # Edges under \"Expected Contributions\"\n    \"Expected Contributions\", \"Expected Contributions\",\n    \n    # Edges under \"Work Plan & Timeline\"\n    \"Work Plan & Timeline\", \"Work Plan & Timeline\", \"Work Plan & Timeline\", \n    \"Work Plan & Timeline\", \"Work Plan & Timeline\", \"Work Plan & Timeline\"\n  ),\n  to = c(\n    # Children of \"PhD Thesis Proposal\"\n    \"Research Motivation\", \"Methodological Framework\", \"Dynamic Programming for Multi-Period Optimization\", \n    \"Empirical Validation\", \"Practical Guidelines\", \"Expected Contributions\", \"Work Plan & Timeline\",\n    \n    # Children of \"Research Motivation\"\n    \"Challenges in Commodity Markets\", \"Need for Improved Forecasting & Risk Management\",\n    \n    # Children of \"Methodological Framework\"\n    \"Econometric Modeling Techniques & Volatility Modeling Approaches\", \"Portfolio Optimization & Allocation\",\n    \n    # Children of \"Econometric Modeling Techniques & Volatility Modeling Approaches\"\n    \"Distributional Analysis (GAMLSS Framework)\", \"Volatility & Regime Switching (MSGARCH Models)\",\n    \n    # Children of \"Portfolio Optimization & Allocation\"\n    \"Reinforcement Learning for Allocation (K-Bandit & Q-Learning)\", \"Multi-Objective Portfolio Optimization Framework\",\n    \n    # Children of \"Multi-Objective Portfolio Optimization Framework\"\n    \"Objective Functions (Return, Risk, Entropy)\", \"Optimization Algorithms\",\n    \n    # Children of \"Optimization Algorithms\"\n    \"Differential Evolution (DE)\", \"NSGA-II\",\n    \n    # Children of \"Expected Contributions\"\n    \"Advances in Volatility Modeling\", \"Innovative Portfolio Optimization\",\n    \n    # Children of \"Work Plan & Timeline\"\n    \"1-3 Months: Data Collection\", \"4-6 Months: Develop GAMLSS & MSGARCH\", \n    \"7-9 Months: Model Comparison & Optimization Framework\", \"10-12 Months: Integrate Reinforcement Learning\", \n    \"13-15 Months: Empirical Validation\", \"16-18 Months: Final Analysis & Thesis\"\n  ),\n  stringsAsFactors = FALSE\n)\n\n# Plot the network using simpleNetwork from networkD3\nsimpleNetwork(data, \n              height = \"800px\", \n              width = \"100%\", \n              Source = 1,      # column number for source nodes\n              Target = 2,      # column number for target nodes\n              linkDistance = 100,  # adjust as needed for spacing\n              charge = -300,       # node repulsion (tweak for layout)\n              fontSize = 14,       # font size for node labels\n              fontFamily = \"serif\",\n              linkColour = \"#666\", # color of the links\n              nodeColour = \"#69b3a2\",  # color of the nodes\n              opacity = 0.9,       # overall opacity\n              zoom = TRUE          # allow zooming\n)",
    "crumbs": [
      "About",
      "Intro",
      "The Research Mindmap"
    ]
  },
  {
    "objectID": "tesdtedarj.html#acknowledgments",
    "href": "tesdtedarj.html#acknowledgments",
    "title": "Rodrigo Hermont Ozon – Curriculum Vitae",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nI am grateful for the insightful comments offered by the anonymous peer reviewers at Books & Texts. The generosity and expertise of one and all have improved this study in innumerable ways and saved me from many errors; those that inevitably remain are entirely my own responsibility."
  },
  {
    "objectID": "tesdtedarj.html#footnotes",
    "href": "tesdtedarj.html#footnotes",
    "title": "Rodrigo Hermont Ozon – Curriculum Vitae",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEste é o conteúdo da minha nota de rodapé.↩︎"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#data-collection-and-preprocessing",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#data-collection-and-preprocessing",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Data Collection and Preprocessing",
    "text": "Data Collection and Preprocessing\nPython libs\n\n\nCode\n# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, Dense, Flatten, MaxPooling1D, Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport time\nimport warnings\nimport psutil\nimport os\nfrom keras_tuner import RandomSearch\n\nwarnings.filterwarnings('ignore')\n\n\nFirst of all, we need to check the hardware availability:\n\n\nCode\n# Collecting hardware information\ndef get_system_info():\n    return {\n        'CPU_cores': psutil.cpu_count(logical=True),\n        'CPU_freq_MHz': psutil.cpu_freq().current,\n        'Total_RAM_GB': round(psutil.virtual_memory().total / (1024 ** 3), 2),\n        'Available_RAM_GB': round(psutil.virtual_memory().available / (1024 ** 3), 2),\n    }\n\nsystem_info = get_system_info()\nprint(\"System Information:\", system_info)\n\n\nSystem Information: {'CPU_cores': 12, 'CPU_freq_MHz': 1800.0, 'Total_RAM_GB': 31.69, 'Available_RAM_GB': 15.36}\n\n\nLoading corn time series data:\n\n\nCode\n# Helper function to prepare LSTM input-output pairs\ndef prepare_data(series, time_steps):\n    X, y = [], []\n    for i in range(len(series) - time_steps):\n        X.append(series[i:(i + time_steps)])\n        y.append(series[i + time_steps][0])  # Extract only the target variable (returns)\n    return np.array(X), np.array(y)\n\n# Define ticker for Corn Futures\nticker = \"ZC=F\"\n\n# Downloading price data\ndata = yf.download(ticker, start=\"2015-01-01\")[[\"Close\"]].fillna(method='ffill').dropna()\n\n# Calculating logarithmic returns\nreturns = np.log(data / data.shift(1)).dropna()\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\nCNN+LSTM preprocessing step:\nNormalizing time series, to deal much better with outliers via QuantileTransformer function:\n\n\nCode\n# Normalizing the data using QuantileTransformer to handle outliers\nscaler = QuantileTransformer(output_distribution='normal')\nscaled_data = scaler.fit_transform(returns)\n\n# Preparing the dataset\ntime_steps = 30\nX, y = prepare_data(scaled_data, time_steps)"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#train-x-test-split",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#train-x-test-split",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Train x test split",
    "text": "Train x test split\nWe use 80% for train data x 20% test, and then we can reshape the data (samples, time steps and features)\n\n\nCode\n# Splitting the data into training and testing sets\nsplit_ratio = 0.8\ntrain_size  = int(len(X) * split_ratio)\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# Reshaping the input to be 3D [samples, time steps, features]\nX_train = np.expand_dims(X_train, axis=-1)\nX_test = np.expand_dims(X_test, axis=-1)"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#cnnlstm-setting",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#cnnlstm-setting",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "CNN+LSTM setting",
    "text": "CNN+LSTM setting\nWe can set an grid search for the CNN+LSTM time series forecasting\n\n\nCode\n# Building the CNN+LSTM model with additional GRU layer and Dropout for regularization\ndef build_model(include_conv=True, conv_filters=64, include_lstm=True, lstm_units=50, gru_units=0, activation='relu', include_dropout=True, dropout_rate=0.1, l2_regularization=0.01):\n    model = Sequential()\n    if include_conv:\n        model.add(Conv1D(filters=conv_filters, kernel_size=3, activation=activation, kernel_regularizer=l2(l2_regularization), input_shape=(time_steps, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n    if include_lstm:\n        model.add(LSTM(lstm_units, activation=activation, kernel_regularizer=l2(l2_regularization), return_sequences=True if gru_units &gt; 0 else False))\n    if gru_units &gt; 0:\n        model.add(GRU(gru_units, activation=activation, kernel_regularizer=l2(l2_regularization), return_sequences=False))\n    model.add(Flatten())\n    model.add(Dense(50, activation=activation, kernel_regularizer=l2(l2_regularization)))\n    if include_dropout:\n        model.add(Dropout(dropout_rate))  # Added dropout layer to reduce overfitting\n    model.add(Dense(1, kernel_regularizer=l2(l2_regularization)))\n    return model\n\n\nThen we can set the Adam optimizer and the early stopping for the model:\n\n\nCode\n# Defining optimizer and callbacks\ndef get_optimizer():\n    return Adam(learning_rate=0.001)\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n\nBy using the Keras tunner, we can set the hyperparameters tunning:\n\n\nCode\n# Hyperparameter search using Keras Tuner\ndef hyperparameter_search():\n    def build_model_kt(hp):\n        model = Sequential()\n        include_conv = hp.Boolean('include_conv')\n        if include_conv:\n            conv_filters = hp.Choice('conv_filters', values=[32, 64, 128])\n            model.add(Conv1D(filters=conv_filters, kernel_size=3, activation='relu', input_shape=(time_steps, 1)))\n            model.add(MaxPooling1D(pool_size=2))\n        \n        include_lstm = hp.Boolean('include_lstm')\n        if include_lstm:\n            lstm_units = hp.Choice('lstm_units', values=[50, 100])\n            model.add(LSTM(lstm_units, activation='relu', return_sequences=True))\n        \n        include_gru = hp.Boolean('include_gru')\n        if include_gru:\n            gru_units = hp.Choice('gru_units', values=[50, 100])\n            model.add(GRU(gru_units, activation='relu', return_sequences=False))\n        \n        model.add(Flatten())\n        model.add(Dense(50, activation='relu'))\n        \n        include_dropout = hp.Boolean('include_dropout')\n        if include_dropout:\n            dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n            model.add(Dropout(dropout_rate))\n        \n        model.add(Dense(1))\n        \n        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mse'])\n        return model\n\n    tuner = RandomSearch(\n        build_model_kt,\n        objective='val_mse',\n        max_trials=10,\n        executions_per_trial=1,\n        directory='my_dir',\n        project_name='corn_forecast_tuning'\n    )\n\n    tuner.search(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n    return best_hps\n\n# Running hyperparameter search\nbest_params = hyperparameter_search()\n\n\nReloading Tuner from my_dir\\corn_forecast_tuning\\tuner0.json"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#setting-the-best-model-before-hyp-tunning",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#setting-the-best-model-before-hyp-tunning",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Setting the best model before hyp tunning:",
    "text": "Setting the best model before hyp tunning:\n\n\nCode\ndef build_best_model(best_params):\n    model = Sequential()\n    \n    # Adiciona camada convolucional se ativada\n    if 'include_conv' in best_params.values and best_params.values['include_conv']:\n        conv_filters = best_params.values['conv_filters']\n        model.add(Conv1D(filters=conv_filters, kernel_size=3, activation='relu', input_shape=(time_steps, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n    \n    # Adiciona camada LSTM se ativada\n    if 'include_lstm' in best_params.values and best_params.values['include_lstm']:\n        lstm_units = best_params.values['lstm_units']\n        return_sequences = True if 'include_gru' in best_params.values and best_params.values['include_gru'] else False\n        model.add(LSTM(lstm_units, activation='relu', return_sequences=return_sequences))\n    \n    # Adiciona camada GRU se ativada\n    if 'include_gru' in best_params.values and best_params.values['include_gru']:\n        gru_units = best_params.values['gru_units']\n        model.add(GRU(gru_units, activation='relu', return_sequences=False))\n    \n    # Camadas adicionais\n    model.add(Flatten())\n    model.add(Dense(50, activation='relu'))\n    \n    # Adiciona camada Dropout se ativada\n    if 'include_dropout' in best_params.values and best_params.values['include_dropout']:\n        dropout_rate = best_params.values.get('dropout_rate', 0.1)\n        model.add(Dropout(dropout_rate))\n    \n    # Camada de saída\n    model.add(Dense(1))\n    \n    return model\n\n\nAnd then the history fit:\n\n\nCode\n# Certifique-se de que best_params foi definido corretamente\nprint(\"Best hyperparameters:\", best_params.values)\n\n# Construa o modelo\nmodel = build_best_model(best_params)\n\n# Compile o modelo\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n\n# Confirme que o modelo foi construído corretamente\nprint(\"Model summary:\")\nmodel.summary()\n\n# Treine o modelo\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=32,\n    validation_data=(X_test, y_test),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\n\nBest hyperparameters: {'include_conv': True, 'include_lstm': False, 'include_gru': True, 'include_dropout': False, 'conv_filters': 64, 'lstm_units': 50, 'dropout_rate': 0.30000000000000004, 'gru_units': 50}\nModel summary:\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv1d (Conv1D)                 │ (None, 28, 64)         │           256 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling1d (MaxPooling1D)    │ (None, 14, 64)         │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru (GRU)                       │ (None, 50)             │        17,400 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (Flatten)               │ (None, 50)             │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 50)             │         2,550 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │            51 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 20,257 (79.13 KB)\n\n\n\n Trainable params: 20,257 (79.13 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nEpoch 1/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1:08 1s/step - loss: 1.2850 2/63 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - loss: 1.1999\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.0864 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0757\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0585\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 1.0578 - val_loss: 1.0375\nEpoch 2/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.8427\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8980 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9371\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9514\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9630 - val_loss: 1.0372\nEpoch 3/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.9068\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9385 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9662\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9819\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9824 - val_loss: 1.0400\nEpoch 4/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.9736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0295 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0240\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0100\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9951 - val_loss: 1.0337\nEpoch 5/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 18ms/step - loss: 0.3063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8799 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9158\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9287\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b60/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9392\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9431 - val_loss: 1.0317\nEpoch 6/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.5503\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.1716 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.1285\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0997\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0702 - val_loss: 1.0346\nEpoch 7/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 0.8511\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9834 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0034\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0060\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0058 - val_loss: 1.0328\nEpoch 8/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.6796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0799 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0338\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b56/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0098\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0085 - val_loss: 1.0245\nEpoch 9/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.8550\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9303 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9423\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9524\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b56/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9630\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9673 - val_loss: 1.0267\nEpoch 10/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 22ms/step - loss: 0.9402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9745 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9730\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9552\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9683 - val_loss: 1.0581\nEpoch 11/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.6234\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0699 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0204\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0061\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9977\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9959 - val_loss: 1.0361\nEpoch 12/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.0651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.0411 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0439\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0378\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 1.0222 - val_loss: 1.0432\nEpoch 13/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 17ms/step - loss: 0.8871\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0297 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0144\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9788\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b61/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9748 - val_loss: 1.0658\nEpoch 14/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 1.0064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9454 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9841\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9793 - val_loss: 1.0207\nEpoch 15/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 21ms/step - loss: 0.9187\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9735 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9779\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9705\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9700\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9703 - val_loss: 1.0157\nEpoch 16/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - loss: 0.7292\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7873 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8567\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8830\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b62/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9162 - val_loss: 1.0800\nEpoch 17/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 0.8483\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7909 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8824\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9047 - val_loss: 1.0194\nEpoch 18/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 1.1324\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9649 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9571\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9547 - val_loss: 1.0270\nEpoch 19/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 0.7409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8275 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8665\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9146 - val_loss: 1.0140\nEpoch 20/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.9388\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9421 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9268\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9176 - val_loss: 1.0415\nEpoch 21/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.0534\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0351 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9931\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9509 - val_loss: 1.1168\nEpoch 22/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.0725\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0047 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9163\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9151\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9150 - val_loss: 1.0167\nEpoch 23/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.1516\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0398 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9983\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9630\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b62/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9524\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.9506 - val_loss: 1.0923\nEpoch 24/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.4731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.7419 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7905\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8176\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b46/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8290\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8377\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.8405 - val_loss: 1.0485\nEpoch 25/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - loss: 0.9536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8086 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8167\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8237\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8270 - val_loss: 1.0314\nEpoch 26/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 0.9687\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.9127 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.9010\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8868\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.8774\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8736 - val_loss: 1.0872\nEpoch 27/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - loss: 0.5574\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.7478 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7450\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b46/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7692\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7776\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.7845 - val_loss: 1.0884\nEpoch 28/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: 0.6348\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.6944 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7172\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.7411\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/63 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.7613\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.7723 - val_loss: 1.2496\nEpoch 29/50\n 1/63 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.7269\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8679 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8459\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8374\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/63 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 0.8332 - val_loss: 1.1209"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#model-backtest",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#model-backtest",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Model backtest",
    "text": "Model backtest\n\n\nCode\n# Predicting and evaluating the model\ny_pred = model.predict(X_test)\ny_test_inverse = scaler.inverse_transform(y_test.reshape(-1, 1))\ny_pred_inverse = scaler.inverse_transform(y_pred)\n\nmse = mean_squared_error(y_test_inverse, y_pred_inverse)\nmape = mean_absolute_percentage_error(y_test_inverse, y_pred_inverse) * 100  # Convert to percentage\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"Mean Absolute Percentage Error: {mape}%\")\n\n\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 133ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step\nMean Squared Error: 0.0002890161366973252\nMean Absolute Percentage Error: 3200378914379.584%\n\n\nAblation Study: Evaluating the Impact of Each Layer\nThe ablation study aims to assess the influence of different architectural components and configurations on the performance of the CNN+LSTM model for time series forecasting. By systematically altering specific layers and hyperparameters, we gain insights into the contribution of each element to the overall model accuracy and robustness.\n\nConfigurations\nThe following configurations were tested as part of the ablation study:\n\nConfiguration 1:\n\nConvolutional layer included with 64 filters.\nLSTM layer included with 50 units.\nGRU layer included with 50 units.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 2:\n\nConvolutional layer included with 32 filters.\nLSTM layer included with 50 units.\nGRU layer excluded.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 3:\n\nConvolutional layer included with 128 filters.\nLSTM layer included with 50 units.\nGRU layer included with 50 units.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 4:\n\nConvolutional layer included with 64 filters.\nLSTM layer excluded.\nGRU layer included with 50 units.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 5:\n\nConvolutional layer excluded.\nLSTM layer included with 50 units.\nGRU layer excluded.\nActivation function: ReLU.\nDropout layer included.\n\nConfiguration 6:\n\nConvolutional layer included with 64 filters.\nLSTM layer included with 50 units.\nGRU layer included with 50 units.\nActivation function: Tanh.\nDropout layer included.\n\n\nFor each configuration, the model was:\n\nCompiled with the Adam optimizer and mean squared error (MSE) loss function.\nTrained on the dataset with early stopping to prevent overfitting.\nEvaluated on the test set to calculate the mean squared error (MSE) and mean absolute percentage error (MAPE).\n\n\n\nCode\n# Ablation Study: Evaluating the impact of each layer\ndef ablation_study():\n    results = []\n    configurations = [\n        {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': True, 'conv_filters': 32, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': True, 'conv_filters': 128, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': True, 'conv_filters': 64, 'include_lstm': False, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': False, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True},\n        {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'tanh', 'include_dropout': True},\n    ]\n    for config in configurations:\n        optimizer = get_optimizer()  # Recreate optimizer for each configuration\n        model = build_model(**config)\n        model.build(input_shape=(None, time_steps, 1))  # Explicitly build the model with correct input shape\n        model.compile(optimizer=optimizer, loss='mse')\n        model.fit(\n            X_train, y_train,\n            epochs=50,\n            batch_size=32,\n            validation_data=(X_test, y_test),\n            callbacks=[early_stopping],\n            verbose=0\n        )\n        y_pred = model.predict(X_test)\n        y_pred_inverse = scaler.inverse_transform(y_pred)\n        mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n        mape = mean_absolute_percentage_error(y_test_inverse, y_pred_inverse) * 100  # Convert to percentage\n        results.append({'config': config, 'mse': mse, 'mape': mape})\n    return results\n\ndef generate_ablation_report(results):\n    report = \"\\nAblation Study Report\\n\"\n    report += \"===================\\n\"\n    for result in results:\n        report += (f\"Configuration: {result['config']}, MSE: {result['mse']}, MAPE: {result['mape']}%\\n\")\n    report += \"===================\\n\"\n    return report\n\nablation_results = ablation_study()\n\nprint(generate_ablation_report(ablation_results))\n\n\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 175ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 177ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 179ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/16 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 1s 100ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\n 1/16 ━━━━━━━━━━━━━━━━━━━━ 2s 180ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\n\nAblation Study Report\n===================\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}, MSE: 0.0002981824980357916, MAPE: 99.20318725099602%\nConfiguration: {'include_conv': True, 'conv_filters': 32, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True}, MSE: 0.00029739035256384154, MAPE: 415885659688.06445%\nConfiguration: {'include_conv': True, 'conv_filters': 128, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}, MSE: 0.00029949885368034694, MAPE: 2521000988657.1562%\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': False, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}, MSE: 0.00029937068511983806, MAPE: 2454989630722.6016%\nConfiguration: {'include_conv': False, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True}, MSE: 0.00029963638029014024, MAPE: 2339832904717.413%\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'tanh', 'include_dropout': True}, MSE: 0.0002986295145228721, MAPE: 305550686135.86865%\n===================\n\n\n\nThe results of the ablation study include detailed performance metrics for each configuration, providing a quantitative basis for identifying the most impactful layers and hyperparameters.\nA summary of the ablation results is presented in the report generated by the following function:"
  },
  {
    "objectID": "to be improved/CNNLSTM_Ablation_Corn.html#final-analysis-ablation-study-report",
    "href": "to be improved/CNNLSTM_Ablation_Corn.html#final-analysis-ablation-study-report",
    "title": "Final Project: Time Series Forecasting with CNNLSTMs, Neural Networks Eng. Class",
    "section": "Final Analysis: Ablation Study Report",
    "text": "Final Analysis: Ablation Study Report\nThe ablation study was conducted to evaluate the impact of different architectural configurations on the CNN+LSTM model’s performance in time series forecasting. The results are summarized below, focusing on the Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE) for each configuration.\n\nKey Observations\n\nBest Configuration:\n\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000296357\nMAPE: 99.18%\n\nThis configuration achieved the lowest MSE and the lowest MAPE among all setups tested, indicating a strong balance between convolutional and recurrent layers with sufficient filters and neurons.\n\nPoor Generalization with Higher Filters:\n\nConfiguration: {'include_conv': True, 'conv_filters': 128, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000297815\nMAPE: 2460438667962.732%\n\nWhile MSE remained competitive, the extremely high MAPE indicates poor generalization, likely caused by overfitting due to the increased complexity of convolutional filters.\n\nEffect of Removing LSTM Layers:\n\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': False, 'gru_units': 50, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000296818\nMAPE: 545753800735.74896%\n\nRemoving LSTM layers slightly increased MSE and drastically impacted MAPE, demonstrating that the absence of temporal dependency modeling reduces the network’s ability to forecast accurately.\n\nEffect of Excluding Convolutional Layers:\n\nConfiguration: {'include_conv': False, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000297667\nMAPE: 2250906755637.2246%\n\nThe exclusion of convolutional layers increased both MSE and MAPE significantly, reinforcing the importance of feature extraction via convolution for improved performance.\n\nEffect of Activation Function Change:\n\nConfiguration: {'include_conv': True, 'conv_filters': 64, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 50, 'activation': 'tanh', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000297963\nMAPE: 2525076505075.051%\n\nThe use of tanh instead of relu led to a slight increase in both MSE and MAPE, suggesting that relu performs better for this dataset and task.\n\nLower Filter Configurations:\n\nConfiguration: {'include_conv': True, 'conv_filters': 32, 'include_lstm': True, 'lstm_units': 50, 'gru_units': 0, 'activation': 'relu', 'include_dropout': True}\nPerformance:\n\nMSE: 0.000298109\nMAPE: 2649331642167.829%\n\nWhile a lower filter count in convolutional layers reduced complexity, it also led to poorer performance in both MSE and MAPE.\n\n\n\n\nConclusion\nThe ablation study highlights the importance of balancing the network’s architecture for time series forecasting: - A combination of convolutional layers (conv_filters=64) and LSTM layers (lstm_units=50) enhanced temporal feature extraction while maintaining efficient generalization. - Over-complex architectures (e.g., excessive filters or layers) resulted in higher errors, likely due to overfitting. - Proper activation functions (relu) contributed to improved performance compared to alternative configurations.\nThe findings provide actionable insights for tuning CNN+LSTM models in future applications."
  },
  {
    "objectID": "time_series_portfolio.html",
    "href": "time_series_portfolio.html",
    "title": "Data Extraction for preselected commodities portfolio",
    "section": "",
    "text": "Abstract\n\n\n\nThis small document have the goal to share the time series extraction and the two basic features building, like price returns and their conditional variance…\n\n\n\n  \n\n\nIntro\n[… to be written …]\n\n\n\nPython codes\n\nPython libsLoading time seriesPrices log-returnsLog-returns conditional variances\n\n\n\n\nCode\n\n#import yfinance as yf\nfrom yahooquery import Ticker\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom arch import arch_model\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal\n\n\n\n\nThe portfolio contains the following commodities price returns:\n\nCorn Futures\nWheat Futures\nKC HRW Wheat Futures\nRough Rice Futures\nFeeder Cattle Futures\nSoyMeal Futures\nSoy Meal Futures\nSoyBeans Futures\n\n\n\nCode\n# Tickers for portfolio\nTICKERS = [\n    \"ZC=F\",  # Corn Futures\n    \"ZO=F\",  # Wheat Futures\n    \"KE=F\",  # KC HRW Wheat Futures\n    \"ZR=F\",  # Rough Rice Futures\n    \"GF=F\",  # Feeder Cattle Futures\n    \"ZS=F\",  # SoyMeal Futures\n    \"ZM=F\",  # Soybean Meal Futures\n    \"ZL=F\"   # SoyBeans Futures\n]\n\n# Downloading data from Yahoo Finance using yahooquery\nstart_date = \"2019-01-01\"\nend_date = datetime.today().strftime('%Y-%m-%d')\n\nticker_data = Ticker(TICKERS)\nportfolio_prices = ticker_data.history(start=start_date, end=end_date)\n\n\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~3\\Lib\\site-packages\\yahooquery\\utils\\__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~3\\Lib\\site-packages\\yahooquery\\utils\\__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~3\\Lib\\site-packages\\yahooquery\\utils\\__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~3\\Lib\\site-packages\\yahooquery\\utils\\__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~3\\Lib\\site-packages\\yahooquery\\utils\\__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~3\\Lib\\site-packages\\yahooquery\\utils\\__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~3\\Lib\\site-packages\\yahooquery\\utils\\__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\nC:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\PYTHON~3\\Lib\\site-packages\\yahooquery\\utils\\__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n\n\nCode\n\n# Check if 'adjclose' exists in the returned data\nif isinstance(portfolio_prices, pd.DataFrame) and 'adjclose' in portfolio_prices.columns:\n    portfolio_prices = portfolio_prices[['adjclose']].reset_index()\nelse:\n    raise KeyError(\"The data fetched does not contain 'adjclose'. Check Yahoo Finance for availability.\")\n\n# Pivot the DataFrame to have a similar format to yfinance output\nportfolio_prices = portfolio_prices.pivot(index='date', columns='symbol', values='adjclose')\nportfolio_prices.index = pd.to_datetime(portfolio_prices.index)\nportfolio_prices.dropna(inplace=True)\n\n# Ensure the index is properly named\nportfolio_prices.index.name = \"Date\"\n\n# Renaming columns for better readability\nportfolio_prices.columns = [\n    \"corn_fut\",\n    \"wheat_fut\",\n    \"KCWheat_fut\",\n    \"rice_fut\",\n    \"Feeder_Cattle\",\n    \"soymeal_fut\",\n    \"soyF_fut\",\n    \"soybeans_fut\"\n]\n\n\nShowing the prices time series side by side: (data in level)\n\n\nCode\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal, theme_void\n\n# Ensure column name consistency\nportfolio_prices_long = portfolio_prices.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Price')\n\n# Function for visualization\ndef plot_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Create plot using plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Price', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Stack plots vertically\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Set minimal theme\n         theme(\n             figure_size=(fig_width, fig_height),\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Adjust subplot spacing\n         ))\n    return p\n\np_prices = plot_with_ggplot(portfolio_prices_long, 'Commodity Prices Over Time', 'Price', background='white', fig_height=14, fig_width=8)\np_prices\n\n\n&lt;plotnine.ggplot.ggplot object at 0x000001CBE230AA50&gt;\n\n\n\n\nObtain the returns time series (first feature):\n\\[\n\\mbox{Price log returns}_t = ln(p_t) - ln(p_{t-1})\n\\]\n\n\nCode\n\n# Calculate log returns\nportfolio_log_returns = np.log(portfolio_prices / portfolio_prices.shift(1)).dropna()\nportfolio_log_returns.columns = [\n    \"ret_corn_fut\",\n    \"ret_wheat_fut\",\n    \"ret_KCWheat_fut\",\n    \"ret_rice_fut\",\n    \"ret_Feeder_Cattle\",\n    \"ret_soymeal_fut\",\n    \"ret_soyF_fut\",\n    \"ret_soybeans_fut\"\n]\n\n\nAnd plot it:\n\n\nCode\n# Preparar os dados no formato long para os log-retornos\nportfolio_log_returns_long = portfolio_log_returns.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Log Return')\n\ndef plot_log_returns_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Log Return', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\np_log_returns = plot_log_returns_with_ggplot(portfolio_log_returns_long, 'Log Returns of Commodities Over Time', 'Log Return', background='white', fig_height=12, fig_width=8)\n\n# Exibir o gráfico\np_log_returns\n\n\n&lt;plotnine.ggplot.ggplot object at 0x000001CBE4804250&gt;\n\n\n\n\nAs risk measure, we use the conditional variances (volatilities), to deal better with day by day of the prices log-returns.\nThe GARCH(1,1) model with an asymmetric Student-t distribution is not directly available in most Python libraries. However, we can still use a GARCH(1,1) model with a standard Student-t distribution to estimate the conditional variance. The GARCH(1,1) model is represented as follows:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(r_t\\) is the log-return at time \\(t\\).\n\\(\\mu\\) is the mean of the returns.\n\\(\\epsilon_t\\) is the error term, modeled as conditional on past information.\n\\(\\sigma_t^2\\) is the conditional variance at time \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) are the parameters to be estimated, with \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) follows a Student-t distribution with \\(\\nu\\) degrees of freedom to capture the heavy tails observed in financial returns.\n\n\n\nCode\n# Initialize an empty DataFrame to store conditional variances\ncond_variances = pd.DataFrame(index=portfolio_log_returns.index, columns=portfolio_log_returns.columns)\n\n# Loop through each commodity's log-returns and fit a GARCH(1,1) model\nfor col in portfolio_log_returns.columns:\n    # Fit a GARCH(1,1) model with a Student-t distribution for each series of log returns\n    model = arch_model(portfolio_log_returns[col], vol='Garch', p=1, q=1, dist='t')\n    res = model.fit(disp='off')\n    \n    # Extract conditional variances and store them in the DataFrame\n    cond_variances[col] = res.conditional_volatility\n\n# Show the first few rows of the conditional variances DataFrame\ncond_variances.head()\n\n\n            ret_corn_fut  ret_wheat_fut  ...  ret_soyF_fut  ret_soybeans_fut\nDate                                     ...                                \n2019-01-03      0.038268       0.042802  ...      0.087175          0.007952\n2019-01-04      0.053450       0.048330  ...      0.104786          7.767961\n2019-01-07      0.065066       0.045604  ...      0.111780          7.954770\n2019-01-08      0.074834       0.045535  ...      0.114990          7.753064\n2019-01-09      0.083385       0.045063  ...      0.116232          7.695460\n\n[5 rows x 8 columns]\n\n\nand visualizing them:\n\n\nCode\n# Preparar os dados no formato long para as variâncias condicionais\ncond_variances_long = cond_variances.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Conditional Variance')\n\n# Função para criar o gráfico com fundo branco ou transparente e ajustar o tamanho da figura\ndef plot_cond_variances_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Conditional Variance', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\n# Exemplo de uso para as variâncias condicionais das commodities\np_cond_variances = plot_cond_variances_with_ggplot(cond_variances_long, 'Conditional Variances Over Time (GARCH(1,1))', 'Conditional Variance', background='white', fig_height=12, fig_width=8)\n\np_cond_variances\n\n\n&lt;plotnine.ggplot.ggplot object at 0x000001CBE4262CD0&gt;\n\n\n\n\n\n\n\n\nR codes\n\nR packagesPortfolio set\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n#library(plotly)\nlibrary(rugarch)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(caTools)\nlibrary(PerformanceAnalytics)\nlibrary(MASS)\nlibrary(PortfolioAnalytics)\nlibrary(ROI)\nrequire(ROI.plugin.glpk)\nrequire(ROI.plugin.quadprog)\nlibrary(quadprog)\nlibrary(corpcor)\nlibrary(DEoptim)\nlibrary(cowplot) # devtools::install_github(\"wilkelab/cowplot/\")\nlibrary(lattice)\nlibrary(timetk)\n\n\n\n\nLoading time series data, for portfolio setting…\n\n\nCode\ntickers &lt;- c(\n         \"ZC=F\", # Corn Futures\n         \"ZO=F\", # Wheat Futures\n         \"KE=F\", # Futuros KC HRW Wheat Futures\n         \"ZR=F\", # Rough Rice Futures\n         \"GF=F\", # Feeder Cattle Futures\n         \"ZS=F\", # SoyMeal Futures \n         \"ZM=F\", # Futuros farelo soja\n         \"ZL=F\"  # SoyBeans Futures\n)\n\n\nObtain daily prices and their returns:\n\n\nCode\nportfolioPrices &lt;- NULL\n  for ( Ticker in tickers )\n    portfolioPrices &lt;- cbind(\n      portfolioPrices, \n      getSymbols.yahoo(\n        Ticker,\n        from = \"2019-01-01\",\n        auto.assign = FALSE\n      )[,4]\n    )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"corn_fut\",\n  \"wheat_fut\",\n  \"KCWheat_fut\",\n  \"rice_fut\",\n  \"Feeder_Cattle\",\n  \"soymeal_fut\",\n  \"soyF_fut\",\n  \"soybeans_fut\"\n)\n\ntail(portfolioPrices)\n\n\n           corn_fut wheat_fut KCWheat_fut rice_fut Feeder_Cattle soymeal_fut\n2025-02-28   453.50    360.25      558.25   1328.5       274.975     1011.50\n2025-03-03   440.25    353.50      547.50   1334.5       274.025      998.25\n2025-03-04   436.00    374.75      534.00   1328.0       273.850      984.00\n2025-03-05   440.25    370.00      542.25   1306.0       276.100      997.75\n2025-03-06   449.50    366.25      551.50   1289.0       274.025     1014.00\n2025-03-07   455.25    360.50      551.25   1324.5       276.975     1010.25\n           soyF_fut soybeans_fut\n2025-02-28    291.7        43.53\n2025-03-03    290.1        42.90\n2025-03-04    285.9        42.27\n2025-03-05    293.0        42.44\n2025-03-06    297.1        42.60\n2025-03-07    296.5        42.87\n\n\nPlotting the time series prices (in level):\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along( corn_fut )\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\nObtain the returns time series (first feature):\n\\[\n\\mbox{Price log returns}_t = ln(p_t) - ln(p_{t-1})\n\\]\n\n\nCode\n# Calculate log returns for the portfolio prices\nportfolioReturs &lt;- na.omit(diff(log(portfolioPrices))) |&gt; as.data.frame()\n\ncolnames(portfolioReturs) &lt;- c(\n  \"ret_corn_fut\",\n  \"ret_wheat_fut\",\n  \"ret_KCWheat_fut\",\n  \"ret_rice_fut\",\n  \"ret_Feeder_Cattle\",\n  \"ret_soymeal_fut\",\n  \"ret_soyF_fut\",\n  \"ret_soybeans_fut\"\n)\n\nglimpse(portfolioReturs)\n\n\nRows: 1,554\nColumns: 8\n$ ret_corn_fut      &lt;dbl&gt; 0.0105891128, 0.0085218477, -0.0019601444, -0.005903…\n$ ret_wheat_fut     &lt;dbl&gt; 0.0008980692, 0.0053715438, -0.0017873106, 0.0115608…\n$ ret_KCWheat_fut   &lt;dbl&gt; 0.0220892515, 0.0049529571, -0.0059464992, 0.0039682…\n$ ret_rice_fut      &lt;dbl&gt; 0.0083930387, 0.0053934919, 0.0174507579, 0.00813596…\n$ ret_Feeder_Cattle &lt;dbl&gt; -0.0096783375, -0.0111522135, 0.0075628145, 0.011068…\n$ ret_soymeal_fut   &lt;dbl&gt; 0.0061281529, 0.0102224954, 0.0030190774, -0.0065988…\n$ ret_soyF_fut      &lt;dbl&gt; 0.0054513913, 0.0076457646, 0.0097900861, -0.0018874…\n$ ret_soybeans_fut  &lt;dbl&gt; 0.0099858421, 0.0081286732, -0.0052938051, -0.002834…\n\n\nCode\n#portfolioReturs &lt;- as.timeSeries(portfolioReturs)\n\n\nPlot all time series and their returns:\n\n\nCode\nportfolioReturs |&gt; \n  mutate(\n    time = seq_along( ret_corn_fut )\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\nPlotting the histograms:\n\n\nCode\nportfolioPrices_df &lt;- as_tibble(portfolioPrices, rownames = \"date\")\nportfolioPrices_df$date &lt;- ymd(portfolioPrices_df$date)\n\nportfolioReturs_df &lt;- na.omit( ROC( portfolioPrices ), type = \"discrete\" ) |&gt;\n  as_tibble(rownames = \"date\")\nportfolioReturs_df$date &lt;- ymd(portfolioReturs_df$date)\ncolnames(portfolioReturs_df) &lt;- c(\n  \"date\",\n  \"ret_corn_fut\",\n  \"ret_wheat_fut\",\n  \"ret_KCWheat_fut\",\n  \"ret_rice_fut\",\n  \"ret_Feeder_Cattle\",\n  \"ret_soymeal_fut\",\n  \"ret_soyF_fut\",\n  \"ret_soybeans_fut\"\n)\n\n# Remover a coluna com nome NA\nportfolioReturs_df &lt;- portfolioReturs_df[, !is.na(colnames(portfolioReturs_df))]\n\n# Verificar novamente os nomes das colunas para garantir que estão corretos\ncolnames(portfolioReturs_df)\n\n\n[1] \"date\"              \"ret_corn_fut\"      \"ret_wheat_fut\"    \n[4] \"ret_KCWheat_fut\"   \"ret_rice_fut\"      \"ret_Feeder_Cattle\"\n[7] \"ret_soymeal_fut\"   \"ret_soyF_fut\"      \"ret_soybeans_fut\" \n\n\nCode\nportfolioReturs_long &lt;- portfolioReturs_df |&gt; \n  pivot_longer(\n    cols = -date, # Exclui a coluna de data\n    names_to = \"fut_type\", \n    values_to = \"returns\"\n  )\n\nggplot(portfolioReturs_long, aes(x = returns)) + \n  geom_histogram(aes(y = ..density..), binwidth = .01, color = \"black\", fill = \"white\") +\n  geom_density(alpha = .2, fill=\"lightgray\") +\n  theme_minimal() +\n  theme(\n    axis.line  = element_line(colour = \"black\"),\n    axis.text  = element_text(colour = \"black\"),  \n    axis.ticks = element_line(colour = \"black\"), \n    legend.position = c(.1,.9), \n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank()\n  ) +\n  theme(plot.title   = element_text(size = 10),  \n        axis.title.x = element_text(size = 7), \n        axis.title.y = element_text(size = 7)) + \n  labs(x = \"Returns\", y = \"Density\") +\n  facet_wrap(~fut_type, scales = \"free\", ncol = 2) \n\n\n\n\n\n\n\n\n\nAnd finnaly, the last feature, is called, the conditional variance (risk measure), obtained by GARCH(1,1) model, formalized as:\nThe GARCH(1,1) model with asymmetric Student-t distribution can be represented mathematically as:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(r_t\\) is the return at time \\(t\\).\n\\(\\mu\\) is the mean of the returns.\n\\(\\epsilon_t\\) is the error term, modeled as conditional on past information.\n\\(\\sigma_t^2\\) is the conditional variance at time \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) are the parameters to be estimated, with \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) follows an asymmetric Student-t distribution with \\(\\nu\\) degrees of freedom to better capture the heavy tails and skewness observed in financial returns.\n\n\n\nCode\n# Load necessary packages\nlibrary(rugarch)\n\n# Define the GARCH(1,1) model specification with Student-t distribution\nspec &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n  mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),\n  distribution.model = \"std\" # Using Student-t distribution\n)\n\n# Estimate the model for each asset in the portfolio and extract conditional variances\ngarch_models &lt;- list()\nconditional_variances &lt;- list()\n\nfor (i in colnames(portfolioReturs)) {\n  garch_models[[i]] &lt;- ugarchfit(spec, data = portfolioReturs[[i]])\n  conditional_variances[[i]] &lt;- sigma(garch_models[[i]])^2\n}\n\n# Convert conditional variances list to a data frame\nconditional_variances_df &lt;- do.call(cbind, conditional_variances) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(time = seq_along(conditional_variances[[1]]))\n\ncolnames(conditional_variances_df) &lt;- c(\n  \"cond_var_corn_fut\",\n  \"cond_var_wheat_fut\",\n  \"cond_var_KCWheat_fut\",\n  \"cond_var_rice_fut\",\n  \"cond_var_Feeder_Cattle\",\n  \"cond_var_soymeal_fut\",\n  \"cond_var_soyF_fut\",\n  \"cond_var_soybeans_fut\",\n  \"time\"\n)\n\n# Reshape data for plotting\nconditional_variances_long &lt;- conditional_variances_df %&gt;%\n  pivot_longer(!time, names_to = \"Variables\", values_to = \"Value\")\n\n\nAnd the plot of the conditional variance (risk):\n\n\nCode\nconditional_variances_long |&gt; \n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\nReferences\nGujarati, D., N. (2004) Basic Econometrics, fourth edition, The McGraw−Hill Companies\nHair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2019). Multivariate Data Analysis. Pearson.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on oct 2023.\n \n \n\n\n\nCode\n# Total timing to compile this Quarto document\n\nend_time = datetime.now()\ntime_diff = end_time - start_time\n\nprint(f\"Total Quarto document compiling time: {time_diff}\")\n\n\nTotal Quarto document compiling time: 0:00:56.102477",
    "crumbs": [
      "About",
      "Predictive Models",
      "Data Extraction for preselected commodities portfolio"
    ]
  },
  {
    "objectID": "openAi_agent.html#environment-secrets",
    "href": "openAi_agent.html#environment-secrets",
    "title": "Automating Commodity‑Market Intelligence with ChatGPT  Plus and R:",
    "section": "3.1  Environment & Secrets",
    "text": "3.1  Environment & Secrets\nCreate a project‑level .Renviron (not committed to VCS):\n\n\nCode\nNEWSAPI_KEY   = \"YOUR_NEWSAPI_KEY\"\nOPENAI_API_KEY= \"YOUR_OPENAI_KEY\"\n\n\nThen restart R or call readRenviron(“~/.Renviron”).",
    "crumbs": [
      "About",
      "Commodities (grains) news",
      "Automating Commodity‑Market Intelligence with ChatGPT  Plus and R:"
    ]
  },
  {
    "objectID": "openAi_agent.html#helper-functions",
    "href": "openAi_agent.html#helper-functions",
    "title": "Automating Commodity‑Market Intelligence with ChatGPT  Plus and R:",
    "section": "3.2  Helper Functions",
    "text": "3.2  Helper Functions\n\n\nCode\nlibrary(httr2)     # next‑gen HTTP for R\nlibrary(jsonlite)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(glue)\nlibrary(openai)    # remotes::install_github(\"rOpenAI/openai\")\n\n# Small wrapper ----------------------------------------------------------\nnews_endpoint &lt;- \"https://newsapi.org/v2/everything\"\n\nfetch_news &lt;- function(keyword,\n                       from   = Sys.Date() - 1,\n                       to     = Sys.Date(),\n                       page_size = 100) {\n\n  resp &lt;- request(news_endpoint) |&gt;\n    req_url_query(\n      q           = keyword,\n      language    = \"en\",\n      sortBy      = \"publishedAt\",\n      from        = as.character(from),\n      to          = as.character(to),\n      pageSize    = page_size,\n      apiKey      = Sys.getenv(\"NEWSAPI_KEY\")\n    ) |&gt;\n    req_perform()\n\n  out &lt;- resp |&gt;\n    resp_body_json() |&gt;\n    purrr::pluck(\"articles\") |&gt;\n    tibble::as_tibble() |&gt;\n    mutate(\n      keyword     = keyword,\n      publishedAt = ymd_hms(publishedAt, tz = \"UTC\")\n    )\n  return(out)\n}\n\n# Summarise a tibble of articles with OpenAI ------------------------------\nsummarise_cluster &lt;- function(df, model = \"gpt-4o-mini\") {\n\n  prompt &lt;- glue(\"\nYou are a financial analyst. Produce a 5‑bullet summary of the following\n{nrow(df)} news headlines about **{unique(df$keyword)}** published in the last 24 h.\nFocus on market‑moving information (prices, policy, weather, supply‑demand).\nWrite in clear, jargon‑free English. 120 words max.\n\nHEADLINES:\n{paste0('- ', df$title, collapse = '\\n')}\n\")\n\n  res &lt;- openai::create_chat_completion(\n    model  = model,\n    messages = list(\n      list(role = \"system\",\n           content = \"You are an expert commodity market analyst.\"),\n      list(role = \"user\", content = prompt)\n    ),\n    temperature = 0.3\n  )\n\n  summary &lt;- res$choices[[1]]$message$content\n  tibble(keyword = unique(df$keyword), summary = summary)\n}",
    "crumbs": [
      "About",
      "Commodities (grains) news",
      "Automating Commodity‑Market Intelligence with ChatGPT  Plus and R:"
    ]
  },
  {
    "objectID": "openAi_agent.html#daily-driver-script-fetch_news.r",
    "href": "openAi_agent.html#daily-driver-script-fetch_news.r",
    "title": "Automating Commodity‑Market Intelligence with ChatGPT  Plus and R:",
    "section": "3.3  Daily Driver Script (fetch_news.R)",
    "text": "3.3  Daily Driver Script (fetch_news.R)\n\n\nCode\nlibrary(dplyr)\nlibrary(purrr)\nsource(\"functions.R\")   # helpers above\n\ncommodities &lt;- c(\"corn\", \"soybean\", \"soybean meal\", \"soybean oil\",\n                 \"wheat\", \"coffee\", \"cotton\")\n\n# 1 Pull raw headlines -----------------------------------------------------\nnews_raw &lt;- map_dfr(commodities, fetch_news)\n\n# 2 Deduplicate & keep latest headline per source --------------------------\nnews_clean &lt;- news_raw |&gt;\n  distinct(url, .keep_all = TRUE)\n\n# 3 Generate bullet summaries via OpenAI -----------------------------------\nnews_summaries &lt;- news_clean |&gt;\n  group_split(keyword) |&gt;\n  map_dfr(summarise_cluster)\n\n# 4 Persist to disk for the Quarto doc -------------------------------------\n#saveRDS(news_clean,      file = \"data/news_headlines.rds\")\n#saveRDS(news_summaries,  file = \"data/news_summaries.rds\")",
    "crumbs": [
      "About",
      "Commodities (grains) news",
      "Automating Commodity‑Market Intelligence with ChatGPT  Plus and R:"
    ]
  },
  {
    "objectID": "openAi_agent.html#reporting-with-quarto-daily_report.qmd",
    "href": "openAi_agent.html#reporting-with-quarto-daily_report.qmd",
    "title": "Automating Commodity‑Market Intelligence with ChatGPT  Plus and R:",
    "section": "3.4  Reporting with Quarto (daily_report.qmd)",
    "text": "3.4  Reporting with Quarto (daily_report.qmd)\n\n\nCode\n---\ntitle: \"Daily Commodity News Digest\"\nformat:\n  html:\n    self-contained: true\n    toc: false\n    theme: cosmo\n---\n\nlibrary(gt)\nlibrary(glue)\n\n#news_clean     &lt;- readRDS(\"data/news_headlines.rds\")\n#news_summaries &lt;- readRDS(\"data/news_summaries.rds\")\n\nnews_summaries |&gt;\n  mutate(summary = gsub('\\\\n', ' ', summary)) |&gt;\n  knitr::kable()\n\nnews_clean |&gt;\n  select(publishedAt, source.name, title, url) |&gt;\n  arrange(desc(publishedAt)) |&gt;\n  gt::gt() |&gt;\n  gt::fmt_datetime(publishedAt, rows = everything(), sep = \" \")\n\n\nThe content file produced above is minimal; we keep heavy tables folded behind Quarto’s code‑fold UI so the landing page loads fast.\n\n\nCode\n## 3.5  Scheduling with **cronR**\n\nlibrary(cronR)\n\ncmd &lt;- cron_rscript(\"fetch_news.R\")\n\ncron_add(command = cmd,\n         frequency = 'daily',\n         at        = \"07:00\",\n         description = \"Fetch & summarise commodity news\")\n\n\nThe same cron entry can also trigger quarto render daily_report.qmd. If you host your site on GitHub Pages, push the rendered HTML to the docs/ folder and commit. GitHub will automatically redeploy.",
    "crumbs": [
      "About",
      "Commodities (grains) news",
      "Automating Commodity‑Market Intelligence with ChatGPT  Plus and R:"
    ]
  },
  {
    "objectID": "markovian_bayesian_timeseries.html",
    "href": "markovian_bayesian_timeseries.html",
    "title": "Markovian Bayesian Time Series",
    "section": "",
    "text": "Here we used the time series breakpoints forecasting TSLM model:\nFollow the Google Colab here: https://colab.research.google.com/drive/1w6joN7-RkbUCnMYn-jfxVB8JTR2Xy2G8\nOr here: https://colab.research.google.com/drive/1FOpFHOJ2wdekNXjVap31x5SAl2-7eVOB",
    "crumbs": [
      "About",
      "Predictive Models",
      "Markovian Bayesian Time Series"
    ]
  },
  {
    "objectID": "portfolio_multiobj_opt_algos.html",
    "href": "portfolio_multiobj_opt_algos.html",
    "title": "Markovian Bayesian Time Series",
    "section": "",
    "text": "We used the followinf Colab Notebook for this purpose:\n\nhttps://colab.research.google.com/drive/16IkoDSpoTjtAmpH4CUAVD76pBDzrHIQC",
    "crumbs": [
      "About",
      "MultiObjective Multiperiod Optimization",
      "Markovian Bayesian Time Series"
    ]
  },
  {
    "objectID": "einforcement_algos_decision_making.html",
    "href": "einforcement_algos_decision_making.html",
    "title": "Reinforcement Learning Decision Making Notebook",
    "section": "",
    "text": "We used the following Colab Notebook for this purpose:\n\nFollow here"
  },
  {
    "objectID": "pareto_front_reinforcement.html",
    "href": "pareto_front_reinforcement.html",
    "title": "Multiobjective Portfolio Optim routines",
    "section": "",
    "text": "We used the following Colab Notebook for this purpose:",
    "crumbs": [
      "About",
      "Reinforcement Learning Strategies",
      "Multiobjective Portfolio Optim routines"
    ]
  },
  {
    "objectID": "reinforcement_algos_decision_making.html",
    "href": "reinforcement_algos_decision_making.html",
    "title": "Reinforcement Learning Decision Making Notebook",
    "section": "",
    "text": "We used the following Colab Notebook for this purpose:\n\nFollow here\nAnd here too",
    "crumbs": [
      "About",
      "Reinforcement Learning Strategies",
      "Reinforcement Learning Decision Making Notebook"
    ]
  }
]