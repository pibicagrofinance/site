[
  {
    "objectID": "time_series_portfolio.html",
    "href": "time_series_portfolio.html",
    "title": "Data Extraction for preselected commodities portfolio",
    "section": "",
    "text": "Abstract\n\n\n\nThis small document have the goal to share the time series extraction and the two basic features building, like price returns and their conditional variance…\n\n\n\n  \n\n\nIntro\n[… to be written …]\n\n\n\nPython codes\n\nPython libsLoading time seriesPrices log-returnsLog-returns conditional variances\n\n\n\n\nCode\n\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom arch import arch_model\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal\n\n\n\n\nThe portfolio contains the following commodities price returns:\n\nCorn Futures\nWheat Futures\nKC HRW Wheat Futures\nRough Rice Futures\nFeeder Cattle Futures\nSoyMeal Futures\nSoy Meal Futures\nSoyBeans Futures\n\n\n\nCode\n# Tickers for portfolio\nTICKERS = [\n    \"ZC=F\",  # Corn Futures\n    \"ZO=F\",  # Wheat Futures\n    \"KE=F\",  # KC HRW Wheat Futures\n    \"ZR=F\",  # Rough Rice Futures\n    \"GF=F\",  # Feeder Cattle Futures\n    \"ZS=F\",  # SoyMeal Futures\n    \"ZM=F\",  # Soybean Meal Futures\n    \"ZL=F\"   # SoyBeans Futures\n]\n\n\n# Downloading data from Yahoo Finance\nportfolio_prices = yf.download(TICKERS, start=\"2019-01-01\")['Adj Close']\n\n\n\n[                       0%                       ]\n[************          25%                       ]  2 of 8 completed\n[******************    38%                       ]  3 of 8 completed\n[**********************50%                       ]  4 of 8 completed\n[**********************62%*****                  ]  5 of 8 completed\n[**********************75%***********            ]  6 of 8 completed\n[**********************88%*****************      ]  7 of 8 completed\n[*********************100%***********************]  8 of 8 completed\n\n\nCode\nportfolio_prices.dropna(inplace=True)\n\n# Renaming columns for better readability\nportfolio_prices.columns = [\n    \"corn_fut\",\n    \"wheat_fut\",\n    \"KCWheat_fut\",\n    \"rice_fut\",\n    \"Feeder_Cattle\",\n    \"soymeal_fut\",\n    \"soyF_fut\",\n    \"soybeans_fut\"\n]\n\n\nShowing the prices time series side by side: (data in level)\n\n\nCode\nfrom plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme, element_text, theme_minimal, theme_void\n\n# Preparar os dados no formato long (necessário para plotnine/ggplot2)\nportfolio_prices_long = portfolio_prices.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Price')\n\ndef plot_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Price', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\np_prices = plot_with_ggplot(portfolio_prices_long, 'Commodity Prices Over Time', 'Price', background='white', fig_height=14, fig_width=8)\n\np_prices\n\n\n&lt;string&gt;:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\Rodrigo\\AppData\\Local\\Programs\\Python\\PYTHON~1\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1400)&gt;\n\n\n\n\n\n\n\n\n\n\n\nObtain the returns time series (first feature):\n\\[\n\\mbox{Price log returns}_t = ln(p_t) - ln(p_{t-1})\n\\]\n\n\nCode\n\n# Calculate log returns\nportfolio_log_returns = np.log(portfolio_prices / portfolio_prices.shift(1)).dropna()\nportfolio_log_returns.columns = [\n    \"ret_corn_fut\",\n    \"ret_wheat_fut\",\n    \"ret_KCWheat_fut\",\n    \"ret_rice_fut\",\n    \"ret_Feeder_Cattle\",\n    \"ret_soymeal_fut\",\n    \"ret_soyF_fut\",\n    \"ret_soybeans_fut\"\n]\n\n\nAnd plot it:\n\n\nCode\n# Preparar os dados no formato long para os log-retornos\nportfolio_log_returns_long = portfolio_log_returns.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Log Return')\n\ndef plot_log_returns_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Log Return', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\np_log_returns = plot_log_returns_with_ggplot(portfolio_log_returns_long, 'Log Returns of Commodities Over Time', 'Log Return', background='white', fig_height=12, fig_width=8)\n\n# Exibir o gráfico\np_log_returns\n\n\n&lt;string&gt;:3: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\Rodrigo\\AppData\\Local\\Programs\\Python\\PYTHON~1\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1200)&gt;\n\n\n\n\n\n\n\n\n\n\n\nAs risk measure, we use the conditional variances (volatilities), to deal better with day by day of the prices log-returns.\nThe GARCH(1,1) model with an asymmetric Student-t distribution is not directly available in most Python libraries. However, we can still use a GARCH(1,1) model with a standard Student-t distribution to estimate the conditional variance. The GARCH(1,1) model is represented as follows:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(r_t\\) is the log-return at time \\(t\\).\n\\(\\mu\\) is the mean of the returns.\n\\(\\epsilon_t\\) is the error term, modeled as conditional on past information.\n\\(\\sigma_t^2\\) is the conditional variance at time \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) are the parameters to be estimated, with \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) follows a Student-t distribution with \\(\\nu\\) degrees of freedom to capture the heavy tails observed in financial returns.\n\n\n\nCode\n# Initialize an empty DataFrame to store conditional variances\ncond_variances = pd.DataFrame(index=portfolio_log_returns.index, columns=portfolio_log_returns.columns)\n\n# Loop through each commodity's log-returns and fit a GARCH(1,1) model\nfor col in portfolio_log_returns.columns:\n    # Fit a GARCH(1,1) model with a Student-t distribution for each series of log returns\n    model = arch_model(portfolio_log_returns[col], vol='Garch', p=1, q=1, dist='t')\n    res = model.fit(disp='off')\n    \n    # Extract conditional variances and store them in the DataFrame\n    cond_variances[col] = res.conditional_volatility\n\n# Show the first few rows of the conditional variances DataFrame\ncond_variances.head()\n\n\n                           ret_corn_fut  ...  ret_soybeans_fut\nDate                                     ...                  \n2019-01-03 00:00:00+00:00      0.036112  ...          0.027180\n2019-01-04 00:00:00+00:00      0.037841  ...          0.027044\n2019-01-07 00:00:00+00:00      0.038367  ...          0.027402\n2019-01-08 00:00:00+00:00      0.035591  ...          0.026891\n2019-01-09 00:00:00+00:00      0.036049  ...          0.027089\n\n[5 rows x 8 columns]\n\n\nand visualizing them:\n\n\nCode\n# Preparar os dados no formato long para as variâncias condicionais\ncond_variances_long = cond_variances.reset_index().melt(id_vars='Date', var_name='Commodity', value_name='Conditional Variance')\n\n# Função para criar o gráfico com fundo branco ou transparente e ajustar o tamanho da figura\ndef plot_cond_variances_with_ggplot(data, title, ylabel, background='white', fig_height=10, fig_width=10):\n    # Cria o gráfico usando plotnine (ggplot)\n    p = (ggplot(data, aes(x='Date', y='Conditional Variance', color='Commodity')) +\n         geom_line() +\n         facet_wrap('~Commodity', ncol=1, scales='free_y') +  # Um gráfico em cima do outro\n         labs(title=title, x='Date', y=ylabel) +\n         theme_minimal() +  # Define o tema minimalista com fundo branco\n         theme(\n             figure_size=(fig_width, fig_height),  # Ajuste da altura e largura da figura\n             panel_background=element_text(fill=background),\n             plot_background=element_text(fill=background),\n             axis_text_x=element_text(rotation=45, hjust=1),\n             subplots_adjust={'wspace': 0.25, 'hspace': 0.5}  # Ajuste do espaçamento entre os gráficos\n         ))\n    return p\n\n# Exemplo de uso para as variâncias condicionais das commodities\np_cond_variances = plot_cond_variances_with_ggplot(cond_variances_long, 'Conditional Variances Over Time (GARCH(1,1))', 'Conditional Variance', background='white', fig_height=12, fig_width=8)\n\np_cond_variances\n\n\n&lt;string&gt;:2: FutureWarning: Using repr(plot) to draw and show the plot figure is deprecated and will be removed in a future version. Use plot.show().\nC:\\Users\\Rodrigo\\AppData\\Local\\Programs\\Python\\PYTHON~1\\Lib\\site-packages\\plotnine\\themes\\themeable.py:2419: FutureWarning: You no longer need to use subplots_adjust to make space for the legend or text around the panels. This paramater will be removed in a future version. You can still use 'plot_margin' 'panel_spacing' for your other spacing needs.\n&lt;Figure Size: (800 x 1200)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR codes\n\nR packagesPortfolio set\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n#library(plotly)\nlibrary(rugarch)\nlibrary(timeSeries)\nlibrary(fPortfolio)\nlibrary(quantmod)\nlibrary(caTools)\nlibrary(PerformanceAnalytics)\nlibrary(MASS)\nlibrary(PortfolioAnalytics)\nlibrary(ROI)\nrequire(ROI.plugin.glpk)\nrequire(ROI.plugin.quadprog)\nlibrary(quadprog)\nlibrary(corpcor)\nlibrary(DEoptim)\nlibrary(cowplot) # devtools::install_github(\"wilkelab/cowplot/\")\nlibrary(lattice)\nlibrary(timetk)\n\n\n\n\nLoading time series data, for portfolio setting…\n\n\nCode\ntickers &lt;- c(\n         \"ZC=F\", # Corn Futures\n         \"ZO=F\", # Wheat Futures\n         \"KE=F\", # Futuros KC HRW Wheat Futures\n         \"ZR=F\", # Rough Rice Futures\n         \"GF=F\", # Feeder Cattle Futures\n         \"ZS=F\", # SoyMeal Futures \n         \"ZM=F\", # Futuros farelo soja\n         \"ZL=F\"  # SoyBeans Futures\n)\n\n\nObtain daily prices and their returns:\n\n\nCode\nportfolioPrices &lt;- NULL\n  for ( Ticker in tickers )\n    portfolioPrices &lt;- cbind(\n      portfolioPrices, \n      getSymbols.yahoo(\n        Ticker,\n        from = \"2019-01-01\",\n        auto.assign = FALSE\n      )[,4]\n    )\n\nportfolioPrices &lt;- portfolioPrices[apply(portfolioPrices, 1, function(x) all(!is.na(x))),]\n\ncolnames(portfolioPrices) &lt;- c(\n  \"corn_fut\",\n  \"wheat_fut\",\n  \"KCWheat_fut\",\n  \"rice_fut\",\n  \"Feeder_Cattle\",\n  \"soymeal_fut\",\n  \"soyF_fut\",\n  \"soybeans_fut\"\n)\n\ntail(portfolioPrices)\n\n\n           corn_fut wheat_fut KCWheat_fut rice_fut Feeder_Cattle soymeal_fut\n2024-10-18   404.75    382.50      580.75   1500.0       248.325      970.00\n2024-10-21   409.50    379.50      582.25   1509.0       247.575      981.00\n2024-10-22   416.50    383.00      586.50   1512.5       249.425      991.75\n2024-10-23   419.00    380.50      585.50   1510.5       248.575      997.50\n2024-10-24   421.50    376.75      587.00   1504.0       249.400      996.25\n2024-10-25   415.25    380.25      572.00   1508.0       249.625      987.75\n           soyF_fut soybeans_fut\n2024-10-18    315.6        41.82\n2024-10-21    318.3        42.39\n2024-10-22    317.7        43.69\n2024-10-23    315.0        43.39\n2024-10-24    310.4        44.33\n2024-10-25    305.8        44.15\n\n\nPlotting the time series prices (in level):\n\n\nCode\nportfolioPrices |&gt; as.data.frame() |&gt;\n  mutate(\n    time = seq_along( corn_fut )\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\nObtain the returns time series (first feature):\n\\[\n\\mbox{Price log returns}_t = ln(p_t) - ln(p_{t-1})\n\\]\n\n\nCode\n# Calculate log returns for the portfolio prices\nportfolioReturs &lt;- na.omit(diff(log(portfolioPrices))) |&gt; as.data.frame()\n\ncolnames(portfolioReturs) &lt;- c(\n  \"ret_corn_fut\",\n  \"ret_wheat_fut\",\n  \"ret_KCWheat_fut\",\n  \"ret_rice_fut\",\n  \"ret_Feeder_Cattle\",\n  \"ret_soymeal_fut\",\n  \"ret_soyF_fut\",\n  \"ret_soybeans_fut\"\n)\n\nglimpse(portfolioReturs)\n\n\nRows: 1,464\nColumns: 8\n$ ret_corn_fut      &lt;dbl&gt; 0.0105891128, 0.0085218477, -0.0019601444, -0.005903…\n$ ret_wheat_fut     &lt;dbl&gt; 0.0008980692, 0.0053715438, -0.0017873106, 0.0115608…\n$ ret_KCWheat_fut   &lt;dbl&gt; 0.0220892515, 0.0049529571, -0.0059464992, 0.0039682…\n$ ret_rice_fut      &lt;dbl&gt; 0.0083930387, 0.0053934919, 0.0174507579, 0.00813596…\n$ ret_Feeder_Cattle &lt;dbl&gt; -0.0096783375, -0.0111522135, 0.0075628145, 0.011068…\n$ ret_soymeal_fut   &lt;dbl&gt; 0.0061281529, 0.0102224954, 0.0030190774, -0.0065988…\n$ ret_soyF_fut      &lt;dbl&gt; 0.0054513913, 0.0076457646, 0.0097900861, -0.0018874…\n$ ret_soybeans_fut  &lt;dbl&gt; 0.0099858421, 0.0081286732, -0.0052938051, -0.002834…\n\n\nCode\n#portfolioReturs &lt;- as.timeSeries(portfolioReturs)\n\n\nPlot all time series and their returns:\n\n\nCode\nportfolioReturs |&gt; \n  mutate(\n    time = seq_along( ret_corn_fut )\n  ) |&gt;\n  pivot_longer(\n    !time,\n    names_to = \"Variables\",\n    values_to = \"Value\"  \n      ) |&gt;\n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\nPlotting the histograms:\n\n\nCode\nportfolioPrices_df &lt;- as_tibble(portfolioPrices, rownames = \"date\")\nportfolioPrices_df$date &lt;- ymd(portfolioPrices_df$date)\n\nportfolioReturs_df &lt;- na.omit( ROC( portfolioPrices ), type = \"discrete\" ) |&gt;\n  as_tibble(rownames = \"date\")\nportfolioReturs_df$date &lt;- ymd(portfolioReturs_df$date)\ncolnames(portfolioReturs_df) &lt;- c(\n  \"date\",\n  \"ret_corn_fut\",\n  \"ret_wheat_fut\",\n  \"ret_KCWheat_fut\",\n  \"ret_rice_fut\",\n  \"ret_Feeder_Cattle\",\n  \"ret_soymeal_fut\",\n  \"ret_soyF_fut\",\n  \"ret_soybeans_fut\"\n)\n\n# Remover a coluna com nome NA\nportfolioReturs_df &lt;- portfolioReturs_df[, !is.na(colnames(portfolioReturs_df))]\n\n# Verificar novamente os nomes das colunas para garantir que estão corretos\ncolnames(portfolioReturs_df)\n\n\n[1] \"date\"              \"ret_corn_fut\"      \"ret_wheat_fut\"    \n[4] \"ret_KCWheat_fut\"   \"ret_rice_fut\"      \"ret_Feeder_Cattle\"\n[7] \"ret_soymeal_fut\"   \"ret_soyF_fut\"      \"ret_soybeans_fut\" \n\n\nCode\nportfolioReturs_long &lt;- portfolioReturs_df |&gt; \n  pivot_longer(\n    cols = -date, # Exclui a coluna de data\n    names_to = \"fut_type\", \n    values_to = \"returns\"\n  )\n\nggplot(portfolioReturs_long, aes(x = returns)) + \n  geom_histogram(aes(y = ..density..), binwidth = .01, color = \"black\", fill = \"white\") +\n  geom_density(alpha = .2, fill=\"lightgray\") +\n  theme_minimal() +\n  theme(\n    axis.line  = element_line(colour = \"black\"),\n    axis.text  = element_text(colour = \"black\"),  \n    axis.ticks = element_line(colour = \"black\"), \n    legend.position = c(.1,.9), \n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank()\n  ) +\n  theme(plot.title   = element_text(size = 10),  \n        axis.title.x = element_text(size = 7), \n        axis.title.y = element_text(size = 7)) + \n  labs(x = \"Returns\", y = \"Density\") +\n  facet_wrap(~fut_type, scales = \"free\", ncol = 2) \n\n\n\n\n\n\n\n\n\nAnd finnaly, the last feature, is called, the conditional variance (risk measure), obtained by GARCH(1,1) model, formalized as:\nThe GARCH(1,1) model with asymmetric Student-t distribution can be represented mathematically as:\n\\[\nr_t = \\mu + \\epsilon_t\n\\]\n\\[\n\\epsilon_t = \\sigma_t z_t, \\quad z_t \\sim t_{\\nu}(0, 1)\n\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\]\nWhere:\n\n\\(r_t\\) is the return at time \\(t\\).\n\\(\\mu\\) is the mean of the returns.\n\\(\\epsilon_t\\) is the error term, modeled as conditional on past information.\n\\(\\sigma_t^2\\) is the conditional variance at time \\(t\\).\n\\(\\omega, \\alpha, \\beta\\) are the parameters to be estimated, with \\(\\omega &gt; 0, \\alpha \\geq 0, \\beta \\geq 0\\).\n\\(z_t\\) follows an asymmetric Student-t distribution with \\(\\nu\\) degrees of freedom to better capture the heavy tails and skewness observed in financial returns.\n\n\n\nCode\n# Load necessary packages\nlibrary(rugarch)\n\n# Define the GARCH(1,1) model specification with Student-t distribution\nspec &lt;- ugarchspec(\n  variance.model = list(model = \"sGARCH\", garchOrder = c(1, 1)),\n  mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),\n  distribution.model = \"std\" # Using Student-t distribution\n)\n\n# Estimate the model for each asset in the portfolio and extract conditional variances\ngarch_models &lt;- list()\nconditional_variances &lt;- list()\n\nfor (i in colnames(portfolioReturs)) {\n  garch_models[[i]] &lt;- ugarchfit(spec, data = portfolioReturs[[i]])\n  conditional_variances[[i]] &lt;- sigma(garch_models[[i]])^2\n}\n\n# Convert conditional variances list to a data frame\nconditional_variances_df &lt;- do.call(cbind, conditional_variances) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(time = seq_along(conditional_variances[[1]]))\n\ncolnames(conditional_variances_df) &lt;- c(\n  \"cond_var_corn_fut\",\n  \"cond_var_wheat_fut\",\n  \"cond_var_KCWheat_fut\",\n  \"cond_var_rice_fut\",\n  \"cond_var_Feeder_Cattle\",\n  \"cond_var_soymeal_fut\",\n  \"cond_var_soyF_fut\",\n  \"cond_var_soybeans_fut\",\n  \"time\"\n)\n\n# Reshape data for plotting\nconditional_variances_long &lt;- conditional_variances_df %&gt;%\n  pivot_longer(!time, names_to = \"Variables\", values_to = \"Value\")\n\n\nAnd the plot of the conditional variance (risk):\n\n\nCode\nconditional_variances_long |&gt; \n  group_by(Variables) |&gt;\n  plot_time_series(\n    time,\n    Value,\n    .interactive = F, # Change for TRUE for better visualization\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) +\n  theme(\n    strip.background = element_rect(fill = \"white\", colour = \"white\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n\n\n\nReferences\nGujarati, D., N. (2004) Basic Econometrics, fourth edition, The McGraw−Hill Companies\nHair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2019). Multivariate Data Analysis. Pearson.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on oct 2023.\n \n \n\n\n\nCode\n# Total timing to compile this Quarto document\n\nend_time = datetime.now()\ntime_diff = end_time - start_time\n\nprint(f\"Total Quarto document compiling time: {time_diff}\")\n\n\nTotal Quarto document compiling time: 0:01:01.145377",
    "crumbs": [
      "About",
      "Predictive Models",
      "Data Extraction for preselected commodities portfolio"
    ]
  },
  {
    "objectID": "projects.html#project-1",
    "href": "projects.html#project-1",
    "title": "Projects",
    "section": "Project 1",
    "text": "Project 1\n\nOur first Colab Notebook  here\n\n This project is related with the step News and Impact on price and volatilities dynamics"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "Multiobjective Optimization",
    "section": "",
    "text": "… to be included…",
    "crumbs": [
      "About",
      "MultiObjective Multiperiod Optimization"
    ]
  },
  {
    "objectID": "mindmap.html",
    "href": "mindmap.html",
    "title": "The Research Mindmap",
    "section": "",
    "text": "Overview\nThe following flowchart illustrates the research workflow, detailing the key methodologies and how they are interconnected. It starts from the problem identification and moves through volatility modeling, predictive modeling, and optimization processes.\n\n\n\n\n\n\n\n\n\n\n\nResearch Proposal: Advanced Techniques for Multiperiod Multiobjective Portfolio Optimization in Commodity Markets\nThis research addresses the significant challenge of modeling and forecasting agricultural commodity prices, which are subject to high volatility and complex dynamics. Agricultural markets are highly sensitive to external factors such as climatic changes, geopolitical events, and supply-demand imbalances, making accurate forecasting and risk management difficult for investors and policymakers.\n\nKey Components of the Research:\n\nVolatility Modeling Using GAMLSS and MSGARCH:\n\nGAMLSS: This technique provides a nuanced understanding of the distributional characteristics of commodity returns, capturing the probabilistic behaviors that traditional models often overlook.\nMSGARCH: By implementing the Markov-Switching GARCH model, the research captures regime shifts in volatility, which are common in commodities due to external shocks and systemic changes.\n\nMulti-Objective Portfolio Optimization:\n\nThis step involves developing a multi-objective optimization framework using evolutionary algorithms such as NSGA-II and Differential Evolution (DEOptim). These algorithms help optimize portfolio allocation by balancing risk, return, and diversification, particularly for portfolios with high-volatility assets like agricultural commodities.\n\nReinforcement Learning for Portfolio Management:\n\nThe research also introduces Reinforcement Learning (RL) methods, such as Q-Learning and K-Bandit algorithms, to adaptively manage portfolio strategies. These techniques are particularly suited for dynamic portfolio management, allowing strategies to evolve as market conditions change.\n\n\n\n\nContribution to Knowledge:\nThe research’s innovative contribution lies in combining these advanced econometric and machine learning techniques to tackle the unique challenges of commodity markets. It offers a comprehensive methodological framework that improves the modeling and forecasting of volatility and returns in agricultural commodities. This work enhances portfolio optimization strategies, offering practical applications for financial markets by providing tools that help portfolio managers make informed, data-driven decisions in the face of volatile market conditions.\nFinal Objective: The primary goal of this research is to develop robust methods for volatility modeling and portfolio optimization that dynamically adapt to market conditions. This approach offers a significant advancement in both academic and professional fields by providing actionable insights for managing portfolios in volatile commodity markets.",
    "crumbs": [
      "About",
      "Intro",
      "The Research Mindmap"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About the PIBIC and PIBITI Jr.",
    "section": "",
    "text": "About the PIBIC and PIBITI Jr.\nThis project is carried out as part of a research initiation (PIBIC and PIBITI Jr.) at PUCPR, focusing on advanced forecasting and optimization techniques applied to agriculture.\n\nRodrigoRicardoGabrielly\n\n\n  \n\nAdvisor/Researcher\n\nEconomist and Msc. in Economic Development (UFPR, 2008 & 2010), PhD Candidate at PPGEPS/PUCPR (2022-2026)",
    "crumbs": [
      "About",
      "Intro",
      "About the PIBIC and PIBITI Jr."
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "acknowlegment.html",
    "href": "acknowlegment.html",
    "title": "Acknowlegment",
    "section": "",
    "text": "To the professor Gilberto Reynoso-Meza for all the help needed to evaluate at this research subject.",
    "crumbs": [
      "About",
      "Acknowlegment"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "AgriPredict",
    "section": "",
    "text": "This research project is being conducted at PUCPR with the support of undergraduate students participating in the Scientific Initiation Program (PIBIC Jr.). The primary aim is to explore advanced predictive models and multi-objective optimization approaches applied to agricultural commodities, particularly focusing on grains.\nThe research combines the efforts of faculty members and students, promoting an enriching learning experience while addressing real-world agricultural challenges. By leveraging time series models for price forecasting and optimization techniques, this project aims to provide insights that support decision-making processes in the agricultural sector.\nThrough the collaboration with our undergraduate students, we focus on developing practical skills and research capabilities, empowering the next generation of researchers and professionals in agricultural analytics and data-driven decision-making.",
    "crumbs": [
      "About",
      "Intro"
    ]
  },
  {
    "objectID": "introduction.html#learn-more",
    "href": "introduction.html#learn-more",
    "title": "AgriPredict",
    "section": "Learn More",
    "text": "Learn More\nFor more information about the Scientific Initiation Program, visit the PUCPR Scientific Initiation Program.",
    "crumbs": [
      "About",
      "Intro"
    ]
  },
  {
    "objectID": "news.html#analyzing-the-causal-impact-of-news-on-commodity-prices-returns-and-volatility",
    "href": "news.html#analyzing-the-causal-impact-of-news-on-commodity-prices-returns-and-volatility",
    "title": "News and Impact on price and volatilities dynamics",
    "section": "Analyzing the Causal Impact of News on Commodity Prices, Returns, and Volatility",
    "text": "Analyzing the Causal Impact of News on Commodity Prices, Returns, and Volatility\nThe hypothesis that abrupt changes in prices, returns, and volatility in agricultural commodity markets are driven by emerging market news is supported by a robust body of literature. One of the foundational works in this area is Robert Engle’s introduction of the News Impact Curve (Engle, 1993), which demonstrates how news can asymmetrically affect volatility in financial markets. Engle’s work shows that negative news tends to increase volatility more than positive news, creating a nonlinear relationship between news and market behavior. This framework suggests that sudden shifts in market conditions may be attributed to external news shocks that alter expectations and investor sentiment, thereby influencing price dynamics and return volatility.\nIn the context of agricultural commodities, this phenomenon is even more pronounced due to the sensitivity of these markets to exogenous shocks such as geopolitical developments, climatic events, and policy changes. A similar argument was made in the work of Ozon (2008), who applied volatility forecasting models to the Brazilian agricultural market. His research, which utilized data-driven models for volatility prediction, highlights the importance of incorporating external information, such as market news, into predictive models to capture the impact of unforeseen market shifts. Ozon’s findings suggest that neglecting the influence of market news can lead to significant forecasting errors, particularly in markets characterized by high volatility and abrupt price changes. His work is documented in the project repository at this link.\n\nObjective of Causal Analysis Using Market News and Search Trends\nBuilding on the insights provided by Engle and Ozon, this research aims to investigate the causal relationship between news events, search trends, and market volatility in agricultural commodity markets. Our hypothesis is that news events, particularly those that are not easily quantifiable through traditional numerical data, have a measurable and statistically significant effect on commodity prices and returns. Additionally, we will explore how search trends (e.g., Google Trends data) for specific keywords related to commodities reflect real-time market sentiment and drive price changes.\nThis study will utilize causal inference techniques to test whether certain news events are not only correlated with but are causally linked to market volatility. The innovative contribution of this research lies in combining news data with large language models (LLMs), such as GPT, to automatically identify and rank the most impactful news events. These LLMs will help in filtering vast amounts of information, pinpointing the events most likely to influence market movements.\n\n\nMethodological Approach\n\nData Collection: We will collect time-series data on commodity prices, returns, and volatility from various financial databases. Additionally, news articles and reports related to agricultural commodities will be gathered from reputable financial news sources.\nGoogle Trends: We will use Google Trends data to track search frequencies for specific keywords related to the commodities under study. The hypothesis here is that increased search activity correlates with heightened market volatility, as more people become aware of emerging news events.\nNews Filtering Using LLMs: LLMs, such as GPT, will be employed to filter news articles and highlight those with the highest potential to influence market dynamics. This automated news filtering process allows us to focus on key events that are likely to cause market disruptions.\nCausal Impact Testing: Once we have identified key news events, we will conduct causal impact tests using methodologies such as Granger causality tests, Bayesian structural time series (BSTS) or maybe the most recent CausalImpact algorithm (Brodersen et. al., 2015), models. These tests will allow us to determine whether a specific news event had a statistically significant impact on commodity prices or volatility.\nEvaluation: Finally, we will evaluate the overall effectiveness of our news-based predictive models by comparing them against baseline models that do not account for news events or search trends.\n\n\n\nExpected Contributions\nThe innovative contribution of this research is twofold. First, it introduces a novel framework that links market sentiment, captured through news data and search trends, to real-time volatility and price forecasting. Second, by incorporating LLMs for news filtering, the research provides a more scalable and efficient method of analyzing vast amounts of textual data, which is crucial in fast-moving markets.\nThis research aims to provide both theoretical insights and practical tools for market participants, enabling them to anticipate market shifts based on news events and search trends, and adjust their portfolio strategies accordingly.\n \n \n\n\n\nReferences\n\nBrodersen, K. H., Gallusser, F., Koehler, J., Remy, N., & Scott, S. L. (2015). Inferring causal impact using Bayesian structural time-series models. Annals of Applied Statistics, 9, 247–274.\nEngle, R. F. (1993). Statistical Models for Financial Volatility. Financial Analysts Journal.\nOzon, R. H. (2008). Volatility Forecasting in Agricultural Markets. Available at this link.",
    "crumbs": [
      "About",
      "Commodities (grains) news"
    ]
  },
  {
    "objectID": "predictive_model.html",
    "href": "predictive_model.html",
    "title": "Predictive Model",
    "section": "",
    "text": "Just for testing …",
    "crumbs": [
      "About",
      "Predictive Models"
    ]
  },
  {
    "objectID": "revsyslit.html",
    "href": "revsyslit.html",
    "title": "Literature Review and Scientific Challenge",
    "section": "",
    "text": "Code\nstart_time &lt;- Sys.time()",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-sentiment-analysis-and-commodity-prices",
    "href": "revsyslit.html#keyword-sentiment-analysis-and-commodity-prices",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Sentiment Analysis AND Commodity Prices",
    "text": "Keyword: “Sentiment Analysis AND Commodity Prices\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Sentiment Analysis AND Commodity Prices\"\nresults &lt;- cr_works(query = \"Sentiment Analysis AND Commodity Prices\", limit = 1000)$data # Max is 1000 searches\n\n\ncat(\"Show all features avaiable in selected keywords... \\n\")\n\n\nShow all features avaiable in selected keywords... \n\n\nCode\n# Exibir os resultados da busca\nglimpse(results)\n\n\nRows: 1,000\nColumns: 40\n$ alternative.id         &lt;chr&gt; \"10.1002/9781119603849.ch1,10.1002/978111960384…\n$ archive                &lt;chr&gt; \"Portico\", NA, NA, NA, NA, NA, \"Portico\", NA, N…\n$ container.title        &lt;chr&gt; \"Advanced Positioning, Flow, and Sentiment Anal…\n$ created                &lt;chr&gt; \"2019-12-20\", \"2009-07-03\", \"2010-06-25\", \"2013…\n$ deposited              &lt;chr&gt; \"2023-08-17\", \"2021-09-03\", \"2021-09-03\", \"2022…\n$ published.print        &lt;chr&gt; \"2019-11-18\", NA, NA, \"2013-08-26\", NA, NA, \"20…\n$ published.online       &lt;chr&gt; \"2019-12-19\", NA, NA, NA, NA, NA, \"2019-12-19\",…\n$ doi                    &lt;chr&gt; \"10.1002/9781119603849.ch1\", \"10.1787/656681831…\n$ indexed                &lt;chr&gt; \"2024-05-11\", \"2022-04-01\", \"2022-03-29\", \"2022…\n$ isbn                   &lt;chr&gt; \"9781119603825,9781119603849\", NA, NA, NA, NA, …\n$ issued                 &lt;chr&gt; \"2019-11-18\", NA, NA, \"2013-08-26\", NA, NA, \"20…\n$ member                 &lt;chr&gt; \"311\", \"1963\", \"1963\", \"2026\", \"1963\", \"1963\", …\n$ page                   &lt;chr&gt; \"7-18\", NA, NA, \"2038-2040\", NA, NA, \"163-176\",…\n$ prefix                 &lt;chr&gt; \"10.1002\", \"10.1787\", \"10.1787\", \"10.3724\", \"10…\n$ publisher              &lt;chr&gt; \"Wiley\", \"Organisation for Economic Co-Operatio…\n$ score                  &lt;chr&gt; \"29.128933\", \"29.09139\", \"28.30468\", \"28.26208\"…\n$ source                 &lt;chr&gt; \"Crossref\", \"Crossref\", \"Crossref\", \"Crossref\",…\n$ reference.count        &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ references.count       &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"2\", \"0…\n$ title                  &lt;chr&gt; \"Advanced Positioning, Flow, and Sentiment Anal…\n$ type                   &lt;chr&gt; \"other\", \"component\", \"component\", \"journal-art…\n$ url                    &lt;chr&gt; \"http://dx.doi.org/10.1002/9781119603849.ch1\", …\n$ language               &lt;chr&gt; \"en\", NA, NA, \"en\", NA, NA, \"en\", NA, NA, \"en\",…\n$ link                   &lt;list&gt; [&lt;tbl_df[3 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[1 …\n$ license                &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NU…\n$ issn                   &lt;chr&gt; NA, NA, NA, \"1001-9081\", NA, NA, NA, NA, NA, NA…\n$ issue                  &lt;chr&gt; NA, NA, NA, \"7\", NA, NA, NA, NA, NA, NA, NA, NA…\n$ subtitle               &lt;chr&gt; NA, NA, NA, \"Text sentiment analysis-oriented c…\n$ volume                 &lt;chr&gt; NA, NA, NA, \"32\", NA, NA, NA, NA, NA, NA, NA, N…\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Computer Applications\",…\n$ author                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], &lt;NU…\n$ reference              &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ abstract               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ update.policy          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ assertion              &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ funder                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ archive_               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ na.                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ na..1                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nFirst_Keyword &lt;- results |&gt; \n  arrange( desc(score), desc(is.referenced.by.count) ) |&gt; \n  distinct() |&gt; \n  filter(title != \"Front Matter\", \n         title != \"Cover\",\n         title != \"Books Received\")\n\nFirst_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-volatility-models-and-agricultural-markets",
    "href": "revsyslit.html#keyword-volatility-models-and-agricultural-markets",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Volatility Models AND Agricultural Markets”",
    "text": "Keyword: “Volatility Models AND Agricultural Markets”\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Volatility Models AND Agricultural Markets\"\nresults &lt;- cr_works(query = \"Volatility Models AND Agricultural Markets\", limit = 1000)$data # Max is 1000 searches\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nSecond_Keyword &lt;- results |&gt; arrange( desc(score), desc(is.referenced.by.count) ) |&gt; distinct()\n\nSecond_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-structural-breaks-and-agricultural-prices",
    "href": "revsyslit.html#keyword-structural-breaks-and-agricultural-prices",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Structural Breaks AND Agricultural Prices”",
    "text": "Keyword: “Structural Breaks AND Agricultural Prices”\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Structural Breaks AND Agricultural Prices\"\nresults &lt;- cr_works(query = \"Structural Breaks AND Agricultural Prices\", limit = 1000)$data # Max is 1000 searches\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nThird_Keyword &lt;- results |&gt; \n  arrange( desc(score), desc(is.referenced.by.count) ) |&gt; distinct()\n\nThird_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#final-dataframe",
    "href": "revsyslit.html#final-dataframe",
    "title": "Literature Review and Scientific Challenge",
    "section": "Final dataframe",
    "text": "Final dataframe\n\n\nCode\nFinal_df &lt;- bind_rows(\n  First_Keyword,\n  Second_Keyword,\n  Third_Keyword\n) |&gt; \n  arrange( desc(title), desc(score) ) |&gt; \n  distinct()\n\n\nglimpse(Final_df)\n\n\nRows: 388\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 2]&gt;], [&lt;tbl_df[3 x 3]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"dataset\", \"dataset\", \"journal-article…\n$ url                    &lt;chr&gt; \"http://dx.doi.org/10.1108/oxan-ga261626\", \"htt…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"CRAN: Contributed …\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Commodity Markets\", NA,…\n$ publisher              &lt;chr&gt; \"Emerald\", \"The R Foundation\", \"The R Foundatio…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.32614/cran.package…\n$ published.print        &lt;mth&gt; 2021 mai, 2020 set, 2023 jan, 2023 dez, 2023 ju…\n$ score                  &lt;dbl&gt; 19.97818, 18.75415, 17.08935, 19.97048, 16.1196…\n$ reference.count        &lt;chr&gt; \"0\", \"1\", \"1\", \"47\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[2 …\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[1 x 3]&gt;], …\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", NA, NA, \"2405-8513\", \"1999-1142\", …\n\n\n\n\nCode\nFinal_df_unique &lt;- Final_df |&gt;\n  distinct(title, doi, .keep_all = TRUE)\n\nglimpse(Final_df_unique)\n\n\nRows: 384\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 2]&gt;], [&lt;tbl_df[3 x 3]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"dataset\", \"dataset\", \"journal-article…\n$ url                    &lt;chr&gt; \"http://dx.doi.org/10.1108/oxan-ga261626\", \"htt…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"CRAN: Contributed …\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Commodity Markets\", NA,…\n$ publisher              &lt;chr&gt; \"Emerald\", \"The R Foundation\", \"The R Foundatio…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.32614/cran.package…\n$ published.print        &lt;mth&gt; 2021 mai, 2020 set, 2023 jan, 2023 dez, 2023 ju…\n$ score                  &lt;dbl&gt; 19.97818, 18.75415, 17.08935, 19.97048, 16.1196…\n$ reference.count        &lt;chr&gt; \"0\", \"1\", \"1\", \"47\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"1\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[2 …\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[1 x 3]&gt;], …\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", NA, NA, \"2405-8513\", \"1999-1142\", …\n\n\nCode\nFinal_df_unique\n\n\n\n  \n\n\n\nBy expanding the author names columns:\n\n\nCode\n# Unnest the author column\nFinal_df_unique_expanded &lt;- Final_df_unique |&gt;\n  unnest(author) |&gt;\n  select(title, given, family)\n\n# Rename the author name columns\nFinal_df_unique_expanded &lt;- Final_df_unique_expanded |&gt;\n  rename(Author_First_Name = given, Author_Last_Name = family)\n\n# Print the expanded data frame\nprint(Final_df_unique_expanded)\n\n\n# A tibble: 611 × 3\n   title                                      Author_First_Name Author_Last_Name\n   &lt;chr&gt;                                      &lt;chr&gt;             &lt;chr&gt;           \n 1 stochvolTMB: Likelihood Estimation of Sto… &lt;NA&gt;              &lt;NA&gt;            \n 2 mbreaks: Estimation and Inference for Str… Linh              Nguyen          \n 3 mbreaks: Estimation and Inference for Str… Yohei             Yamamoto        \n 4 mbreaks: Estimation and Inference for Str… Pierre            Perron          \n 5 World regional natural gas prices: Conver… Jose Roberto      Loureiro        \n 6 World regional natural gas prices: Conver… Julian            Inchauspe       \n 7 World regional natural gas prices: Conver… Roberto F.        Aguilera        \n 8 World commodity prices and partial defaul… Manoj             Atolia          \n 9 World commodity prices and partial defaul… Shuang            Feng            \n10 Who should buy stocks when volatility spi… Andrés            Schneider       \n# ℹ 601 more rows\n\n\nNow inserting the authors name in a new df\n\n\nCode\nauthors_df &lt;- Final_df_unique_expanded |&gt;\n  group_by(title) |&gt;\n  summarise(Author_Names = paste(Author_First_Name, Author_Last_Name, collapse = \"; \")) |&gt;\n  ungroup()\n\nprint(authors_df)\n\n\n# A tibble: 270 × 2\n   title                                                            Author_Names\n   &lt;chr&gt;                                                            &lt;chr&gt;       \n 1 A Behavioral Approach to Pricing in Commodity Markets: Dual Pro… Florian Dos…\n 2 A CEEMD-ARIMA-SVM model with structural breaks to forecast the … Yuxiang Che…\n 3 A Commodity Review Sentiment Analysis Based on BERT-CNN Model    Junchao Don…\n 4 A Study of Asymmetric Volatilities in Korean Stock Markets Usin… Eunhee Lee  \n 5 A Theoretical Framework for Reconceiving Agricultural Markets    Anthony Pah…\n 6 A double mixture autoregressive model of commodity prices        Gilbert Mba…\n 7 A note on institutional hierarchy and volatility in financial m… S. Alfarano…\n 8 A quantile autoregression analysis of price volatility in agric… Jean‐Paul C…\n 9 ARE AGRICULTURAL COMMODITY PRICES AFFECTED BY COVID-19? A STRUC… Katarzyna C…\n10 ASV: Stochastic Volatility Models with or without Leverage       Yasuhiro Om…\n# ℹ 260 more rows\n\n\nAuthor(s)\n\n\nCode\ndf_author &lt;- Final_df_unique |&gt;\n  select(title,\n         author) |&gt;\n  unnest(cols = author)\n\nglimpse(df_author)\n\n\nRows: 611\nColumns: 11\n$ title               &lt;chr&gt; \"stochvolTMB: Likelihood Estimation of Stochastic …\n$ name                &lt;chr&gt; \"Jens Christian Wahl &lt;jens.c.wahl@gmail.com&gt;\", NA,…\n$ sequence            &lt;chr&gt; \"first\", \"first\", \"additional\", \"additional\", \"fir…\n$ given               &lt;chr&gt; NA, \"Linh\", \"Yohei\", \"Pierre\", \"Jose Roberto\", \"Ju…\n$ family              &lt;chr&gt; NA, \"Nguyen\", \"Yamamoto\", \"Perron\", \"Loureiro\", \"I…\n$ ORCID               &lt;chr&gt; NA, NA, NA, NA, \"http://orcid.org/0000-0003-4643-1…\n$ authenticated.orcid &lt;lgl&gt; NA, NA, NA, NA, FALSE, NA, NA, FALSE, NA, NA, NA, …\n$ affiliation.name    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bank for …\n$ affiliation1.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ affiliation2.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ affiliation3.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nCode\nprint(df_author)\n\n\n# A tibble: 611 × 11\n   title  name  sequence given family ORCID authenticated.orcid affiliation.name\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;lgl&gt;               &lt;chr&gt;           \n 1 stoch… Jens… first    &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;  NA                  &lt;NA&gt;            \n 2 mbrea… &lt;NA&gt;  first    Linh  Nguyen &lt;NA&gt;  NA                  &lt;NA&gt;            \n 3 mbrea… &lt;NA&gt;  additio… Yohei Yamam… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 4 mbrea… &lt;NA&gt;  additio… Pier… Perron &lt;NA&gt;  NA                  &lt;NA&gt;            \n 5 World… &lt;NA&gt;  first    Jose… Loure… http… FALSE               &lt;NA&gt;            \n 6 World… &lt;NA&gt;  additio… Juli… Incha… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 7 World… &lt;NA&gt;  additio… Robe… Aguil… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 8 World… &lt;NA&gt;  first    Manoj Atolia http… FALSE               &lt;NA&gt;            \n 9 World… &lt;NA&gt;  additio… Shua… Feng   &lt;NA&gt;  NA                  &lt;NA&gt;            \n10 Who s… &lt;NA&gt;  first    Andr… Schne… &lt;NA&gt;  NA                  &lt;NA&gt;            \n# ℹ 601 more rows\n# ℹ 3 more variables: affiliation1.name &lt;chr&gt;, affiliation2.name &lt;chr&gt;,\n#   affiliation3.name &lt;chr&gt;\n\n\nLinks\n\n\nCode\ndf_link &lt;- Final_df |&gt;\n  select(\n    title,\n    link\n  ) |&gt;\n  unnest(\n    cols = link\n  )\n\nglimpse(df_link)\n\n\nReferences\n\n\nCode\ndf_references &lt;- Final_df |&gt;\n  select(\n    title,\n    reference\n  ) |&gt;\n  unnest(\n    cols = reference\n  )\n\nglimpse(df_references)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#n-grams",
    "href": "revsyslit.html#n-grams",
    "title": "Literature Review and Scientific Challenge",
    "section": "N-grams",
    "text": "N-grams\nInstead of analyzing individual words, N-gram analysis examines sequences of two or more words that frequently appear together (bi-grams, tri-grams).\nObjective: To identify more relevant phrases or compound terms in the literature, which may not be captured in the analysis of individual words.\nApplication: To discover compound terms like “volatility spillover,” “agricultural prices,” or “portfolio optimization” that frequently appear.\n\n\nCode\n# Filtrar apenas artigos com abstracts disponíveis\nFinal_df_filtered &lt;- Final_df_unique |&gt;\n  filter(!is.na(abstract))  # Remove os artigos com abstracts 'NA'\n\n# Verificar o resultado\nglimpse(Final_df_filtered)\n\n\nRows: 78\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 4]&gt;], [&lt;tbl_df[2 x 4]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"journal-article\", \"journal-article\", …\n$ url                    &lt;chr&gt; \"http://dx.doi.org/10.1108/oxan-ga261626\", \"htt…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"Review of Finance\"…\n$ short.container.title  &lt;chr&gt; NA, NA, \"Agricultural Economics\", \"Journal of F…\n$ publisher              &lt;chr&gt; \"Emerald\", \"Oxford University Press (OUP)\", \"Wi…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.1093/rof/rfad038\",…\n$ published.print        &lt;mth&gt; 2021 mai, 2024 mai, 2021 mar, 2021 fev, 2020 ab…\n$ score                  &lt;dbl&gt; 19.97818, 18.46325, 16.16151, 18.01863, 19.2565…\n$ reference.count        &lt;chr&gt; \"0\", \"62\", \"41\", \"49\", \"66\", \"64\", \"36\", \"45\", …\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"6\", \"9\", \"20\", \"5\", \"3\", \"16\", \"0\", …\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], [&lt;tbl_df[3 x 4]&gt;], [&lt;tbl_df…\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[62 x 9]&gt;], [&lt;tbl_df[41 x 12]&gt;…\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", \"1572-3097,1573-692X\", \"0169-5150,…\n\n\nThen we build corpus with only avaiable abstracts\n\n\nCode\n# Criar o corpus de texto a partir da coluna de resumos (abstracts) filtrados\ncorpus &lt;- Corpus(VectorSource(Final_df_filtered$abstract))\n\n# Visualizar parte do corpus\ninspect(corpus[1:3])\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] &lt;jats:p&gt;Demand will surge for the minerals for low-carbon technologies, but supply is concentrated and will lag, raising prices&lt;/jats:p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n[2] &lt;jats:title&gt;Abstract&lt;/jats:title&gt;\\n               &lt;jats:p&gt;This article studies exchange-traded funds’ (ETFs) price impact in the most ETF-dominated asset classes: volatility (VIX) and commodities. I propose a new way to measure ETF-related price distortions based on the specifics of futures contracts. This allows me to isolate a component in VIX futures prices that is strongly related to the rebalancing of ETFs. I derive a novel decomposition of ETF trading demand into leverage rebalancing, calendar rebalancing, and flow rebalancing, and show that trading against ETFs is risky. Leverage rebalancing has the largest effects on the ETF-related price component. This rebalancing amplifies price changes and exposes ETF counterparties to variance.&lt;/jats:p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n[3] &lt;jats:title&gt;Abstract&lt;/jats:title&gt;&lt;jats:p&gt;Teff is an ancient grain primarily produced in Ethiopia and providing more than 10% of the total calories consumed in the country. The grain is considered as “super grain” due to its nutritional qualities, and it has seen an increase in its demand and price in recent years. These trends raise public concerns about the affordability of the grain and the prevalence of food insecurity in Ethiopia. Therefore, we investigate the impacts of increasing teff prices on consumers’ welfare by regions. Using data from two waves of the Ethiopia Socioeconomic Survey 2013–2014 and 2015–2016, we examine the consumption patterns of cereals in Ethiopia and estimate a two‐stage structural demand system. We find that teff is the most own‐price inelastic grain in Ethiopia and an increase of 10% in teff prices will reduce consumer welfare by 0.81, 1.29, and 1.73 Birrs per week for the average rural, town, and urban consumers, respectively. These estimates correspond to 1.64, 2.31, and 2.46% of their weekly food budgets. We also find the negative effects of an increase in teff prices are smaller for the lower‐income groups as they have relatively lower expenditures on teff. Additionally, we analyze the effects of simultaneous changes in wheat and teff prices to measure the extent to which Ethiopia's food policy of distributing subsidized wheat could offset the consumer welfare loss due to an increase in teff prices.&lt;/jats:p&gt;\n\n\nclean corpus and apply the N-grams processing\n\n\nCode\n# Função para limpar o texto e remover termos indesejados\nclean_text &lt;- function(corpus){\n  # Remover tags HTML\n  corpus &lt;- tm_map(corpus, content_transformer(function(x) gsub(\"&lt;.*?&gt;\", \"\", x))) # Remove tags HTML\n  corpus &lt;- tm_map(corpus, content_transformer(tolower)) # Converter para minúsculas\n  corpus &lt;- tm_map(corpus, removePunctuation) # Remover pontuação\n  corpus &lt;- tm_map(corpus, removeNumbers) # Remover números\n  corpus &lt;- tm_map(corpus, removeWords, stopwords(\"english\")) # Remover stopwords em inglês\n  corpus &lt;- tm_map(corpus, stripWhitespace) # Remover espaços extras\n  \n  # Lista de termos indesejados a remover\n  termos_indesejados &lt;- c(\"jats\", \"title\", \"sec\", \"abstract\", \"type\", \"content\", \"of the\", \"in the\", \"and the\")\n  \n  # Remover termos indesejados\n  corpus &lt;- tm_map(corpus, removeWords, termos_indesejados)\n  \n  return(corpus)\n}\n# Limpar o corpus\ncorpus_clean &lt;- clean_text(corpus)\n\n# Criar um dataframe de texto limpo\nclean_text_df &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE)\n\n# Criar um tokenizador de bigramas com o corpus limpo\nbigram_data_clean &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE) |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# Contar a frequência dos bigramas\nbigram_filtered_clean &lt;- bigram_data_clean |&gt;\n  count(bigram, sort = TRUE)\n\n# Filtrar os bigramas que não estão vazios ou não são códigos\nbigram_filtered_clean &lt;- bigram_filtered_clean |&gt;\n  filter(!str_detect(bigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os bigramas mais frequentes\nhead(bigram_filtered_clean, 10)\n\n\n\n  \n\n\n\nNow we can plot bars with the most frequent bigrams:\n\n\nCode\n# Filtrar os 10 bigramas mais frequentes\ntop_bigrams_clean &lt;- bigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_bigrams_clean, aes(x = reorder(bigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Bigramas Mais Frequentes\", x = \"Bigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nWe can expand for three grams:\n\n\nCode\n# Tokenização em trigramas\ntrigram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(trigram, text, token = \"ngrams\", n = 3)\n\n# Contar a frequência dos trigramas\ntrigram_filtered_clean &lt;- trigram_data_clean |&gt;\n  count(trigram, sort = TRUE)\n\n# Filtrar os trigramas indesejados\ntrigram_filtered_clean &lt;- trigram_filtered_clean |&gt;\n  filter(!str_detect(trigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os trigramas mais frequentes\nhead(trigram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 trigramas mais frequentes\ntop_trigrams_clean &lt;- trigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_trigrams_clean, aes(x = reorder(trigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Trigramas Mais Frequentes\", x = \"Trigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nAnd quadrigrams:\n\n\nCode\n# Tokenização em quadrigramas\nquadrigram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(quadrigram, text, token = \"ngrams\", n = 4)\n\n# Contar a frequência dos quadrigramas\nquadrigram_filtered_clean &lt;- quadrigram_data_clean |&gt;\n  count(quadrigram, sort = TRUE)\n\n# Filtrar os quadrigram indesejados\nquadrigram_filtered_clean &lt;- quadrigram_filtered_clean |&gt;\n  filter(!str_detect(quadrigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os quadrigram mais frequentes\nhead(quadrigram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 quadrigramas mais frequentes\ntop_quadrigrams_clean &lt;- quadrigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_quadrigrams_clean, aes(x = reorder(quadrigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Quadrigramas Mais Frequentes\", x = \"Quadrigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nAnd finnaly the darkest pentagram:\n\n\nCode\n# Tokenização em pentagram\npentagram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(pentagram, text, token = \"ngrams\", n = 5)\n\n# Contar a frequência dos pentagram\npentagram_filtered_clean &lt;- pentagram_data_clean |&gt;\n  count(pentagram, sort = TRUE)\n\n# Filtrar os pentagram indesejados\npentagram_filtered_clean &lt;- pentagram_filtered_clean |&gt;\n  filter(!str_detect(pentagram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os quadrigram mais frequentes\nhead(pentagram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 pentagrams mais frequentes\ntop_pentagrams_clean &lt;- pentagram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_pentagrams_clean, aes(x = reorder(pentagram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Pentagramas Mais Frequentes\", x = \"Pentagramas\", y = \"Frequência\")",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#word-co-occurrence-analysis",
    "href": "revsyslit.html#word-co-occurrence-analysis",
    "title": "Literature Review and Scientific Challenge",
    "section": "Word Co-Occurrence Analysis",
    "text": "Word Co-Occurrence Analysis\nWhat it is: Examine which words tend to appear together in a text.\nObjective: Explore the relationships between different terms and how they are connected within a broader context.\nApplication: Identify patterns in the associations between important terms such as “volatility” and “crisis” or “agricultural prices” and “portfolio optimization.”\n\n\nCode\n# Carregar pacotes necessários\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tm)\n\n# Passo 1: Limpeza do Corpus (assumindo que o corpus já foi criado e limpo anteriormente)\n# Criar o dataframe com os textos limpos (usando o corpus_clean criado anteriormente)\ntext_df &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE)\n\n# Passo 2: Tokenização para Bigramas (palavras em pares)\nbigrams &lt;- text_df %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# Passo 3: Separar os bigramas em duas colunas para fazer a análise de co-ocorrência\nbigram_separated &lt;- bigrams %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\n# Passo 4: Contar as co-ocorrências de palavras\nbigram_count &lt;- bigram_separated %&gt;%\n  count(word1, word2, sort = TRUE)\n\n# Passo 5: Filtrar co-ocorrências que aparecem mais de uma vez (ou outro limite que faça sentido)\nbigram_filtered &lt;- bigram_count %&gt;%\n  filter(n &gt; 1)\n\n# Passo 6: Criar o grafo de co-ocorrência usando o pacote igraph\nword_network &lt;- bigram_filtered %&gt;%\n  graph_from_data_frame()\n\n# Passo 7: Plotar a rede de co-ocorrência usando o pacote ggraph\nset.seed(1234)  # Para reprodutibilidade\nplotly::ggplotly(ggraph(word_network, layout = \"fr\") +  # Layout da rede (fr = force-directed)\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +  # Conexões entre palavras\n  geom_node_point(color = \"lightblue\", size = 5) +  # Nós das palavras\n  geom_node_text(aes(label = name), vjust = 1.5, hjust = 1.5) +  # Textos das palavras\n  theme_void() +  # Remover fundo e eixos\n  labs(title = \"Word Co-occurrence Network\", subtitle = \"Bigram Co-occurrences\")\n)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#tdm-and-lda",
    "href": "revsyslit.html#tdm-and-lda",
    "title": "Literature Review and Scientific Challenge",
    "section": "TDM and LDA",
    "text": "TDM and LDA\nWhat it is: Extract underlying topics from texts by grouping related keywords.\nObjective: Automatically discover the main themes discussed in a large collection of articles. Application: Help identify different research areas within a broader field, for example, whether articles on “sentiment analysis” focus more on price volatility, market predictions, or risk analysis.\nTo continue with our Exploratory Lit Rev, we first need to build the corpus:\n\n\nCode\n# Criar um corpus de texto a partir da coluna de resumos (abstracts)\ncorpus &lt;- Corpus(VectorSource(Final_df_unique$abstract))\n\n# Visualizar parte do corpus\ninspect(corpus[1:3])\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] &lt;jats:p&gt;Demand will surge for the minerals for low-carbon technologies, but supply is concentrated and will lag, raising prices&lt;/jats:p&gt;\n[2] &lt;NA&gt;                                                                                                                                    \n[3] &lt;NA&gt;                                                                                                                                    \n\n\nThen we proceed for the next step, cleaning the text:\nNow we can build the terms matrix (Term-Document Matrix - TDM), were each row represents an term and each column represents an document.\n\n\nCode\n# Criar a matriz de termos\ntdm &lt;- TermDocumentMatrix(corpus_clean)\n\n# Converter para um dataframe\ntdm_matrix &lt;- as.matrix(tdm)\n\n# Ver os termos mais frequentes\nterm_freq &lt;- rowSums(tdm_matrix)\nterm_freq_sorted &lt;- sort(term_freq, decreasing = TRUE)\n\n# Visualizar as palavras mais frequentes\nhead(term_freq_sorted, 10)\n\n\n  volatility       prices        price        model      markets       models \n         195           91           90           82           76           68 \n       study       market        stock agricultural \n          61           57           56           48 \n\n\nConvert the matrix for an LDA format:\n\n\nCode\n# Criar a matriz de termos a partir do corpus limpo\ntdm &lt;- TermDocumentMatrix(corpus_clean)\n\n# Converter a matriz de termos para um formato compatível com o LDA\ntdm_sparse &lt;- as.matrix(tdm)\n\n# Verifique se a ordem dos termos é correta\nterms &lt;- Terms(tdm)  # Extraia os termos da TDM\n\n\nOne wordcloud is an good view to see the most frequent terms:\n\n\nCode\n# Criar uma nuvem de palavras\nwordcloud(words = names(term_freq_sorted), freq = term_freq_sorted, min.freq = 5,\n          max.words=100, random.order=FALSE, colors=brewer.pal(8, \"Dark2\"))\n\n\n\n\n\n\n\n\n\nIf we needs an more quant analisys, we can generate an bar graph with the most frequent words:\n\n\nCode\n# Converter para dataframe\ndf_term_freq &lt;- data.frame(term = names(term_freq_sorted), freq = term_freq_sorted)\n\n# Filtrar as 10 palavras mais frequentes\ntop_terms &lt;- df_term_freq |&gt; top_n(10, freq)\n\n# Criar gráfico de barras\nggplot(top_terms, aes(x = reorder(term, freq), y = freq)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Frequent Terms\", x = \"Terms\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nTo indentify the principal topics inside the papers, we can use Latent Dirichlet Allocation (LDA).\n\n\nCode\n# Definir o número de tópicos\nnum_topics &lt;- 3\n\n# Rodar o modelo LDA\nlda_model &lt;- LDA(tdm_sparse, k = num_topics, control = list(seed = 1234))\n\n# Extrair os tópicos e garantir que os termos sejam corretamente mapeados\ntopics &lt;- tidy(lda_model, matrix = \"beta\")\n\n# Conecte os IDs de termos reais\ntopics &lt;- topics |&gt;\n  mutate(term = terms[as.numeric(term)])  # Substituir os índices pelos termos reais\n\ntopics\n\n\n\n  \n\n\n\nSeeing the most representatives terms by topic:\n\n\nCode\n# Mostrar as palavras mais importantes para cada tópico\ntop_terms_per_topic &lt;- topics |&gt;\n  group_by(topic) |&gt;\n  top_n(10, beta) |&gt;\n  ungroup() |&gt;\n  arrange(topic, -beta)\n\n# Visualização corrigida com os termos reais\nlibrary(ggplot2)\nlibrary(forcats)\n\ntop_terms_per_topic |&gt;\n  mutate(term = fct_reorder(term, beta)) |&gt;\n  ggplot(aes(term, beta, fill = as.factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  labs(title = \"Top terms in each topic\", x = \"Terms\", y = \"Beta (Importance)\")\n\n\n\n\n\n\n\n\n\nThe graph generated is a topic modeling visualization based on an LDA (Latent Dirichlet Allocation) model. Each bar shows the most important words in each identified topic, derived from the text data provided. Let’s break down each part:\nInterpreting the Axes:\n\nX-axis (“Beta Importance”): The “beta” value indicates the probability that the word is relevant to the specific topic. The higher the beta value, the more important the word is in describing that topic.\nY-axis (Terms): Shows the terms (words) that are most representative within each topic.\n\nInterpretation by Topic:\nTopic 1 (red bar)\n\nThe most relevant terms include: “novel”, “exposes”, “way”, “contracts”, “volatility”, “surge”, “specifics”, “birrs”, “calendar”, “consumer.”\nThese terms suggest a set of documents focused on new contracts or methods related to volatility and consumption. The term “volatility” indicates that this topic likely discusses market fluctuations or innovations in managing contracts and consumption.\n\nTopic 2 (green bar)\n\nMost relevant terms: “contracts”, “propose”, “studies”, “etfs”, “correspond”, “lag”, “measure”, “new”, “consumption”, “based.”\nThis topic seems to be related to studies proposing the use of ETFs (Exchange-Traded Funds) and financial contracts, with a focus on consumption measures and market response lags. It could be discussing proposals for new studies on contracts and ETFs related to different consumption patterns.\n\nTopic 3 (blue bar)\n\nMost relevant terms: “consumed”, “propose”, “trading”, “impact”, “component”, “consumers”, “country”, “affordability”, “risky”, “amplifies.”\nThis topic is clearly related to consumption, trading, and impact across different regions or countries. The terms “affordability” and “risky” suggest that this topic might be discussing market accessibility and associated risks, perhaps in the context of trade policies.\n\nConclusion:\nThe graph highlights the most relevant words within each of the three topics, which seem to be:\n\nInnovations and contracts in the context of volatility and consumption.\nProposals and studies on ETFs and financial contracts, with a focus on consumption and performance measures.\nTrade and consumption, focusing on the impact on countries, market affordability, and potential associated risks.\n\nEach topic provides insight into different areas of study within the context of the analyzed documents. If you’re interested in a deeper analysis, you can explore how these topics relate to specific articles and contexts they cover.",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#exploratory-literature-review-with-text-mining-final-considerations",
    "href": "revsyslit.html#exploratory-literature-review-with-text-mining-final-considerations",
    "title": "Literature Review and Scientific Challenge",
    "section": "Exploratory Literature Review with Text Mining: Final Considerations",
    "text": "Exploratory Literature Review with Text Mining: Final Considerations\nIn this section, we applied text mining techniques to explore the main themes and trends in the literature related to sentiment analysis, volatility models, structural breaks, and multiobjective portfolio optimization in agricultural markets. The Latent Dirichlet Allocation (LDA) method was used to extract latent topics from the abstracts of papers, providing insights into the key terms that characterize the body of research.\n\nKey Insights from Topic Modeling\nThree topics were identified through LDA, each represented by a set of significant terms with their respective importance (Beta values):\n\nTopic 1 primarily emphasizes GARCH models, including terms like “bekk-garch” and “dcc-garch,” as well as terms related to market quality, volatility spillovers (“vov”), and settlement effects. This topic highlights a focus on econometric models used to assess volatility and risk in financial markets, with applications to agricultural commodities.\nTopic 2 features terms such as “return,” “option-impli,” and “long,” indicating that this topic revolves around the long-term returns and option pricing in agricultural markets. Other terms like “find” and “pcs” suggest explorations of statistical methods used to identify patterns and model market behavior.\nTopic 3 focuses on spillovers and global market trends, with terms like “spillov,” “precis,” “global,” and “detect.” This topic suggests an emphasis on crisis detection, market interdependence, and the global nature of agricultural commodities. The presence of “india” and “japan” also suggests the regional analysis of agricultural markets.\n\n\n\nImplications for Future Research\nThe results of this exploratory text mining analysis indicate that the literature in the field of agricultural market volatility, sentiment analysis, and portfolio optimization is deeply rooted in econometric modeling, risk assessment, and global market interconnections. Moving forward, researchers may want to delve further into the integration of sentiment metrics with econometric models to enhance predictive capabilities, especially in the context of volatility and structural breaks.\nBy leveraging these insights, future work could focus on developing multiobjective optimization strategies that incorporate both financial metrics and qualitative sentiment data, offering a more holistic approach to managing risk and improving investment decisions in agricultural markets.",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#frank-fabozzi",
    "href": "revsyslit.html#frank-fabozzi",
    "title": "Literature Review and Scientific Challenge",
    "section": "Frank Fabozzi",
    "text": "Frank Fabozzi\nScholar link https://scholar.google.com/citations?user=tqXS4IMAAAAJ&hl=en\n\n\nCode\n## Define the id for Frank Fabozzi\nid &lt;- 'tqXS4IMAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Frank J Fabozzi\"\n\n\nCode\nl$affiliation\n\n\n[1] \"EDHEC Business School\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 89\n\n\nCode\nl$i10_index\n\n\n[1] 465\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[6])\n\n\n[1] \"Project financing\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[6])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and David Ardia\nids &lt;- c('tqXS4IMAAAAJ', 'BPNrOUYAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'BPNrOUYAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nget_profile('BPNrOUYAAAAJ')$name\n\n\n[1] \"David Ardia\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('BPNrOUYAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#duan-li",
    "href": "revsyslit.html#duan-li",
    "title": "Literature Review and Scientific Challenge",
    "section": "Duan Li",
    "text": "Duan Li\nScholar link https://scholar.google.com/citations?user=e0IkYKcAAAAJ&hl=en\n\n\nCode\n## Define the id for Duan Li\nid &lt;- 'e0IkYKcAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Duan Li\"\n\n\nCode\nl$affiliation\n\n\n[1] \"City University of Hong Kong + The Chinese University of Hong Kong + University of Virginia\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 54\n\n\nCode\nl$i10_index\n\n\n[1] 187\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"Optimal dynamic portfolio selection: Multiperiod mean‐variance formulation\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Duan Li\nids &lt;- c('tqXS4IMAAAAJ', 'e0IkYKcAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'e0IkYKcAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"Duan Li\"\n\n\nCode\nget_profile('e0IkYKcAAAAJ')$name\n\n\n[1] \"Duan Li\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('e0IkYKcAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#woo-chang-kim",
    "href": "revsyslit.html#woo-chang-kim",
    "title": "Literature Review and Scientific Challenge",
    "section": "Woo Chang Kim",
    "text": "Woo Chang Kim\nScholar link https://scholar.google.com/citations?user=e0IkYKcAAAAJ&hl=en\n\n\nCode\n## Define the id for Woo Chang Kim\nid &lt;- '7NmBs1kAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\nl$affiliation\n\n\n[1] \"Professor, Industrial and Systems Engineering, KAIST\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 18\n\n\nCode\nl$i10_index\n\n\n[1] 35\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"Dynamic asset allocation for varied financial markets under regime switching framework\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Woo Chang Kim\nids &lt;- c('tqXS4IMAAAAJ', '7NmBs1kAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- '7NmBs1kAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\nget_profile('7NmBs1kAAAAJ')$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('7NmBs1kAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#david-ardia",
    "href": "revsyslit.html#david-ardia",
    "title": "Literature Review and Scientific Challenge",
    "section": "David Ardia",
    "text": "David Ardia\nScholar link https://scholar.google.com/citations?hl=en&user=BPNrOUYAAAAJ\n\n\nCode\n## Define the id for David Ardia\nid &lt;- 'BPNrOUYAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nl$affiliation\n\n\n[1] \"HEC Montréal & GERAD\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 25\n\n\nCode\nl$i10_index\n\n\n[1] 40\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"DEoptim: An R package for global optimization by differential evolution\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and David Ardia\nids &lt;- c('tqXS4IMAAAAJ', 'BPNrOUYAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'BPNrOUYAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nget_profile('BPNrOUYAAAAJ')$name\n\n\n[1] \"David Ardia\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('BPNrOUYAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#herman-koene-van-dijk",
    "href": "revsyslit.html#herman-koene-van-dijk",
    "title": "Literature Review and Scientific Challenge",
    "section": "Herman Koene Van Dijk",
    "text": "Herman Koene Van Dijk\nScholar link https://scholar.google.com/citations?user=8y5_FWQAAAAJ&hl=en\n\n\nCode\n## Define the id for Herman Koene Van Dijk\nid &lt;- 'FWQAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\nl$affiliation\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\nl$i10_index\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Herman Van Dijk\nids &lt;- c('tqXS4IMAAAAJ', 'FWQAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'FWQAAAAJ&hl'\nget_profile(coautorias)$name\n\nget_profile('FWQAAAAJ')$name\n\n\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('FWQAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#bernhard-pfaff",
    "href": "revsyslit.html#bernhard-pfaff",
    "title": "Literature Review and Scientific Challenge",
    "section": "Bernhard Pfaff",
    "text": "Bernhard Pfaff\n\nGitHub https://github.com/bpfaff/\nScholar https://scholar.google.com.br/scholar?q=bernhard+pfaff&hl=pt-BR&as_sdt=0&as_vis=1&oi=scholart",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  }
]