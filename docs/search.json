[
  {
    "objectID": "revsyslit.html",
    "href": "revsyslit.html",
    "title": "Literature Review and Scientific Challenge",
    "section": "",
    "text": "Code\nstart_time &lt;- Sys.time()",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-sentiment-analysis-and-commodity-prices",
    "href": "revsyslit.html#keyword-sentiment-analysis-and-commodity-prices",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Sentiment Analysis AND Commodity Prices",
    "text": "Keyword: “Sentiment Analysis AND Commodity Prices\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Sentiment Analysis AND Commodity Prices\"\nresults &lt;- cr_works(query = \"Sentiment Analysis AND Commodity Prices\", limit = 1000)$data # Max is 1000 searches\n\n\ncat(\"Show all features avaiable in selected keywords... \\n\")\n\n\nShow all features avaiable in selected keywords... \n\n\nCode\n# Exibir os resultados da busca\nglimpse(results)\n\n\nRows: 1,000\nColumns: 40\n$ alternative.id         &lt;chr&gt; \"10.1002/9781119603849.ch1,10.1002/978111960384…\n$ archive                &lt;chr&gt; \"Portico\", NA, NA, NA, NA, NA, \"Portico\", NA, \"…\n$ container.title        &lt;chr&gt; \"Advanced Positioning, Flow, and Sentiment Anal…\n$ created                &lt;chr&gt; \"2019-12-20\", \"2009-07-03\", \"2013-08-26\", \"2010…\n$ deposited              &lt;chr&gt; \"2023-08-17\", \"2021-09-03\", \"2022-03-04\", \"2021…\n$ published.print        &lt;chr&gt; \"2019-11-18\", NA, \"2013-08-26\", NA, NA, NA, \"20…\n$ published.online       &lt;chr&gt; \"2019-12-19\", NA, NA, NA, NA, NA, \"2019-12-19\",…\n$ doi                    &lt;chr&gt; \"10.1002/9781119603849.ch1\", \"10.1787/656681831…\n$ indexed                &lt;chr&gt; \"2024-05-11\", \"2022-04-01\", \"2022-04-30\", \"2022…\n$ isbn                   &lt;chr&gt; \"9781119603825,9781119603849\", NA, NA, NA, NA, …\n$ issued                 &lt;chr&gt; \"2019-11-18\", NA, \"2013-08-26\", NA, NA, NA, \"20…\n$ member                 &lt;chr&gt; \"311\", \"1963\", \"2026\", \"1963\", \"1963\", \"1963\", …\n$ page                   &lt;chr&gt; \"7-18\", NA, \"2038-2040\", NA, NA, NA, \"163-176\",…\n$ prefix                 &lt;chr&gt; \"10.1002\", \"10.1787\", \"10.3724\", \"10.1787\", \"10…\n$ publisher              &lt;chr&gt; \"Wiley\", \"Organisation for Economic Co-Operatio…\n$ score                  &lt;chr&gt; \"29.01646\", \"28.903824\", \"28.149002\", \"28.12492…\n$ source                 &lt;chr&gt; \"Crossref\", \"Crossref\", \"Crossref\", \"Crossref\",…\n$ reference.count        &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ references.count       &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"2…\n$ title                  &lt;chr&gt; \"Advanced Positioning, Flow, and Sentiment Anal…\n$ type                   &lt;chr&gt; \"other\", \"component\", \"journal-article\", \"compo…\n$ url                    &lt;chr&gt; \"https://doi.org/10.1002/9781119603849.ch1\", \"h…\n$ language               &lt;chr&gt; \"en\", NA, \"en\", NA, NA, NA, \"en\", NA, \"en\", NA,…\n$ link                   &lt;list&gt; [&lt;tbl_df[3 x 4]&gt;], &lt;NULL&gt;, [&lt;tbl_df[1 x 4]&gt;], …\n$ license                &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NU…\n$ issn                   &lt;chr&gt; NA, NA, \"1001-9081\", NA, NA, NA, NA, NA, NA, NA…\n$ issue                  &lt;chr&gt; NA, NA, \"7\", NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ subtitle               &lt;chr&gt; NA, NA, \"Text sentiment analysis-oriented commo…\n$ volume                 &lt;chr&gt; NA, NA, \"32\", NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ short.container.title  &lt;chr&gt; NA, NA, \"Journal of Computer Applications\", NA,…\n$ author                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], &lt;NULL&gt;, &lt;NU…\n$ reference              &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ abstract               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ update.policy          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ assertion              &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ funder                 &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;…\n$ archive_               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ na.                    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ na..1                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nFirst_Keyword &lt;- results |&gt; \n  arrange( desc(score), desc(is.referenced.by.count) ) |&gt; \n  distinct() |&gt; \n  filter(title != \"Front Matter\", \n         title != \"Cover\",\n         title != \"Books Received\")\n\nFirst_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-volatility-models-and-agricultural-markets",
    "href": "revsyslit.html#keyword-volatility-models-and-agricultural-markets",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Volatility Models AND Agricultural Markets”",
    "text": "Keyword: “Volatility Models AND Agricultural Markets”\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Volatility Models AND Agricultural Markets\"\nresults &lt;- cr_works(query = \"Volatility Models AND Agricultural Markets\", limit = 1000)$data # Max is 1000 searches\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nSecond_Keyword &lt;- results |&gt; arrange( desc(score), desc(is.referenced.by.count) ) |&gt; distinct()\n\nSecond_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#keyword-structural-breaks-and-agricultural-prices",
    "href": "revsyslit.html#keyword-structural-breaks-and-agricultural-prices",
    "title": "Literature Review and Scientific Challenge",
    "section": "Keyword: “Structural Breaks AND Agricultural Prices”",
    "text": "Keyword: “Structural Breaks AND Agricultural Prices”\n\n\nCode\n# Realizar a busca por artigos contendo a palavra-chave \"Structural Breaks AND Agricultural Prices\"\nresults &lt;- cr_works(query = \"Structural Breaks AND Agricultural Prices\", limit = 1000)$data # Max is 1000 searches\n\n\nNow showing only selected columns for the table (dataframe) necessary:\n\n\nCode\nresults &lt;- results |&gt; \n  mutate(author_expand = map(author, ~as.character(.x))) |&gt; \n  unnest(author_expand) |&gt;\n  mutate(author_expand = gsub(\"c\\\\(\\\"|\\\"\\\\)\", \"\", author_expand))\n         \nresults &lt;- results |&gt; \n  mutate(link_expand = map(link, ~as.character(.x))) |&gt; \n  unnest(link_expand)\n\n\n\n\nCode\nresults &lt;- results |&gt; \n  select(\n    title,\n    author,\n    type,\n    url,\n    container.title,\n    short.container.title,\n    publisher,\n    doi,\n    published.print,\n    score,\n    reference.count,\n    is.referenced.by.count,\n    link,\n    reference,\n    abstract,\n    issn\n) |&gt;\n  mutate(\n    score = as.numeric(score),\n    published.print = coalesce(published.print, \"1900-01\"), # substitui NAs por \"1900-01\"\n    published.print = yearmonth( published.print )\n  ) |&gt;\n  filter(\n    year(published.print) &gt;= 2020\n  ) |&gt; \n  distinct()\n\n\nShowing the results in a table:\n\n\nCode\nThird_Keyword &lt;- results |&gt; \n  arrange( desc(score), desc(is.referenced.by.count) ) |&gt; distinct()\n\nThird_Keyword",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#final-dataframe",
    "href": "revsyslit.html#final-dataframe",
    "title": "Literature Review and Scientific Challenge",
    "section": "Final dataframe",
    "text": "Final dataframe\n\n\nCode\nFinal_df &lt;- bind_rows(\n  First_Keyword,\n  Second_Keyword,\n  Third_Keyword\n) |&gt; \n  arrange( desc(title), desc(score) ) |&gt; \n  distinct()\n\n\nglimpse(Final_df)\n\n\nRows: 406\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[3 x 3]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"dataset\", \"dataset\", \"journal-article…\n$ url                    &lt;chr&gt; \"https://doi.org/10.1108/oxan-ga261626\", \"https…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"CRAN: Contributed …\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Commodity Markets\", NA,…\n$ publisher              &lt;chr&gt; \"Emerald\", \"The R Foundation\", \"The R Foundatio…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.32614/cran.package…\n$ published.print        &lt;mth&gt; 2021 mai, 2020 set, 2023 jan, 2023 dez, 2023 ju…\n$ score                  &lt;dbl&gt; 19.83983, 18.75384, 17.06678, 19.84145, 16.0170…\n$ reference.count        &lt;chr&gt; \"0\", \"1\", \"1\", \"47\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[2 …\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[1 x 3]&gt;], …\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", NA, NA, \"2405-8513\", \"1999-1142\", …\n\n\n\n\nCode\nFinal_df_unique &lt;- Final_df |&gt;\n  distinct(title, doi, .keep_all = TRUE)\n\nglimpse(Final_df_unique)\n\n\nRows: 399\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[3 x 3]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"dataset\", \"dataset\", \"journal-article…\n$ url                    &lt;chr&gt; \"https://doi.org/10.1108/oxan-ga261626\", \"https…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"CRAN: Contributed …\n$ short.container.title  &lt;chr&gt; NA, NA, NA, \"Journal of Commodity Markets\", NA,…\n$ publisher              &lt;chr&gt; \"Emerald\", \"The R Foundation\", \"The R Foundatio…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.32614/cran.package…\n$ published.print        &lt;mth&gt; 2021 mai, 2020 set, 2023 jan, 2023 dez, 2023 ju…\n$ score                  &lt;dbl&gt; 19.83983, 18.75384, 17.06678, 19.84145, 16.0170…\n$ reference.count        &lt;chr&gt; \"0\", \"1\", \"1\", \"47\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"0\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], &lt;NULL&gt;, &lt;NULL&gt;, [&lt;tbl_df[2 …\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 3]&gt;], [&lt;tbl_df[1 x 3]&gt;], …\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", NA, NA, \"2405-8513\", \"1999-1142\", …\n\n\nCode\nFinal_df_unique\n\n\n\n  \n\n\n\nBy expanding the author names columns:\n\n\nCode\n# Unnest the author column\nFinal_df_unique_expanded &lt;- Final_df_unique |&gt;\n  unnest(author) |&gt;\n  select(title, given, family)\n\n# Rename the author name columns\nFinal_df_unique_expanded &lt;- Final_df_unique_expanded |&gt;\n  rename(Author_First_Name = given, Author_Last_Name = family)\n\n# Print the expanded data frame\nprint(Final_df_unique_expanded)\n\n\n# A tibble: 646 × 3\n   title                                      Author_First_Name Author_Last_Name\n   &lt;chr&gt;                                      &lt;chr&gt;             &lt;chr&gt;           \n 1 stochvolTMB: Likelihood Estimation of Sto… Jens              Wahl            \n 2 mbreaks: Estimation and Inference for Str… Linh              Nguyen          \n 3 mbreaks: Estimation and Inference for Str… Yohei             Yamamoto        \n 4 mbreaks: Estimation and Inference for Str… Pierre            Perron          \n 5 World regional natural gas prices: Conver… Jose Roberto      Loureiro        \n 6 World regional natural gas prices: Conver… Julian            Inchauspe       \n 7 World regional natural gas prices: Conver… Roberto F.        Aguilera        \n 8 World commodity prices and partial defaul… Manoj             Atolia          \n 9 World commodity prices and partial defaul… Shuang            Feng            \n10 Who should buy stocks when volatility spi… Andrés            Schneider       \n# ℹ 636 more rows\n\n\nNow inserting the authors name in a new df\n\n\nCode\nauthors_df &lt;- Final_df_unique_expanded |&gt;\n  group_by(title) |&gt;\n  summarise(Author_Names = paste(Author_First_Name, Author_Last_Name, collapse = \"; \")) |&gt;\n  ungroup()\n\nprint(authors_df)\n\n\n# A tibble: 283 × 2\n   title                                                            Author_Names\n   &lt;chr&gt;                                                            &lt;chr&gt;       \n 1 A Behavioral Approach to Pricing in Commodity Markets: Dual Pro… Florian Dos…\n 2 A CEEMD-ARIMA-SVM model with structural breaks to forecast the … Yuxiang Che…\n 3 A Commodity Review Sentiment Analysis Based on BERT-CNN Model    Junchao Don…\n 4 A Study of Asymmetric Volatilities in Korean Stock Markets Usin… Eunhee Lee  \n 5 A Theoretical Framework for Reconceiving Agricultural Markets    Anthony Pah…\n 6 A double mixture autoregressive model of commodity prices        Gilbert Mba…\n 7 A note on institutional hierarchy and volatility in financial m… S. Alfarano…\n 8 A quantile autoregression analysis of price volatility in agric… Jean‐Paul C…\n 9 ARE AGRICULTURAL COMMODITY PRICES AFFECTED BY COVID-19? A STRUC… Katarzyna C…\n10 ASV: Stochastic Volatility Models with or without Leverage       Yasuhiro Om…\n# ℹ 273 more rows\n\n\nAuthor(s)\n\n\nCode\ndf_author &lt;- Final_df_unique |&gt;\n  select(title,\n         author) |&gt;\n  unnest(cols = author)\n\nglimpse(df_author)\n\n\nRows: 646\nColumns: 11\n$ title               &lt;chr&gt; \"stochvolTMB: Likelihood Estimation of Stochastic …\n$ given               &lt;chr&gt; \"Jens\", \"Linh\", \"Yohei\", \"Pierre\", \"Jose Roberto\",…\n$ family              &lt;chr&gt; \"Wahl\", \"Nguyen\", \"Yamamoto\", \"Perron\", \"Loureiro\"…\n$ sequence            &lt;chr&gt; \"first\", \"first\", \"additional\", \"additional\", \"fir…\n$ ORCID               &lt;chr&gt; NA, NA, NA, NA, \"http://orcid.org/0000-0003-4643-1…\n$ authenticated.orcid &lt;lgl&gt; NA, NA, NA, NA, FALSE, NA, NA, FALSE, NA, NA, NA, …\n$ affiliation.name    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bank for …\n$ affiliation1.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ affiliation2.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ name                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ affiliation3.name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nCode\nprint(df_author)\n\n\n# A tibble: 646 × 11\n   title        given family sequence ORCID authenticated.orcid affiliation.name\n   &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;lgl&gt;               &lt;chr&gt;           \n 1 stochvolTMB… Jens  Wahl   first    &lt;NA&gt;  NA                  &lt;NA&gt;            \n 2 mbreaks: Es… Linh  Nguyen first    &lt;NA&gt;  NA                  &lt;NA&gt;            \n 3 mbreaks: Es… Yohei Yamam… additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 4 mbreaks: Es… Pier… Perron additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 5 World regio… Jose… Loure… first    http… FALSE               &lt;NA&gt;            \n 6 World regio… Juli… Incha… additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 7 World regio… Robe… Aguil… additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n 8 World commo… Manoj Atolia first    http… FALSE               &lt;NA&gt;            \n 9 World commo… Shua… Feng   additio… &lt;NA&gt;  NA                  &lt;NA&gt;            \n10 Who should … Andr… Schne… first    &lt;NA&gt;  NA                  &lt;NA&gt;            \n# ℹ 636 more rows\n# ℹ 4 more variables: affiliation1.name &lt;chr&gt;, affiliation2.name &lt;chr&gt;,\n#   name &lt;chr&gt;, affiliation3.name &lt;chr&gt;\n\n\nLinks\n\n\nCode\ndf_link &lt;- Final_df |&gt;\n  select(\n    title,\n    link\n  ) |&gt;\n  unnest(\n    cols = link\n  )\n\nglimpse(df_link)\n\n\nReferences\n\n\nCode\ndf_references &lt;- Final_df |&gt;\n  select(\n    title,\n    reference\n  ) |&gt;\n  unnest(\n    cols = reference\n  )\n\nglimpse(df_references)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#n-grams",
    "href": "revsyslit.html#n-grams",
    "title": "Literature Review and Scientific Challenge",
    "section": "N-grams",
    "text": "N-grams\nInstead of analyzing individual words, N-gram analysis examines sequences of two or more words that frequently appear together (bi-grams, tri-grams).\nObjective: To identify more relevant phrases or compound terms in the literature, which may not be captured in the analysis of individual words.\nApplication: To discover compound terms like “volatility spillover,” “agricultural prices,” or “portfolio optimization” that frequently appear.\n\n\nCode\n# Filtrar apenas artigos com abstracts disponíveis\nFinal_df_filtered &lt;- Final_df_unique |&gt;\n  filter(!is.na(abstract))  # Remove os artigos com abstracts 'NA'\n\n# Verificar o resultado\nglimpse(Final_df_filtered)\n\n\nRows: 84\nColumns: 16\n$ title                  &lt;chr&gt; \"‘Green’ tech will power industrial commodity p…\n$ author                 &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[1 x 4]&gt;], [&lt;tbl_df[2 x 4]&gt;], …\n$ type                   &lt;chr&gt; \"other\", \"journal-article\", \"journal-article\", …\n$ url                    &lt;chr&gt; \"https://doi.org/10.1108/oxan-ga261626\", \"https…\n$ container.title        &lt;chr&gt; \"Emerald Expert Briefings\", \"Review of Finance\"…\n$ short.container.title  &lt;chr&gt; NA, NA, \"Agricultural Economics\", \"Journal of F…\n$ publisher              &lt;chr&gt; \"Emerald\", \"Oxford University Press (OUP)\", \"Wi…\n$ doi                    &lt;chr&gt; \"10.1108/oxan-ga261626\", \"10.1093/rof/rfad038\",…\n$ published.print        &lt;mth&gt; 2021 mai, 2024 mai, 2021 mar, 2021 fev, 2020 ab…\n$ score                  &lt;dbl&gt; 19.83983, 18.33538, 16.04424, 18.07271, 19.3064…\n$ reference.count        &lt;chr&gt; \"0\", \"62\", \"41\", \"49\", \"66\", \"64\", \"86\", \"36\", …\n$ is.referenced.by.count &lt;chr&gt; \"0\", \"0\", \"6\", \"9\", \"20\", \"5\", \"0\", \"3\", \"16\", …\n$ link                   &lt;list&gt; [&lt;tbl_df[2 x 4]&gt;], [&lt;tbl_df[3 x 4]&gt;], [&lt;tbl_df…\n$ reference              &lt;list&gt; &lt;NULL&gt;, [&lt;tbl_df[62 x 9]&gt;], [&lt;tbl_df[41 x 12]&gt;…\n$ abstract               &lt;chr&gt; \"&lt;jats:p&gt;Demand will surge for the minerals for…\n$ issn                   &lt;chr&gt; \"2633-304X\", \"1572-3097,1573-692X\", \"0169-5150,…\n\n\nThen we build corpus with only avaiable abstracts\n\n\nCode\n# Criar o corpus de texto a partir da coluna de resumos (abstracts) filtrados\ncorpus &lt;- Corpus(VectorSource(Final_df_filtered$abstract))\n\n# Visualizar parte do corpus\ninspect(corpus[1:3])\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] &lt;jats:p&gt;Demand will surge for the minerals for low-carbon technologies, but supply is concentrated and will lag, raising prices&lt;/jats:p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n[2] &lt;jats:title&gt;Abstract&lt;/jats:title&gt;\\n               &lt;jats:p&gt;This article studies exchange-traded funds’ (ETFs) price impact in the most ETF-dominated asset classes: volatility (VIX) and commodities. I propose a new way to measure ETF-related price distortions based on the specifics of futures contracts. This allows me to isolate a component in VIX futures prices that is strongly related to the rebalancing of ETFs. I derive a novel decomposition of ETF trading demand into leverage rebalancing, calendar rebalancing, and flow rebalancing, and show that trading against ETFs is risky. Leverage rebalancing has the largest effects on the ETF-related price component. This rebalancing amplifies price changes and exposes ETF counterparties to variance.&lt;/jats:p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n[3] &lt;jats:title&gt;Abstract&lt;/jats:title&gt;&lt;jats:p&gt;Teff is an ancient grain primarily produced in Ethiopia and providing more than 10% of the total calories consumed in the country. The grain is considered as “super grain” due to its nutritional qualities, and it has seen an increase in its demand and price in recent years. These trends raise public concerns about the affordability of the grain and the prevalence of food insecurity in Ethiopia. Therefore, we investigate the impacts of increasing teff prices on consumers’ welfare by regions. Using data from two waves of the Ethiopia Socioeconomic Survey 2013–2014 and 2015–2016, we examine the consumption patterns of cereals in Ethiopia and estimate a two‐stage structural demand system. We find that teff is the most own‐price inelastic grain in Ethiopia and an increase of 10% in teff prices will reduce consumer welfare by 0.81, 1.29, and 1.73 Birrs per week for the average rural, town, and urban consumers, respectively. These estimates correspond to 1.64, 2.31, and 2.46% of their weekly food budgets. We also find the negative effects of an increase in teff prices are smaller for the lower‐income groups as they have relatively lower expenditures on teff. Additionally, we analyze the effects of simultaneous changes in wheat and teff prices to measure the extent to which Ethiopia's food policy of distributing subsidized wheat could offset the consumer welfare loss due to an increase in teff prices.&lt;/jats:p&gt;\n\n\nclean corpus and apply the N-grams processing\n\n\nCode\n# Função para limpar o texto e remover termos indesejados\nclean_text &lt;- function(corpus){\n  # Remover tags HTML\n  corpus &lt;- tm_map(corpus, content_transformer(function(x) gsub(\"&lt;.*?&gt;\", \"\", x))) # Remove tags HTML\n  corpus &lt;- tm_map(corpus, content_transformer(tolower)) # Converter para minúsculas\n  corpus &lt;- tm_map(corpus, removePunctuation) # Remover pontuação\n  corpus &lt;- tm_map(corpus, removeNumbers) # Remover números\n  corpus &lt;- tm_map(corpus, removeWords, stopwords(\"english\")) # Remover stopwords em inglês\n  corpus &lt;- tm_map(corpus, stripWhitespace) # Remover espaços extras\n  \n  # Lista de termos indesejados a remover\n  termos_indesejados &lt;- c(\"jats\", \"title\", \"sec\", \"abstract\", \"type\", \"content\", \"of the\", \"in the\", \"and the\")\n  \n  # Remover termos indesejados\n  corpus &lt;- tm_map(corpus, removeWords, termos_indesejados)\n  \n  return(corpus)\n}\n# Limpar o corpus\ncorpus_clean &lt;- clean_text(corpus)\n\n# Criar um dataframe de texto limpo\nclean_text_df &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE)\n\n# Criar um tokenizador de bigramas com o corpus limpo\nbigram_data_clean &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE) |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# Contar a frequência dos bigramas\nbigram_filtered_clean &lt;- bigram_data_clean |&gt;\n  count(bigram, sort = TRUE)\n\n# Filtrar os bigramas que não estão vazios ou não são códigos\nbigram_filtered_clean &lt;- bigram_filtered_clean |&gt;\n  filter(!str_detect(bigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os bigramas mais frequentes\nhead(bigram_filtered_clean, 10)\n\n\n\n  \n\n\n\nNow we can plot bars with the most frequent bigrams:\n\n\nCode\n# Filtrar os 10 bigramas mais frequentes\ntop_bigrams_clean &lt;- bigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_bigrams_clean, aes(x = reorder(bigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Bigramas Mais Frequentes\", x = \"Bigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nWe can expand for three grams:\n\n\nCode\n# Tokenização em trigramas\ntrigram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(trigram, text, token = \"ngrams\", n = 3)\n\n# Contar a frequência dos trigramas\ntrigram_filtered_clean &lt;- trigram_data_clean |&gt;\n  count(trigram, sort = TRUE)\n\n# Filtrar os trigramas indesejados\ntrigram_filtered_clean &lt;- trigram_filtered_clean |&gt;\n  filter(!str_detect(trigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os trigramas mais frequentes\nhead(trigram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 trigramas mais frequentes\ntop_trigrams_clean &lt;- trigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_trigrams_clean, aes(x = reorder(trigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Trigramas Mais Frequentes\", x = \"Trigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nAnd quadrigrams:\n\n\nCode\n# Tokenização em quadrigramas\nquadrigram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(quadrigram, text, token = \"ngrams\", n = 4)\n\n# Contar a frequência dos quadrigramas\nquadrigram_filtered_clean &lt;- quadrigram_data_clean |&gt;\n  count(quadrigram, sort = TRUE)\n\n# Filtrar os quadrigram indesejados\nquadrigram_filtered_clean &lt;- quadrigram_filtered_clean |&gt;\n  filter(!str_detect(quadrigram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os quadrigram mais frequentes\nhead(quadrigram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 quadrigramas mais frequentes\ntop_quadrigrams_clean &lt;- quadrigram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_quadrigrams_clean, aes(x = reorder(quadrigram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Quadrigramas Mais Frequentes\", x = \"Quadrigramas\", y = \"Frequência\")\n\n\n\n\n\n\n\n\n\nAnd finnaly the darkest pentagram:\n\n\nCode\n# Tokenização em pentagram\npentagram_data_clean &lt;- clean_text_df |&gt;\n  unnest_tokens(pentagram, text, token = \"ngrams\", n = 5)\n\n# Contar a frequência dos pentagram\npentagram_filtered_clean &lt;- pentagram_data_clean |&gt;\n  count(pentagram, sort = TRUE)\n\n# Filtrar os pentagram indesejados\npentagram_filtered_clean &lt;- pentagram_filtered_clean |&gt;\n  filter(!str_detect(pentagram, \"NA|jats|title|sec|abstract|type|content\"))\n\n# Visualizar os quadrigram mais frequentes\nhead(pentagram_filtered_clean, 10)\n\n\n\n  \n\n\n\n\n\nCode\n# Filtrar os 10 pentagrams mais frequentes\ntop_pentagrams_clean &lt;- pentagram_filtered_clean |&gt; top_n(10, n)\n\n# Gerar o gráfico de barras\nggplot(top_pentagrams_clean, aes(x = reorder(pentagram, n), y = n)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Pentagramas Mais Frequentes\", x = \"Pentagramas\", y = \"Frequência\")",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#word-co-occurrence-analysis",
    "href": "revsyslit.html#word-co-occurrence-analysis",
    "title": "Literature Review and Scientific Challenge",
    "section": "Word Co-Occurrence Analysis",
    "text": "Word Co-Occurrence Analysis\nWhat it is: Examine which words tend to appear together in a text.\nObjective: Explore the relationships between different terms and how they are connected within a broader context.\nApplication: Identify patterns in the associations between important terms such as “volatility” and “crisis” or “agricultural prices” and “portfolio optimization.”\n\n\nCode\n# Carregar pacotes necessários\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tm)\n\n# Passo 1: Limpeza do Corpus (assumindo que o corpus já foi criado e limpo anteriormente)\n# Criar o dataframe com os textos limpos (usando o corpus_clean criado anteriormente)\ntext_df &lt;- data.frame(text = sapply(corpus_clean, as.character), stringsAsFactors = FALSE)\n\n# Passo 2: Tokenização para Bigramas (palavras em pares)\nbigrams &lt;- text_df %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# Passo 3: Separar os bigramas em duas colunas para fazer a análise de co-ocorrência\nbigram_separated &lt;- bigrams %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\n# Passo 4: Contar as co-ocorrências de palavras\nbigram_count &lt;- bigram_separated %&gt;%\n  count(word1, word2, sort = TRUE)\n\n# Passo 5: Filtrar co-ocorrências que aparecem mais de uma vez (ou outro limite que faça sentido)\nbigram_filtered &lt;- bigram_count %&gt;%\n  filter(n &gt; 1)\n\n# Passo 6: Criar o grafo de co-ocorrência usando o pacote igraph\nword_network &lt;- bigram_filtered %&gt;%\n  graph_from_data_frame()\n\n# Passo 7: Plotar a rede de co-ocorrência usando o pacote ggraph\nset.seed(1234)  # Para reprodutibilidade\nplotly::ggplotly(ggraph(word_network, layout = \"fr\") +  # Layout da rede (fr = force-directed)\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE) +  # Conexões entre palavras\n  geom_node_point(color = \"lightblue\", size = 5) +  # Nós das palavras\n  geom_node_text(aes(label = name), vjust = 1.5, hjust = 1.5) +  # Textos das palavras\n  theme_void() +  # Remover fundo e eixos\n  labs(title = \"Word Co-occurrence Network\", subtitle = \"Bigram Co-occurrences\")\n)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#tdm-and-lda",
    "href": "revsyslit.html#tdm-and-lda",
    "title": "Literature Review and Scientific Challenge",
    "section": "TDM and LDA",
    "text": "TDM and LDA\nWhat it is: Extract underlying topics from texts by grouping related keywords.\nObjective: Automatically discover the main themes discussed in a large collection of articles. Application: Help identify different research areas within a broader field, for example, whether articles on “sentiment analysis” focus more on price volatility, market predictions, or risk analysis.\nTo continue with our Exploratory Lit Rev, we first need to build the corpus:\n\n\nCode\n# Criar um corpus de texto a partir da coluna de resumos (abstracts)\ncorpus &lt;- Corpus(VectorSource(Final_df_unique$abstract))\n\n# Visualizar parte do corpus\ninspect(corpus[1:3])\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 3\n\n[1] &lt;jats:p&gt;Demand will surge for the minerals for low-carbon technologies, but supply is concentrated and will lag, raising prices&lt;/jats:p&gt;\n[2] &lt;NA&gt;                                                                                                                                    \n[3] &lt;NA&gt;                                                                                                                                    \n\n\nThen we proceed for the next step, cleaning the text:\nNow we can build the terms matrix (Term-Document Matrix - TDM), were each row represents an term and each column represents an document.\n\n\nCode\n# Criar a matriz de termos\ntdm &lt;- TermDocumentMatrix(corpus_clean)\n\n# Converter para um dataframe\ntdm_matrix &lt;- as.matrix(tdm)\n\n# Ver os termos mais frequentes\nterm_freq &lt;- rowSums(tdm_matrix)\nterm_freq_sorted &lt;- sort(term_freq, decreasing = TRUE)\n\n# Visualizar as palavras mais frequentes\nhead(term_freq_sorted, 10)\n\n\n  volatility       prices        price      markets        model       models \n         215           91           91           91           88           77 \n       study        stock       market agricultural \n          64           62           59           50 \n\n\nConvert the matrix for an LDA format:\n\n\nCode\n# Criar a matriz de termos a partir do corpus limpo\ntdm &lt;- TermDocumentMatrix(corpus_clean)\n\n# Converter a matriz de termos para um formato compatível com o LDA\ntdm_sparse &lt;- as.matrix(tdm)\n\n# Verifique se a ordem dos termos é correta\nterms &lt;- Terms(tdm)  # Extraia os termos da TDM\n\n\nOne wordcloud is an good view to see the most frequent terms:\n\n\nCode\n# Criar uma nuvem de palavras\nwordcloud(words = names(term_freq_sorted), freq = term_freq_sorted, min.freq = 5,\n          max.words=100, random.order=FALSE, colors=brewer.pal(8, \"Dark2\"))\n\n\n\n\n\n\n\n\n\nIf we needs an more quant analisys, we can generate an bar graph with the most frequent words:\n\n\nCode\n# Converter para dataframe\ndf_term_freq &lt;- data.frame(term = names(term_freq_sorted), freq = term_freq_sorted)\n\n# Filtrar as 10 palavras mais frequentes\ntop_terms &lt;- df_term_freq |&gt; top_n(10, freq)\n\n# Criar gráfico de barras\nggplot(top_terms, aes(x = reorder(term, freq), y = freq)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Top 10 Frequent Terms\", x = \"Terms\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nTo indentify the principal topics inside the papers, we can use Latent Dirichlet Allocation (LDA).\n\n\nCode\n# Definir o número de tópicos\nnum_topics &lt;- 3\n\n# Rodar o modelo LDA\nlda_model &lt;- LDA(tdm_sparse, k = num_topics, control = list(seed = 1234))\n\n# Extrair os tópicos e garantir que os termos sejam corretamente mapeados\ntopics &lt;- tidy(lda_model, matrix = \"beta\")\n\n# Conecte os IDs de termos reais\ntopics &lt;- topics |&gt;\n  mutate(term = terms[as.numeric(term)])  # Substituir os índices pelos termos reais\n\ntopics\n\n\n\n  \n\n\n\nSeeing the most representatives terms by topic:\n\n\nCode\n# Mostrar as palavras mais importantes para cada tópico\ntop_terms_per_topic &lt;- topics |&gt;\n  group_by(topic) |&gt;\n  top_n(10, beta) |&gt;\n  ungroup() |&gt;\n  arrange(topic, -beta)\n\n# Visualização corrigida com os termos reais\nlibrary(ggplot2)\nlibrary(forcats)\n\ntop_terms_per_topic |&gt;\n  mutate(term = fct_reorder(term, beta)) |&gt;\n  ggplot(aes(term, beta, fill = as.factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  labs(title = \"Top terms in each topic\", x = \"Terms\", y = \"Beta (Importance)\")\n\n\n\n\n\n\n\n\n\nThe graph generated is a topic modeling visualization based on an LDA (Latent Dirichlet Allocation) model. Each bar shows the most important words in each identified topic, derived from the text data provided. Let’s break down each part:\nInterpreting the Axes:\n\nX-axis (“Beta Importance”): The “beta” value indicates the probability that the word is relevant to the specific topic. The higher the beta value, the more important the word is in describing that topic.\nY-axis (Terms): Shows the terms (words) that are most representative within each topic.\n\nInterpretation by Topic:\nTopic 1 (red bar)\n\nThe most relevant terms include: “novel”, “exposes”, “way”, “contracts”, “volatility”, “surge”, “specifics”, “birrs”, “calendar”, “consumer.”\nThese terms suggest a set of documents focused on new contracts or methods related to volatility and consumption. The term “volatility” indicates that this topic likely discusses market fluctuations or innovations in managing contracts and consumption.\n\nTopic 2 (green bar)\n\nMost relevant terms: “contracts”, “propose”, “studies”, “etfs”, “correspond”, “lag”, “measure”, “new”, “consumption”, “based.”\nThis topic seems to be related to studies proposing the use of ETFs (Exchange-Traded Funds) and financial contracts, with a focus on consumption measures and market response lags. It could be discussing proposals for new studies on contracts and ETFs related to different consumption patterns.\n\nTopic 3 (blue bar)\n\nMost relevant terms: “consumed”, “propose”, “trading”, “impact”, “component”, “consumers”, “country”, “affordability”, “risky”, “amplifies.”\nThis topic is clearly related to consumption, trading, and impact across different regions or countries. The terms “affordability” and “risky” suggest that this topic might be discussing market accessibility and associated risks, perhaps in the context of trade policies.\n\nConclusion:\nThe graph highlights the most relevant words within each of the three topics, which seem to be:\n\nInnovations and contracts in the context of volatility and consumption.\nProposals and studies on ETFs and financial contracts, with a focus on consumption and performance measures.\nTrade and consumption, focusing on the impact on countries, market affordability, and potential associated risks.\n\nEach topic provides insight into different areas of study within the context of the analyzed documents. If you’re interested in a deeper analysis, you can explore how these topics relate to specific articles and contexts they cover.",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#exploratory-literature-review-with-text-mining-final-considerations",
    "href": "revsyslit.html#exploratory-literature-review-with-text-mining-final-considerations",
    "title": "Literature Review and Scientific Challenge",
    "section": "Exploratory Literature Review with Text Mining: Final Considerations",
    "text": "Exploratory Literature Review with Text Mining: Final Considerations\nIn this section, we applied text mining techniques to explore the main themes and trends in the literature related to sentiment analysis, volatility models, structural breaks, and multiobjective portfolio optimization in agricultural markets. The Latent Dirichlet Allocation (LDA) method was used to extract latent topics from the abstracts of papers, providing insights into the key terms that characterize the body of research.\n\nKey Insights from Topic Modeling\nThree topics were identified through LDA, each represented by a set of significant terms with their respective importance (Beta values):\n\nTopic 1 primarily emphasizes GARCH models, including terms like “bekk-garch” and “dcc-garch,” as well as terms related to market quality, volatility spillovers (“vov”), and settlement effects. This topic highlights a focus on econometric models used to assess volatility and risk in financial markets, with applications to agricultural commodities.\nTopic 2 features terms such as “return,” “option-impli,” and “long,” indicating that this topic revolves around the long-term returns and option pricing in agricultural markets. Other terms like “find” and “pcs” suggest explorations of statistical methods used to identify patterns and model market behavior.\nTopic 3 focuses on spillovers and global market trends, with terms like “spillov,” “precis,” “global,” and “detect.” This topic suggests an emphasis on crisis detection, market interdependence, and the global nature of agricultural commodities. The presence of “india” and “japan” also suggests the regional analysis of agricultural markets.\n\n\n\nImplications for Future Research\nThe results of this exploratory text mining analysis indicate that the literature in the field of agricultural market volatility, sentiment analysis, and portfolio optimization is deeply rooted in econometric modeling, risk assessment, and global market interconnections. Moving forward, researchers may want to delve further into the integration of sentiment metrics with econometric models to enhance predictive capabilities, especially in the context of volatility and structural breaks.\nBy leveraging these insights, future work could focus on developing multiobjective optimization strategies that incorporate both financial metrics and qualitative sentiment data, offering a more holistic approach to managing risk and improving investment decisions in agricultural markets.",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#frank-fabozzi",
    "href": "revsyslit.html#frank-fabozzi",
    "title": "Literature Review and Scientific Challenge",
    "section": "Frank Fabozzi",
    "text": "Frank Fabozzi\nScholar link https://scholar.google.com/citations?user=tqXS4IMAAAAJ&hl=en\n\n\nCode\n## Define the id for Frank Fabozzi\nid &lt;- 'tqXS4IMAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Frank J Fabozzi\"\n\n\nCode\nl$affiliation\n\n\n[1] \"EDHEC Business School\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 90\n\n\nCode\nl$i10_index\n\n\n[1] 469\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[6])\n\n\n[1] \"Project financing\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[6])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and David Ardia\nids &lt;- c('tqXS4IMAAAAJ', 'BPNrOUYAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'BPNrOUYAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nget_profile('BPNrOUYAAAAJ')$name\n\n\n[1] \"David Ardia\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('BPNrOUYAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#duan-li",
    "href": "revsyslit.html#duan-li",
    "title": "Literature Review and Scientific Challenge",
    "section": "Duan Li",
    "text": "Duan Li\nScholar link https://scholar.google.com/citations?user=e0IkYKcAAAAJ&hl=en\n\n\nCode\n## Define the id for Duan Li\nid &lt;- 'e0IkYKcAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Duan Li\"\n\n\nCode\nl$affiliation\n\n\n[1] \"City University of Hong Kong + The Chinese University of Hong Kong + University of Virginia\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 55\n\n\nCode\nl$i10_index\n\n\n[1] 188\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"Optimal dynamic portfolio selection: Multiperiod mean‐variance formulation\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Duan Li\nids &lt;- c('tqXS4IMAAAAJ', 'e0IkYKcAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'e0IkYKcAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"Duan Li\"\n\n\nCode\nget_profile('e0IkYKcAAAAJ')$name\n\n\n[1] \"Duan Li\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('e0IkYKcAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#woo-chang-kim",
    "href": "revsyslit.html#woo-chang-kim",
    "title": "Literature Review and Scientific Challenge",
    "section": "Woo Chang Kim",
    "text": "Woo Chang Kim\nScholar link https://scholar.google.com/citations?user=e0IkYKcAAAAJ&hl=en\n\n\nCode\n## Define the id for Woo Chang Kim\nid &lt;- '7NmBs1kAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\nl$affiliation\n\n\n[1] \"Professor, Industrial and Systems Engineering, KAIST\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 20\n\n\nCode\nl$i10_index\n\n\n[1] 36\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"Dynamic asset allocation for varied financial markets under regime switching framework\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Woo Chang Kim\nids &lt;- c('tqXS4IMAAAAJ', '7NmBs1kAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- '7NmBs1kAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\nget_profile('7NmBs1kAAAAJ')$name\n\n\n[1] \"Woo Chang Kim\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('7NmBs1kAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#david-ardia",
    "href": "revsyslit.html#david-ardia",
    "title": "Literature Review and Scientific Challenge",
    "section": "David Ardia",
    "text": "David Ardia\nScholar link https://scholar.google.com/citations?hl=en&user=BPNrOUYAAAAJ\n\n\nCode\n## Define the id for David Ardia\nid &lt;- 'BPNrOUYAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nl$affiliation\n\n\n[1] \"HEC Montréal & GERAD\"\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\n\n[1] 25\n\n\nCode\nl$i10_index\n\n\n[1] 40\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\n\n  \n\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\n\n\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n[1] \"DEoptim: An R package for global optimization by differential evolution\"\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\n\n\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and David Ardia\nids &lt;- c('tqXS4IMAAAAJ', 'BPNrOUYAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\n\n\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'BPNrOUYAAAAJ&hl'\nget_profile(coautorias)$name\n\n\n[1] \"David Ardia\"\n\n\nCode\nget_profile('BPNrOUYAAAAJ')$name\n\n\n[1] \"David Ardia\"\n\n\nCode\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('BPNrOUYAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\n\n  \n\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#herman-koene-van-dijk",
    "href": "revsyslit.html#herman-koene-van-dijk",
    "title": "Literature Review and Scientific Challenge",
    "section": "Herman Koene Van Dijk",
    "text": "Herman Koene Van Dijk\nScholar link https://scholar.google.com/citations?user=8y5_FWQAAAAJ&hl=en\n\n\nCode\n## Define the id for Herman Koene Van Dijk\nid &lt;- 'FWQAAAAJ'\n\n## Get his profile\nl &lt;- get_profile(id)\n\n## Print his name and affliation\nl$name\n\nl$affiliation\n\n\n\n\nCode\n## Print his citation index\nl$h_index\n\nl$i10_index\n\n\nRetrieving publications\n\n\nCode\n## Get his publications (a large data frame)\np &lt;- get_publications(id)\np\n\n\nRetrieving citation data\n\n\nCode\n## Get his citation history, i.e. citations to his work in a given year\nct &lt;- get_citation_history(id)\n\n## Plot citation trend\nggplotly(\nggplot(ct, aes(year, cites)) + geom_line() + geom_point()\n)\n\n\nUsers can retrieve the citation history of a particular publication with get_article_cite_history().\n\n\nCode\n## The following publication will be used to demonstrate article citation history\nas.character(p$title[1])\n\n\n\n\nCode\n## Get article citation history\nach &lt;- get_article_cite_history(id, p$pubid[1])\n\n## Plot citation trend\nplotly::ggplotly(\nggplot(ach, aes(year, cites)) +\n    geom_segment(aes(xend = year, yend = 0), size=1, color='darkgrey') +\n    geom_point(size=3, color='firebrick')\n)\n\n\nComparing scholars\nYou can compare the citation history of scholars by fetching data with compare_scholars.\n\n\nCode\n# Compare Fabozzi and Herman Van Dijk\nids &lt;- c('tqXS4IMAAAAJ', 'FWQAAAAJ')\n\n# Get a data frame comparing the number of citations to their work in\n# a given year\ncs &lt;- compare_scholars(ids)\n\n## remove some 'bad' records without sufficient information\ncs &lt;- subset(cs, !is.na(year) & year &gt; 2000)\n\nplotly::ggplotly(\nggplot(cs, aes(year, cites, group=name, color=name)) + geom_line() + theme(legend.position=\"bottom\")\n)\n\n\n\n\nCode\n## Compare their career trajectories, based on year of first citation\ncsc &lt;- compare_scholar_careers(ids)\n\nplotly::ggplotly(\nggplot(csc, aes(career_year, cites, group=name, color=name)) + geom_line() + geom_point() +\n    theme(legend.position=c(.2, .8))\n)\n\n\nVisualizing and comparing network of coauthors\n\n\nCode\ncoautorias &lt;- 'FWQAAAAJ&hl'\nget_profile(coautorias)$name\n\nget_profile('FWQAAAAJ')$name\n\n\n# Be careful with specifying too many coauthors as the visualization of the\n# network can get very messy.\ncoauthor_network &lt;- get_coauthors('FWQAAAAJ&hl', n_coauthors = 7)\n\ncoauthor_network\n\n\nAnd then we have a built-in function to plot this visualization.\n\n\nCode\nplot_coauthors(coauthor_network)",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  },
  {
    "objectID": "revsyslit.html#bernhard-pfaff",
    "href": "revsyslit.html#bernhard-pfaff",
    "title": "Literature Review and Scientific Challenge",
    "section": "Bernhard Pfaff",
    "text": "Bernhard Pfaff\n\nGitHub https://github.com/bpfaff/\nScholar https://scholar.google.com.br/scholar?q=bernhard+pfaff&hl=pt-BR&as_sdt=0&as_vis=1&oi=scholart",
    "crumbs": [
      "About",
      "Intro",
      "Literature Review and Scientific Challenge"
    ]
  }
]